---
title: "Windows RDMA 클러스터 toorun MPI 응용 프로그램을 aaaSet | Microsoft Docs"
description: "어떻게 toocreate H16r, H16mr, A8 또는 A9 Vm toouse 크기를 사용 하 여 Windows HPC 팩 클러스터 hello Azure RDMA 네트워크 toorun MPI 응용 프로그램에 알아봅니다."
services: virtual-machines-windows
documentationcenter: 
author: dlepow
manager: timlt
editor: 
tags: azure-service-management,hpc-pack
ms.assetid: 7d9f5bc8-012f-48dd-b290-db81c7592215
ms.service: virtual-machines-windows
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: vm-windows
ms.workload: big-compute
ms.date: 06/01/2017
ms.author: danlep
ms.openlocfilehash: 23bc8740dbd05a7c7ab3f998489a41d0df4520a2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="set-up-a-windows-rdma-cluster-with-hpc-pack-toorun-mpi-applications"></a><span data-ttu-id="7c58f-103">HPC Pack toorun MPI 응용 프로그램을 사용 하 여 Windows RDMA 클러스터 설정</span><span class="sxs-lookup"><span data-stu-id="7c58f-103">Set up a Windows RDMA cluster with HPC Pack toorun MPI applications</span></span>
<span data-ttu-id="7c58f-104">사용 하 여 Azure에서 Windows RDMA 클러스터 설정 [Microsoft HPC Pack](https://technet.microsoft.com/library/cc514029) 및 [VM 크기를 계산 하는 고성능](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) toorun 병렬 인터페이스 MPI (Message Passing) 응용 프로그램입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-104">Set up a Windows RDMA cluster in Azure with [Microsoft HPC Pack](https://technet.microsoft.com/library/cc514029) and [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) toorun parallel Message Passing Interface (MPI) applications.</span></span> <span data-ttu-id="7c58f-105">HPC 팩 클러스터에서 RDMA 지원, Windows Server 기반 노드를 설정하는 경우 MPI 응용 프로그램은 Azure에서 RDMA(원격 직접 메모리 액세스) 기술을 기반으로 하는 낮은 대기 시간 및 높은 처리량의 네트워크에서 효율적으로 통신합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-105">When you set up RDMA-capable, Windows Server-based nodes in an HPC Pack cluster, MPI applications communicate efficiently over a low latency, high throughput network in Azure that is based on remote direct memory access (RDMA) technology.</span></span>

<span data-ttu-id="7c58f-106">Linux Vm의 경우 해당 액세스 hello Azure RDMA 네트워크에 toorun MPI 작업 참조 [Linux RDMA 클러스터 toorun MPI 응용 프로그램 설정](../../linux/classic/rdma-cluster.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-106">If you want toorun MPI workloads on Linux VMs that access hello Azure RDMA network, see [Set up a Linux RDMA cluster toorun MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

## <a name="hpc-pack-cluster-deployment-options"></a><span data-ttu-id="7c58f-107">HPC 팩 클러스터 배포 옵션</span><span class="sxs-lookup"><span data-stu-id="7c58f-107">HPC Pack cluster deployment options</span></span>
<span data-ttu-id="7c58f-108">Microsoft HPC Pack는 없는 추가 비용 toocreate에 HPC 클러스터 온-프레미스 제공 되거나 Azure toorun Windows 또는 Linux HPC 응용 프로그램에는 도구입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-108">Microsoft HPC Pack is a tool provided at no additional cost toocreate HPC clusters on-premises or in Azure toorun Windows or Linux HPC applications.</span></span> <span data-ttu-id="7c58f-109">HPC Pack hello 메시지 전달 인터페이스 Windows 용 MS-MPI ()의 hello Microsoft 구현 위한 런타임 환경이 포함 되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-109">HPC Pack includes a runtime environment for hello Microsoft implementation of hello Message Passing Interface for Windows (MS-MPI).</span></span> <span data-ttu-id="7c58f-110">지원 되는 Windows Server 운영 체제를 실행 하는 RDMA 가능 인스턴스에서만 사용 되므로, HPC 팩 hello Azure RDMA 네트워크를 액세스 효율적인 옵션 toorun Windows MPI 응용 프로그램 제공 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-110">When used with RDMA-capable instances running a supported Windows Server operating system, HPC Pack provides an efficient option toorun Windows MPI applications that access hello Azure RDMA network.</span></span> 

<span data-ttu-id="7c58f-111">이 문서는 두 가지 시나리오를 소개 하 고 toodetailed 지침 tooset Microsoft HPC Pack을 사용한 Windows RDMA 클러스터를 연결 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-111">This article introduces two scenarios and links toodetailed guidance tooset up a Windows RDMA cluster with Microsoft HPC Pack.</span></span> 

* <span data-ttu-id="7c58f-112">시나리오 1.</span><span class="sxs-lookup"><span data-stu-id="7c58f-112">Scenario 1.</span></span> <span data-ttu-id="7c58f-113">계산 집약적 작업자 역할 인스턴스 배포(PaaS)</span><span class="sxs-lookup"><span data-stu-id="7c58f-113">Deploy compute-intensive worker role instances (PaaS)</span></span>
* <span data-ttu-id="7c58f-114">시나리오 2.</span><span class="sxs-lookup"><span data-stu-id="7c58f-114">Scenario 2.</span></span> <span data-ttu-id="7c58f-115">계산 집약적 VM에 계산 노드 배포(IaaS)</span><span class="sxs-lookup"><span data-stu-id="7c58f-115">Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>

<span data-ttu-id="7c58f-116">Windows 일반 전제 조건 toouse 계산 집약적인 인스턴스를 참조 하십시오. [VM 크기를 계산 하는 고성능](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-116">For general prerequisites toouse compute-intensive instances with Windows, see [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>

## <a name="scenario-1-deploy-compute-intensive-worker-role-instances-paas"></a><span data-ttu-id="7c58f-117">시나리오 1: 계산 집약적 작업자 역할 인스턴스 배포(PaaS)</span><span class="sxs-lookup"><span data-stu-id="7c58f-117">Scenario 1: Deploy compute-intensive worker role instances (PaaS)</span></span>
<span data-ttu-id="7c58f-118">기존 HPC 팩 클러스터에서 클라우드 서비스(PaaS)에서 실행 중인 Azure 작업자 역할 인스턴스(Azure 노드)에 추가 계산 리소스를 추가합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-118">From an existing HPC Pack cluster, add extra compute resources in Azure worker role instances (Azure nodes) running in a cloud service (PaaS).</span></span> <span data-ttu-id="7c58f-119">HPC Pack에서 "버스트 tooAzure"이 라고도 하는이 기능을 hello 작업자 역할 인스턴스에 대 한 크기의 범위를 지원 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-119">This feature, also called “burst tooAzure” from HPC Pack, supports a range of sizes for hello worker role instances.</span></span> <span data-ttu-id="7c58f-120">Azure 노드를 hello 추가 때 hello RDMA 가능 크기 중 하나를 지정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-120">When adding hello Azure nodes, specify one of hello RDMA-capable sizes.</span></span>

<span data-ttu-id="7c58f-121">다음은 단계 및 고려 사항은 tooburst tooRDMA 가능 Azure 인스턴스를 기존 (일반적으로 온-프레미스) 클러스터입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-121">Following are considerations and steps tooburst tooRDMA-capable Azure instances from an existing (typically on-premises) cluster.</span></span> <span data-ttu-id="7c58f-122">유사한 프로시저 tooadd 작업자 역할 인스턴스 tooan HPC Pack 헤드 노드는 Azure VM에 배포 된를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-122">Use similar procedures tooadd worker role instances tooan HPC Pack head node that is deployed in an Azure VM.</span></span>

> [!NOTE]
> <span data-ttu-id="7c58f-123">HPC Pack을 사용한 자습서 tooburst tooAzure를 참조 하십시오. [HPC Pack을 사용한 하이브리드 클러스터 설정](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-123">For a tutorial tooburst tooAzure with HPC Pack, see [Set up a hybrid cluster with HPC Pack](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span></span> <span data-ttu-id="7c58f-124">Note 특히 tooRDMA 가능 Azure 노드를 적용 하는 단계를 수행 하는 hello hello 고려해 야 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-124">Note hello considerations in hello following steps that apply specifically tooRDMA-capable Azure nodes.</span></span>
> 
> 

![버스트 tooAzure][burst]

### <a name="steps"></a><span data-ttu-id="7c58f-126">단계</span><span class="sxs-lookup"><span data-stu-id="7c58f-126">Steps</span></span>
1. <span data-ttu-id="7c58f-127">**HPC 팩 2012 R2 헤드 노드 배포 및 구성**</span><span class="sxs-lookup"><span data-stu-id="7c58f-127">**Deploy and configure an HPC Pack 2012 R2 head node**</span></span>
   
    <span data-ttu-id="7c58f-128">Hello에서 hello 최신 HPC Pack 설치 패키지를 다운로드 [Microsoft 다운로드 센터](https://www.microsoft.com/download/details.aspx?id=49922)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-128">Download hello latest HPC Pack installation package from hello [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span> <span data-ttu-id="7c58f-129">참조 된 Azure 버스트 배포에 대 한 요구 사항 및 지침은 tooprepare [Microsoft HPC Pack을 사용한 tooAzure 작업자 인스턴스 버스트](https://technet.microsoft.com/library/gg481749.aspx)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-129">For requirements and instructions tooprepare for an Azure burst deployment, see [Burst tooAzure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx).</span></span>
2. <span data-ttu-id="7c58f-130">**Hello Azure 구독에에서 관리 인증서를 구성 합니다.**</span><span class="sxs-lookup"><span data-stu-id="7c58f-130">**Configure a management certificate in hello Azure subscription**</span></span>
   
    <span data-ttu-id="7c58f-131">Hello 헤드 노드와 Azure 간의 인증서 toosecure hello 연결을 구성 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-131">Configure a certificate toosecure hello connection between hello head node and Azure.</span></span> <span data-ttu-id="7c58f-132">옵션 및 절차에 대 한 참조 [시나리오 tooConfigure hello HPC Pack 용 Azure 관리 인증서](http://technet.microsoft.com/library/gg481759.aspx)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-132">For options and procedures, see [Scenarios tooConfigure hello Azure Management Certificate for HPC Pack](http://technet.microsoft.com/library/gg481759.aspx).</span></span> <span data-ttu-id="7c58f-133">테스트 배포에 대 한 HPC Pack 설치 기본 Microsoft HPC Azure 관리 인증서 tooyour Azure 구독을 신속 하 게 업로드할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-133">For test deployments, HPC Pack installs a Default Microsoft HPC Azure Management Certificate you can quickly upload tooyour Azure subscription.</span></span>
3. <span data-ttu-id="7c58f-134">**새 클라우드 서비스 및 저장소 계정 만들기**</span><span class="sxs-lookup"><span data-stu-id="7c58f-134">**Create a new cloud service and a storage account**</span></span>
   
    <span data-ttu-id="7c58f-135">RDMA 가능 hello 인스턴스를 사용할 수 있는 지역에 배포 hello에 대 한 클라우드 서비스 및 저장소 계정을 Azure 포털 toocreate hello를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-135">Use hello Azure portal toocreate a cloud service and a storage account for hello deployment in a region where hello RDMA-capable instances are available.</span></span>
4. <span data-ttu-id="7c58f-136">**Azure 노드 템플릿 만들기**</span><span class="sxs-lookup"><span data-stu-id="7c58f-136">**Create an Azure node template**</span></span>
   
    <span data-ttu-id="7c58f-137">HPC 클러스터 관리자에서 hello 노드 템플릿 만들기 마법사를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-137">Use hello Create Node Template Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="7c58f-138">단계를 참조 하십시오. [Azure 노드 템플릿 만들기](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) "단계 tooDeploy Microsoft HPC Pack 사용한 Azure 노드"에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-138">For steps, see [Create an Azure node template](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) in “Steps tooDeploy Azure Nodes with Microsoft HPC Pack”.</span></span>
   
    <span data-ttu-id="7c58f-139">초기 테스트에 대 한 hello 템플릿에서 수동 가용성 정책을 구성 하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-139">For initial tests, we suggest configuring a manual availability policy in hello template.</span></span>
5. <span data-ttu-id="7c58f-140">**Toohello 클러스터 노드 추가**</span><span class="sxs-lookup"><span data-stu-id="7c58f-140">**Add nodes toohello cluster**</span></span>
   
    <span data-ttu-id="7c58f-141">HPC 클러스터 관리자에서 노드 추가 마법사 hello를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-141">Use hello Add Node Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="7c58f-142">자세한 내용은 참조 [Azure 노드 추가 toohello Windows HPC 클러스터](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-142">For more information, see [Add Azure Nodes toohello Windows HPC Cluster](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span></span>
   
    <span data-ttu-id="7c58f-143">Hello 노드의 hello 크기를 지정할 때는 hello RDMA 가능 인스턴스 크기 중 하나를 선택 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-143">When specifying hello size of hello nodes, select one of hello RDMA-capable instance sizes.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="7c58f-144">각 버스트 tooAzure hello 계산 집약적인 인스턴스 배포에서 HPC Pack을 자동으로 최소 두 개의 RDMA 가능 인스턴스 (예: A8) 프록시 노드로 배포 또한 toohello Azure 작업자 역할 인스턴스를 지정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-144">In each burst tooAzure deployment with hello compute-intensive instances, HPC Pack automatically deploys a minimum of two RDMA-capable instances (such as A8) as proxy nodes, in addition toohello Azure worker role instances you specify.</span></span> <span data-ttu-id="7c58f-145">hello 프록시 노드 toohello 구독 할당 되 고 hello Azure 작업자 역할 인스턴스와 함께 요금이 코어를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-145">hello proxy nodes use cores that are allocated toohello subscription and incur charges along with hello Azure worker role instances.</span></span>
   > 
   > 
6. <span data-ttu-id="7c58f-146">**(프로 비전) hello 노드를 시작 하 고 직접 온라인 toorun 작업**</span><span class="sxs-lookup"><span data-stu-id="7c58f-146">**Start (provision) hello nodes and bring them online toorun jobs**</span></span>
   
    <span data-ttu-id="7c58f-147">Hello 노드를 선택 하 고 hello를 사용 하 여 **시작** HPC 클러스터 관리자에서 작업 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-147">Select hello nodes and use hello **Start** action in HPC Cluster Manager.</span></span> <span data-ttu-id="7c58f-148">프로 비전 완료 되 면 hello 노드를 선택 하 고 hello를 사용 하 여 **온라인 상태로 만들기** HPC 클러스터 관리자에서 작업 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-148">When provisioning is complete, select hello nodes and use hello **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="7c58f-149">hello 노드는 준비 toorun 작업입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-149">hello nodes are ready toorun jobs.</span></span>
7. <span data-ttu-id="7c58f-150">**Toohello 클러스터 작업 제출**</span><span class="sxs-lookup"><span data-stu-id="7c58f-150">**Submit jobs toohello cluster**</span></span>
   
   <span data-ttu-id="7c58f-151">HPC Pack 작업 제출 도구 toorun 클러스터 작업을 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-151">Use HPC Pack job submission tools toorun cluster jobs.</span></span> <span data-ttu-id="7c58f-152">[Microsoft HPC 팩: 작업 관리](http://technet.microsoft.com/library/jj899585.aspx)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="7c58f-152">See [Microsoft HPC Pack: Job Management](http://technet.microsoft.com/library/jj899585.aspx).</span></span>
8. <span data-ttu-id="7c58f-153">**중지 (프로 비전 해제) hello 노드**</span><span class="sxs-lookup"><span data-stu-id="7c58f-153">**Stop (deprovision) hello nodes**</span></span>
   
   <span data-ttu-id="7c58f-154">작업 실행이 완료 되 면 hello 노드를 오프 라인 및 hello를 사용 하 여 **중지** HPC 클러스터 관리자에서 작업 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-154">When you are done running jobs, take hello nodes offline and use hello **Stop** action in HPC Cluster Manager.</span></span>

## <a name="scenario-2-deploy-compute-nodes-in-compute-intensive-vms-iaas"></a><span data-ttu-id="7c58f-155">시나리오 2: 계산 집약적 VM에 계산 노드 배포(IaaS)</span><span class="sxs-lookup"><span data-stu-id="7c58f-155">Scenario 2: Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>
<span data-ttu-id="7c58f-156">이 시나리오에서는 hello HPC Pack 헤드 노드 및 클러스터 계산 노드 Vm에 Azure 가상 네트워크에 배포 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-156">In this scenario, you deploy hello HPC Pack head node and cluster compute nodes on VMs in an Azure virtual network.</span></span> <span data-ttu-id="7c58f-157">HPC Pack은 자동 배포 스크립트 및 Azure 빠른 시작 템플릿을 포함하여 다양한 [Azure VM의 배포 옵션](../../linux/hpcpack-cluster-options.md)을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-157">HPC Pack provides several [deployment options in Azure VMs](../../linux/hpcpack-cluster-options.md), including automated deployment scripts and Azure quickstart templates.</span></span> <span data-ttu-id="7c58f-158">예를 들어 hello 다음 고려 사항 및 단계를 안내해 toouse hello [HPC Pack IaaS 배포 스크립트](hpcpack-cluster-powershell-script.md) Azure의 HPC Pack 2012 R2 클러스터의 hello 배포를 자동화 하 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-158">As an example, hello following considerations and steps guide you toouse hello [HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md) to automate hello deployment of an HPC Pack 2012 R2 cluster in Azure.</span></span>

![Azure VM의 클러스터][iaas]

### <a name="steps"></a><span data-ttu-id="7c58f-160">단계</span><span class="sxs-lookup"><span data-stu-id="7c58f-160">Steps</span></span>
1. <span data-ttu-id="7c58f-161">**클러스터 헤드 노드를 만들고 클라이언트 컴퓨터에서 hello HPC Pack IaaS 배포 스크립트를 실행 하 여 계산 노드 Vm**</span><span class="sxs-lookup"><span data-stu-id="7c58f-161">**Create a cluster head node and compute node VMs by running hello HPC Pack IaaS deployment script on a client computer**</span></span>
   
    <span data-ttu-id="7c58f-162">Hello에서 hello HPC Pack IaaS 배포 스크립트 패키지를 다운로드 [Microsoft 다운로드 센터](https://www.microsoft.com/download/details.aspx?id=49922)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-162">Download hello HPC Pack IaaS Deployment Script package from hello [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span>
   
    <span data-ttu-id="7c58f-163">tooprepare hello 클라이언트 컴퓨터 hello 스크립트 구성 파일을 만들고 실행된 hello 스크립트, 참조 [hello HPC Pack IaaS 배포 스크립트는 HPC 클러스터 만들기](hpcpack-cluster-powershell-script.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-163">tooprepare hello client computer, create hello script configuration file, and run hello script, see [Create an HPC Cluster with hello HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md).</span></span> 
   
    <span data-ttu-id="7c58f-164">추가 고려 사항에 따라 참고 hello toodeploy RDMA 가능 계산 노드:</span><span class="sxs-lookup"><span data-stu-id="7c58f-164">toodeploy RDMA-capable compute nodes, note hello following additional considerations:</span></span>
   
   * <span data-ttu-id="7c58f-165">**가상 네트워크**: toouse ´ ï ´ 원하는 어떤 hello RDMA 가능 인스턴스 크기의에서 지역에 새 가상 네트워크를 지정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-165">**Virtual network**: Specify a new virtual network in a region in which hello RDMA-capable instance size you want toouse is available.</span></span>
   * <span data-ttu-id="7c58f-166">**Windows Server 운영 체제**: toosupport RDMA 연결은 hello 계산 노드 Vm에 대 한 Windows Server 2012 R2 또는 Windows Server 2012 운영 체제를 지정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-166">**Windows Server operating system**: toosupport RDMA connectivity, specify a Windows Server 2012 R2 or Windows Server 2012 operating system for hello compute node VMs.</span></span>
   * <span data-ttu-id="7c58f-167">**클라우드 서비스**: 헤드 노드를 한 클라우드 서비스에 배포하고 계산 노드를 다른 클라우드 서비스에 배포하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-167">**Cloud services**: We recommend deploying your head node in one cloud service and your compute nodes in a different cloud service.</span></span>
   * <span data-ttu-id="7c58f-168">**헤드 노드 크기**:이 시나리오를 고려해 야의 크기 이상 A4 (매우 큼) hello 헤드 노드에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-168">**Head node size**: For this scenario, consider a size of at least A4 (Extra Large) for hello head node.</span></span>
   * <span data-ttu-id="7c58f-169">**HpcVmDrivers 확장**: hello 배포 스크립트 hello Azure VM 에이전트 및 HpcVmDrivers 확장 hello 자동으로 설치 Windows Server 운영 체제와 크기 A8 또는 A9 계산 노드를 배포 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-169">**HpcVmDrivers extension**: hello deployment script installs hello Azure VM Agent and hello HpcVmDrivers extension automatically when you deploy size A8 or A9 compute nodes with a Windows Server operating system.</span></span> <span data-ttu-id="7c58f-170">HpcVmDrivers는 toohello RDMA 네트워크에 연결할 수 있도록 hello 계산 노드 Vm에 드라이버를 설치 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-170">HpcVmDrivers installs drivers on hello compute node VMs so they can connect toohello RDMA network.</span></span> <span data-ttu-id="7c58f-171">RDMA 가능 H 시리즈 Vm에 HpcVmDrivers 확장 hello를 수동으로 설치 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-171">On RDMA-capable H-series VMs, you must manually install hello HpcVmDrivers extension.</span></span> <span data-ttu-id="7c58f-172">[고성능 계산 VM 크기](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="7c58f-172">See [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
   * <span data-ttu-id="7c58f-173">**클러스터 네트워크 구성**: hello 배포 스크립트를 자동으로 hello HPC Pack 클러스터 토폴로지 5 (hello 엔터프라이즈 네트워크의 모든 노드)를 설정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-173">**Cluster network configuration**: hello deployment script automatically sets up hello HPC Pack cluster in Topology 5 (all nodes on hello Enterprise network).</span></span> <span data-ttu-id="7c58f-174">이 토폴로지는 VM에서의 모든 HPC Pack 클러스터 배포에 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-174">This topology is required for all HPC Pack cluster deployments in VMs.</span></span> <span data-ttu-id="7c58f-175">나중에 hello 클러스터 네트워크 토폴로지를 변경 하지 마십시오.</span><span class="sxs-lookup"><span data-stu-id="7c58f-175">Do not change hello cluster network topology later.</span></span>
2. <span data-ttu-id="7c58f-176">**Hello 계산 노드 온라인 toorun 작업 가져오기**</span><span class="sxs-lookup"><span data-stu-id="7c58f-176">**Bring hello compute nodes online toorun jobs**</span></span>
   
    <span data-ttu-id="7c58f-177">Hello 노드를 선택 하 고 hello를 사용 하 여 **온라인 상태로 만들기** HPC 클러스터 관리자에서 작업 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-177">Select hello nodes and use hello **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="7c58f-178">hello 노드는 준비 toorun 작업입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-178">hello nodes are ready toorun jobs.</span></span>
3. <span data-ttu-id="7c58f-179">**Toohello 클러스터 작업 제출**</span><span class="sxs-lookup"><span data-stu-id="7c58f-179">**Submit jobs toohello cluster**</span></span>
   
    <span data-ttu-id="7c58f-180">Toohello 헤드 노드 toosubmit 작업을 연결 하거나이 온-프레미스 컴퓨터 toodo를 설정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-180">Connect toohello head node toosubmit jobs, or set up an on-premises computer toodo this.</span></span> <span data-ttu-id="7c58f-181">자세한 내용은 참조 [Azure에서 HPC 작업 제출 tooan 클러스터](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-181">For information, see [Submit Jobs tooan HPC cluster in Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
4. <span data-ttu-id="7c58f-182">**Take hello 노드가 오프 라인 및 중지 (할당 취소)에**</span><span class="sxs-lookup"><span data-stu-id="7c58f-182">**Take hello nodes offline and stop (deallocate) them**</span></span>
   
    <span data-ttu-id="7c58f-183">작업 실행이 완료 되 면 HPC 클러스터 관리자에서 hello 노드를 오프 라인으로 수행 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-183">When you are done running jobs, take hello nodes offline in HPC Cluster Manager.</span></span> <span data-ttu-id="7c58f-184">그런 다음 Azure 관리 도구 tooshut를 사용 하 여 다운 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-184">Then, use Azure management tools tooshut them down.</span></span>

## <a name="run-mpi-applications-on-hello-cluster"></a><span data-ttu-id="7c58f-185">Hello 클러스터에서 MPI 응용 프로그램을 실행 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-185">Run MPI applications on hello cluster</span></span>
### <a name="example-run-mpipingpong-on-an-hpc-pack-cluster"></a><span data-ttu-id="7c58f-186">예: HPC 팩 클러스터에서 mpipingpong 실행</span><span class="sxs-lookup"><span data-stu-id="7c58f-186">Example: Run mpipingpong on an HPC Pack cluster</span></span>
<span data-ttu-id="7c58f-187">RDMA 가능 hello 인스턴스를 실행 하는 HPC Pack 배포 tooverify HPC 팩 hello **mpipingpong** hello 클러스터에서 명령을 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-187">tooverify an HPC Pack deployment of hello RDMA-capable instances, run hello HPC Pack **mpipingpong** command on hello cluster.</span></span> <span data-ttu-id="7c58f-188">**mpipingpong** 쌍을 이루는 노드 간에 데이터 패킷을 반복적으로 보내는 toocalculate 대기 시간 및 처리량을 측정 하 고 hello RDMA 사용 응용 프로그램 네트워크에 대 한 통계입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-188">**mpipingpong** sends packets of data between paired nodes repeatedly toocalculate latency and throughput measurements and statistics for hello RDMA-enabled application network.</span></span> <span data-ttu-id="7c58f-189">이 예제에서는 MPI 작업을 실행 하기 위한 일반적인 패턴을 보여 줍니다 (이 경우 **mpipingpong**) hello 클러스터를 사용 하 여 **mpiexec** 명령입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-189">This example shows a typical pattern for running an MPI job (in this case, **mpipingpong**) by using hello cluster **mpiexec** command.</span></span>

<span data-ttu-id="7c58f-190">이 예제에서는 "tooAzure 전환" 구성에 Azure 노드를 추가 가정 ([시나리오 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-190">This example assumes you added Azure nodes in a “burst tooAzure” configuration ([Scenario 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article).</span></span> <span data-ttu-id="7c58f-191">Azure Vm의 클러스터에 HPC Pack을 배포 하는 경우 toomodify hello 명령 구문을 toospecify 다른 노드 그룹 필요 하 고 추가 환경 변수 toodirect 네트워크 트래픽 toohello RDMA 네트워크를 설정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-191">If you deployed HPC Pack on a cluster of Azure VMs, you’ll need toomodify hello command syntax toospecify a different node group and set additional environment variables toodirect network traffic toohello RDMA network.</span></span>

<span data-ttu-id="7c58f-192">hello 클러스터에서 mpipingpong toorun:</span><span class="sxs-lookup"><span data-stu-id="7c58f-192">toorun mpipingpong on hello cluster:</span></span>

1. <span data-ttu-id="7c58f-193">Hello 헤드 노드 또는 올바르게 구성 된 클라이언트 컴퓨터에서 명령 프롬프트를 엽니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-193">On hello head node or on a properly configured client computer, open a Command Prompt.</span></span>
2. <span data-ttu-id="7c58f-194">다음 명령은 toosubmit 작은 패킷 크기와 많은 반복 작업이 toorun mpipingpong 형식 hello 4 개 노드 중 Azure 전환 배포에서 노드 쌍 간의 tooestimate 대기 시간:</span><span class="sxs-lookup"><span data-stu-id="7c58f-194">tooestimate latency between pairs of nodes in an Azure burst deployment of four nodes, type hello following command toosubmit a job toorun mpipingpong with a small packet size and many iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```
   
    <span data-ttu-id="7c58f-195">hello 명령은 제출 된 hello 작업의 hello ID를 반환 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-195">hello command returns hello ID of hello job that is submitted.</span></span>
   
    <span data-ttu-id="7c58f-196">Azure Vm에 배포 된 hello HPC Pack 클러스터를 배포한 경우 포함 하는 노드 그룹 계산 노드 Vm에는 단일 클라우드 서비스 배포를 지정 하 고 hello 수정 **mpiexec** 명령은 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-196">If you deployed hello HPC Pack cluster deployed on Azure VMs, specify a node group that contains compute node VMs deployed in a single cloud service, and modify hello **mpiexec** command as follows:</span></span>
   
    ```Command
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI_DISABLE_SOCK 1 -env MSMPI_PRECONNECT all -env MPICH_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```
3. <span data-ttu-id="7c58f-197">Hello 작업이 완료 되 면 tooview hello 출력에 (이 경우 hello 작업의 작업 1의 hello 출력)를 따르는 형식 hello</span><span class="sxs-lookup"><span data-stu-id="7c58f-197">When hello job completes, tooview hello output (in this case, hello output of task 1 of hello job), type hello following</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
    <span data-ttu-id="7c58f-198">여기서 &lt; *JobID* &gt; hello 제출 된 hello 작업 ID입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-198">where &lt;*JobID*&gt; is hello ID of hello job that was submitted.</span></span>
   
    <span data-ttu-id="7c58f-199">hello 출력 toohello 다음과 유사한 대기 시간 결과 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-199">hello output includes latency results similar toohello following.</span></span>
   
    ![핑퐁 대기 시간][pingpong1]
4. <span data-ttu-id="7c58f-201">Azure의 쌍 간의 처리량 tooestimate 버스트 노드, 형식 hello 다음 명령은 toosubmit 작업 toorun **mpipingpong** 큰 패킷 크기와를 몇 차례 반복:</span><span class="sxs-lookup"><span data-stu-id="7c58f-201">tooestimate throughput between pairs of Azure burst nodes, type hello following command toosubmit a job toorun **mpipingpong** with a large packet size and a few iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```
   
    <span data-ttu-id="7c58f-202">hello 명령은 제출 된 hello 작업의 hello ID를 반환 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-202">hello command returns hello ID of hello job that is submitted.</span></span>
   
    <span data-ttu-id="7c58f-203">Azure Vm에 배포 된 HPC Pack 클러스터에서 2 단계에서 설명 된 대로 hello 명령을 수정 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-203">On an HPC Pack cluster deployed on Azure VMs, modify hello command as noted in step 2.</span></span>
5. <span data-ttu-id="7c58f-204">Hello 작업이 완료 되 면 tooview hello 형식 hello 뒤에 (이 경우 hello 작업의 작업 1의 hello 출력)를 출력 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-204">When hello job completes, tooview hello output (in this case, hello output of task 1 of hello job), type hello following:</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
   <span data-ttu-id="7c58f-205">hello 출력 toohello 다음과를 유사한 처리량 결과가 포함 됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-205">hello output includes throughput results similar toohello following.</span></span>
   
   ![핑퐁 처리량][pingpong2]

### <a name="mpi-application-considerations"></a><span data-ttu-id="7c58f-207">MPI 응용 프로그램 고려 사항</span><span class="sxs-lookup"><span data-stu-id="7c58f-207">MPI application considerations</span></span>
<span data-ttu-id="7c58f-208">다음은 Azure에서 HPC Pack을 사용하여 MPI 응용 프로그램을 실행하기 위한 고려 사항입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-208">Following are considerations for running MPI applications with HPC Pack in Azure.</span></span> <span data-ttu-id="7c58f-209">일부 Azure 노드 ("버스트 tooAzure" 구성에 추가 된 작업자 역할 인스턴스)의 toodeployments만 적용 됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-209">Some apply only toodeployments of Azure nodes (worker role instances added in a “burst tooAzure” configuration).</span></span>

* <span data-ttu-id="7c58f-210">클라우드 서비스의 작업자 역할 인스턴스는 Azure에서 미리 알리지 않고 정기적으로 다시 프로비전됩니다(예: 시스템 유지 관리에서 또는 인스턴스가 실패할 경우).</span><span class="sxs-lookup"><span data-stu-id="7c58f-210">Worker role instances in a cloud service are periodically reprovisioned without notice by Azure (for example, for system maintenance, or in case an instance fails).</span></span> <span data-ttu-id="7c58f-211">MPI 작업을 실행 하는 동안에 인스턴스는 다시 프로 비전, 경우 hello 인스턴스 데이터를 잃고 처음 배포한, hello MPI 작업 toofail을 일으킬 수 있는 경우 toohello 상태를 반환 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-211">If an instance is reprovisioned while it is running an MPI job, hello instance loses its data and returns toohello state when it was first deployed, which can cause hello MPI job toofail.</span></span> <span data-ttu-id="7c58f-212">hello 노드 hello 및 단일 MPI 작업에 대 한 사용 하는 더 이상 hello 작업을 실행 하면 hello는 hello 인스턴스 중 하나는 다시 프로 비전 작업이 실행 되는 동안 가능성이 높습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-212">hello more nodes that you use for a single MPI job, and hello longer hello job runs, hello more likely that one of hello instances is reprovisioned while a job is running.</span></span> <span data-ttu-id="7c58f-213">이러한 사실도 고려해 hello 배포에서 단일 노드를 파일 서버로 지정 하면 됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-213">Also consider this if you designate a single node in hello deployment as a file server.</span></span>
* <span data-ttu-id="7c58f-214">Azure에서 MPI 작업 toorun toouse hello RDMA 가능 인스턴스가 없는 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-214">toorun MPI jobs in Azure, you don't have toouse hello RDMA-capable instances.</span></span> <span data-ttu-id="7c58f-215">HPC 팩에서 지원되는 모든 인스턴스 크기를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-215">You can use any instance size that is supported by HPC Pack.</span></span> <span data-ttu-id="7c58f-216">그러나 RDMA 가능 인스턴스 hello 상대적으로 대규모의 MPI 작업은 중요 한 toohello 대기 시간 및 hello hello 노드를 연결 하는 hello 네트워크 대역폭을 실행 하는 데 권장 됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-216">However, hello RDMA-capable instances are recommended for running relatively large-scale MPI jobs that are sensitive toohello latency and hello bandwidth of hello network that connects hello nodes.</span></span> <span data-ttu-id="7c58f-217">다른 크기 toorun 대기 시간 및 대역폭에 민감한 MPI 작업을 사용 하는 경우에 단일 태스크만 몇 가지 노드에서 실행 되는 소형 작업을 실행 하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-217">If you use other sizes toorun latency- and bandwidth-sensitive MPI jobs, we recommend running small jobs, in which a single task runs on only a few nodes.</span></span>
* <span data-ttu-id="7c58f-218">배포 된 응용 프로그램 tooAzure 인스턴스는 hello 응용 프로그램과 관련 된 약관 제목 toohello입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-218">Applications deployed tooAzure instances are subject toohello licensing terms associated with hello application.</span></span> <span data-ttu-id="7c58f-219">라이선스에 대 한 상용 응용 프로그램 또는 hello 클라우드에서 실행 하기 위한 기타 제한의 hello 공급 업체와 함께 확인 하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c58f-219">Check with hello vendor of any commercial application for licensing or other restrictions for running in hello cloud.</span></span> <span data-ttu-id="7c58f-220">일부 공급 업체는 종량제 라이선스를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-220">Not all vendors offer pay-as-you-go licensing.</span></span>
* <span data-ttu-id="7c58f-221">Azure 인스턴스 tooaccess 온-프레미스 노드, 공유 및 라이선스 서버 설치 추가로 필요 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-221">Azure instances need further setup tooaccess on-premises nodes, shares, and license servers.</span></span> <span data-ttu-id="7c58f-222">예를 들어, tooenable hello Azure 노드 tooaccess 온-프레미스 라이선스 서버를 사용 하는 사이트 간 Azure 가상 네트워크를 구성할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-222">For example, tooenable hello Azure nodes tooaccess an on-premises license server, you can configure a site-to-site Azure virtual network.</span></span>
* <span data-ttu-id="7c58f-223">Azure 인스턴스에서 MPI 응용 프로그램 toorun 각 MPI 응용 프로그램 등록 with Windows Firewall hello 인스턴스에서 hello를 실행 하 여 **hpcfwutil** 명령입니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-223">toorun MPI applications on Azure instances, register each MPI application with Windows Firewall on hello instances by running hello **hpcfwutil** command.</span></span> <span data-ttu-id="7c58f-224">따라서 hello 방화벽에 의해 동적으로 할당 된 포트에서 MPI 통신 tootake 위치를 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-224">This allows MPI communications tootake place on a port that is assigned dynamically by hello firewall.</span></span>
  
  > [!NOTE]
  > <span data-ttu-id="7c58f-225">버스트 tooAzure 배포용 구성할 수도 있습니다 방화벽 예외 명령을 toorun 자동으로 tooyour 클러스터에 추가 된 모든 새 Azure 노드의 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-225">For burst tooAzure deployments, you can also configure a firewall exception command toorun automatically on all new Azure nodes that are added tooyour cluster.</span></span> <span data-ttu-id="7c58f-226">Hello를 실행 한 후 **hpcfwutil** 명령로 스크롤한 다음 응용 프로그램의 작동을 Azure 노드 용 hello 명령 tooa 시작 스크립트를 추가 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-226">After you run hello **hpcfwutil** command and verify that your application works, add hello command tooa startup script for your Azure nodes.</span></span> <span data-ttu-id="7c58f-227">자세한 내용은 [Azure 노드에 시작 스크립트 사용](https://technet.microsoft.com/library/jj899632.aspx)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="7c58f-227">For more information, see [Use a Startup Script for Azure Nodes](https://technet.microsoft.com/library/jj899632.aspx).</span></span>
  > 
  > 
* <span data-ttu-id="7c58f-228">HPC 팩 hello CCP_MPI_NETMASK 클러스터 환경 변수 toospecify 허용 되는 주소의 범위를 사용 하 여 MPI 통신에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-228">HPC Pack uses hello CCP_MPI_NETMASK cluster environment variable toospecify a range of acceptable addresses for MPI communication.</span></span> <span data-ttu-id="7c58f-229">HPC Pack 2012 r 2 부터는 hello CCP_MPI_NETMASK 클러스터 환경 변수만 영향을 도메인에 가입 된 클러스터 계산 노드 간 MPI 통신 (온-프레미스 또는 Azure Vm에서).</span><span class="sxs-lookup"><span data-stu-id="7c58f-229">Starting in HPC Pack 2012 R2, hello CCP_MPI_NETMASK cluster environment variable only affects MPI communication between domain-joined cluster compute nodes (either on-premises or in Azure VMs).</span></span> <span data-ttu-id="7c58f-230">전환 tooAzure 구성에 추가 된 노드에서 hello 변수는 무시 됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-230">hello variable is ignored by nodes added in a burst tooAzure configuration.</span></span>
* <span data-ttu-id="7c58f-231">다른 클라우드 서비스 (예를 들어 여러 클라우드 서비스에 배포 된 Azure VM 계산 노드 또는 서로 다른 노드 템플릿으로 버스트 tooAzure 배포의 경우)에 배포 되는 Azure 인스턴스에서 MPI 작업을 실행할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-231">MPI jobs can't run across Azure instances that are deployed in different cloud services (for example, in burst tooAzure deployments with different node templates, or Azure VM compute nodes deployed in multiple cloud services).</span></span> <span data-ttu-id="7c58f-232">서로 다른 노드 템플릿으로 시작 하는 여러 Azure 노드 배포가 있는 경우 hello MPI 작업은 하나의 Azure 노드 집합에서 실행 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-232">If you have multiple Azure node deployments that are started with different node templates, hello MPI job must run on only one set of Azure nodes.</span></span>
* <span data-ttu-id="7c58f-233">Azure 노드 tooyour 클러스터를 추가 하 고 가져오고으로 온라인 hello HPC 작업 스케줄러 서비스 즉시 hello 노드에서 toostart 작업을 시도 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-233">When you add Azure nodes tooyour cluster and bring them online, hello HPC Job Scheduler Service immediately tries toostart jobs on hello nodes.</span></span> <span data-ttu-id="7c58f-234">경우에 작업의 일부가 Azure에서 실행 수를 업데이트 하거나 작업 템플릿 toodefine 형식을 Azure에서 실행할 수 있는 작업 만들기를 확인 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-234">If only a portion of your workload can run on Azure, ensure that you update or create job templates toodefine what job types can run on Azure.</span></span> <span data-ttu-id="7c58f-235">예를 들어 Azure 노드에서만 실행만 작업 템플릿을 사용 하 여 제출 된 작업이 hello 노드 그룹 속성 toohello 작업 서식 파일을 추가 하 고 hello로 AzureNodes 선택 tooensure 값이 필요 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-235">For example, tooensure that jobs submitted with a job template only run on Azure nodes, add hello Node Groups property toohello job template and select AzureNodes as hello required value.</span></span> <span data-ttu-id="7c58f-236">Azure 노드 용 사용자 지정 그룹 toocreate hello Add-hpcgroup HPC PowerShell cmdlet을 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-236">toocreate custom groups for your Azure nodes, use hello Add-HpcGroup HPC PowerShell cmdlet.</span></span>

## <a name="next-steps"></a><span data-ttu-id="7c58f-237">다음 단계</span><span class="sxs-lookup"><span data-stu-id="7c58f-237">Next steps</span></span>
* <span data-ttu-id="7c58f-238">대체 toousing HPC Pack으로 Azure의 계산 노드 관리 되는 풀에서 MPI 응용 프로그램 hello Azure 배치 서비스 toorun 사용 하 여 개발 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-238">As an alternative toousing HPC Pack, develop with hello Azure Batch service toorun MPI applications on managed pools of compute nodes in Azure.</span></span> <span data-ttu-id="7c58f-239">참조 [toorun 인터페이스 MPI (Message Passing) 응용 프로그램을 Azure 일괄 처리의 작업을 사용 하 여 다중 인스턴스](../../../batch/batch-mpi.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-239">See [Use multi-instance tasks toorun Message Passing Interface (MPI) applications in Azure Batch](../../../batch/batch-mpi.md).</span></span>
* <span data-ttu-id="7c58f-240">Linux MPI toorun 하려는 경우 hello Azure RDMA 네트워크에 액세스 하는 응용 프로그램 참조 [Linux RDMA 클러스터 toorun MPI 응용 프로그램 설정](../../linux/classic/rdma-cluster.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="7c58f-240">If you want toorun Linux MPI applications that access hello Azure RDMA network, see [Set up a Linux RDMA cluster toorun MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

<!--Image references-->
[burst]:media/hpcpack-rdma-cluster/burst.png
[iaas]:media/hpcpack-rdma-cluster/iaas.png
[pingpong1]:media/hpcpack-rdma-cluster/pingpong1.png
[pingpong2]:media/hpcpack-rdma-cluster/pingpong2.png
