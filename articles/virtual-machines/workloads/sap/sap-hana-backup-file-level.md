---
title: "파일 수준에서 Azure 백업의 HANA aaaSAP | Microsoft Docs"
description: "Azure 가상 컴퓨터에는 SAP HANA에 대한 두 가지 주요 백업 방법이 있습니다. 이 문서에서는 파일 수준의 SAP HANA Azure 백업에 대해 설명합니다"
services: virtual-machines-linux
documentationcenter: 
author: hermanndms
manager: timlt
editor: 
ms.service: virtual-machines-linux
ms.devlang: NA
ms.topic: article
ums.tgt_pltfrm: vm-linux
ms.workload: infrastructure-services
ms.date: 3/13/2017
ms.author: rclaus
ms.openlocfilehash: d5a55de5634ac7724e7fd0fa3760c6c408c3db74
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="sap-hana-azure-backup-on-file-level"></a><span data-ttu-id="ebab6-103">파일 수준의 SAP HANA Azure 백업</span><span class="sxs-lookup"><span data-stu-id="ebab6-103">SAP HANA Azure Backup on file level</span></span>

## <a name="introduction"></a><span data-ttu-id="ebab6-104">소개</span><span class="sxs-lookup"><span data-stu-id="ebab6-104">Introduction</span></span>

<span data-ttu-id="ebab6-105">이 문서는 3부로 구성된 SAP HANA 백업 관련 문서 시리즈의 일부입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-105">This is part of a three-part series of related articles on SAP HANA backup.</span></span> <span data-ttu-id="ebab6-106">[Azure 가상 컴퓨터에서 SAP HANA에 대 한 백업 가이드](./sap-hana-backup-guide.md) 시작, 대 한 개요 및 정보를 제공 하 고 [SAP HANA 백업 스냅숏을 기반으로 저장소](./sap-hana-backup-storage-snapshots.md) 표지 hello 저장소 스냅숏 기반 백업 옵션입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-106">[Backup guide for SAP HANA on Azure Virtual Machines](./sap-hana-backup-guide.md) provides an overview and information on getting started, and [SAP HANA backup based on storage snapshots](./sap-hana-backup-storage-snapshots.md) covers hello storage snapshot-based backup option.</span></span>

<span data-ttu-id="ebab6-107">Hello Azure VM 크기 보고, 하나는 GS5 64 개의 연결 된 데이터 디스크를 허용 하는지 확인할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-107">Looking at hello Azure VM sizes, one can see that a GS5 allows 64 attached data disks.</span></span> <span data-ttu-id="ebab6-108">대용량 SAP HANA 시스템의 경우 데이터와 로그 파일을 유지하는 데 많은 수의 디스크를 이미 사용하고 있으므로 소프트웨어 RAID와 결합하여 디스크 IO 처리량을 최적화할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-108">For large SAP HANA systems, a significant number of disks might already be taken for data and log files, possibly in combination with software RAID for optimal disk IO throughput.</span></span> <span data-ttu-id="ebab6-109">hello 문제 다음 toostore SAP HANA 가득 hello 연결 된 데이터 디스크 시간이 지남에 따라 있는 파일을 백업 하는 위치는입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-109">hello question then is where toostore SAP HANA backup files, which could fill up hello attached data disks over time?</span></span> <span data-ttu-id="ebab6-110">참조 [Azure에서 Linux 가상 컴퓨터에 대 한 크기](../../linux/sizes.md) hello Azure VM 크기 테이블에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-110">See [Sizes for Linux virtual machines in Azure](../../linux/sizes.md) for hello Azure VM size tables.</span></span>

<span data-ttu-id="ebab6-111">현재 Azure Backup 서비스에서 사용할 수 있는 SAP HANA 백업 통합은 없습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-111">There is no SAP HANA backup integration available with Azure Backup service at this time.</span></span> <span data-ttu-id="ebab6-112">SAP HANA Studio 또는 SAP HANA SQL 문을 통해 파일 기반 백업으로 hello 파일 수준에서 백업/복원 toomanage는 hello 표준 방법입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-112">hello standard way toomanage backup/restore at hello file level is with a file-based backup via SAP HANA Studio or via SAP HANA SQL statements.</span></span> <span data-ttu-id="ebab6-113">자세한 내용은 [SAP HANA SQL 및 시스템 보기 참조](https://help.sap.com/hana/SAP_HANA_SQL_and_System_Views_Reference_en.pdf)(영문)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebab6-113">See [SAP HANA SQL and System Views Reference](https://help.sap.com/hana/SAP_HANA_SQL_and_System_Views_Reference_en.pdf) for more information.</span></span>

![이 그림에서는 SAP HANA Studio에서 hello 백업 메뉴 항목의 hello 대화 상자를 보여 줍니다.](media/sap-hana-backup-file-level/image022.png)

<span data-ttu-id="ebab6-115">이 그림에서 SAP HANA Studio hello 백업 메뉴 항목의 hello 대화 상자를 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-115">This figure shows hello dialog of hello backup menu item in SAP HANA Studio.</span></span> <span data-ttu-id="ebab6-116">형식을 선택할 때 &quot;파일인&quot; 하나에 toospecify 경로 hello 파일 시스템 SAP HANA hello 백업 파일을 기록 하는 위치에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-116">When choosing type &quot;file,&quot; one has toospecify a path in hello file system where SAP HANA writes hello backup files.</span></span> <span data-ttu-id="ebab6-117">복원 하는 hello 동일한 방식으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-117">Restore works hello same way.</span></span>

<span data-ttu-id="ebab6-118">간단하고 직관적인 선택으로 보이지만 몇 가지 고려 사항이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-118">While this choice sounds simple and straight forward, there are some considerations.</span></span> <span data-ttu-id="ebab6-119">앞에서 설명한 대로 Azure VM에 연결할 수 있는 데이터 디스크의 수는 제한됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-119">As mentioned before, an Azure VM has a limitation of number of data disks that can be attached.</span></span> <span data-ttu-id="ebab6-120">용량 toostore SAP HANA의 hello 데이터베이스 및 디스크 처리량 요구 사항, 소프트웨어를 수행할 수도 있습니다의 hello 크기에 따라 VM hello hello 파일 시스템에 백업 파일 수 없는 경우 여러 데이터 디스크 스트라이프를 사용 하 여 RAID 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-120">There might not be capacity toostore SAP HANA backup files on hello file systems of hello VM, depending on hello size of hello database and disk throughput requirements, which might involve software RAID using striping across multiple data disks.</span></span> <span data-ttu-id="ebab6-121">테라바이트 단위의 데이터를 처리할 때 이러한 백업 파일을 이동하고 파일 크기 제한과 성능을 관리하기 위한 다양한 옵션이 이 문서의 뒷부분에 나와 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-121">Various options for moving these backup files, and managing file size restrictions and performance when handling terabytes of data, are provided later in this article.</span></span>

<span data-ttu-id="ebab6-122">전체 용량과 관련하여 더 많은 유연성을 제공하는 또 다른 옵션은 Azure Blob 저장소입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-122">Another option, which offers more freedom regarding total capacity, is Azure blob storage.</span></span> <span data-ttu-id="ebab6-123">단일 blob 제한 too1 TB도 이지만, 현재 500TB hello 단일 blob 컨테이너의 총 용량이입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-123">While a single blob is also restricted too1 TB, hello total capacity of a single blob container is currently 500 TB.</span></span> <span data-ttu-id="ebab6-124">또한 제공 고객 hello choice tooselect 소위 &quot;쿨&quot; blob 저장소 비용 이점이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-124">Additionally, it gives customers hello choice tooselect so-called &quot;cool&quot; blob storage, which has a cost benefit.</span></span> <span data-ttu-id="ebab6-125">멋진 Blob 저장소에 대한 자세한 내용은 [Azure Blob Storage: 핫 및 쿨 저장소 계층](../../../storage/blobs/storage-blob-storage-tiers.md)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebab6-125">See [Azure Blob Storage: Hot and cool storage tiers](../../../storage/blobs/storage-blob-storage-tiers.md) for details about cool blob storage.</span></span>

<span data-ttu-id="ebab6-126">추가 보안을 위해 지리적 복제 된 저장소 계정 toostore hello SAP HANA 백업을 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-126">For additional safety, use a geo-replicated storage account toostore hello SAP HANA backups.</span></span> <span data-ttu-id="ebab6-127">저장소 계정 복제에 대한 자세한 내용은 [Azure Storage 복제](../../../storage/common/storage-redundancy.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebab6-127">See [Azure Storage replication](../../../storage/common/storage-redundancy.md) for details about storage account replication.</span></span>

<span data-ttu-id="ebab6-128">지리적 복제된 전용 백업 저장소 계정에 SAP HANA 백업 전용 VHD를 배치할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-128">One could place dedicated VHDs for SAP HANA backups in a dedicated backup storage account that is geo-replicated.</span></span> <span data-ttu-id="ebab6-129">그렇지 않으면 tooa 지리적 복제 된 저장소 계정 hello SAP HANA 백업을 유지 하는 hello Vhd 또는 다른 지역에 있는 tooa 저장소 계정을 복사 하나.</span><span class="sxs-lookup"><span data-stu-id="ebab6-129">Or else one could copy hello VHDs that keep hello SAP HANA backups tooa geo-replicated storage account, or tooa storage account that is in a different region.</span></span>

## <a name="azure-backup-agent"></a><span data-ttu-id="ebab6-130">Azure Backup 에이전트</span><span class="sxs-lookup"><span data-stu-id="ebab6-130">Azure backup agent</span></span>

<span data-ttu-id="ebab6-131">Azure 백업 제안의 hello 옵션 toonot toobe hello 게스트 OS에 설치 되어 있는 hello 백업 에이전트를 통해 전체 Vm 하지만 또한 파일 및 디렉터리를만 백업 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-131">Azure backup offers hello option toonot only back up complete VMs, but also files and directories via hello backup agent, which has toobe installed on hello guest OS.</span></span> <span data-ttu-id="ebab6-132">하지만 2016 년 12 월을 기준으로이 에이전트는 Windows에서 지원만 (참조 [hello 리소스 관리자 배포 모델을 사용 하 여 Windows 서버 또는 클라이언트 tooAzure 백업](../../../backup/backup-configure-vault.md)).</span><span class="sxs-lookup"><span data-stu-id="ebab6-132">But as of December 2016, this agent is only supported on Windows (see [Back up a Windows Server or client tooAzure using hello Resource Manager deployment model](../../../backup/backup-configure-vault.md)).</span></span>

<span data-ttu-id="ebab6-133">해결 방법 toofirst 복사본이 예를 들어 (SAMBA 공유)를 통해 SAP HANA 백업 파일 tooa Azure에서 Windows VM 하 고 여기에서 hello Azure 백업 에이전트를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-133">A workaround is toofirst copy SAP HANA backup files tooa Windows VM on Azure (for example, via SAMBA share) and then use hello Azure backup agent from there.</span></span> <span data-ttu-id="ebab6-134">기술적으로 가능 하지만 것를 복잡성이 증가할 및 hello 백업 속도가 저하 하거나 복원 프로세스 hello Linux와 Windows VM hello 간의 toohello 복사 인해 크게 줄었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-134">While it is technically possible, it would add complexity and slow down hello backup or restore process quite a bit due toohello copy between hello Linux and hello Windows VM.</span></span> <span data-ttu-id="ebab6-135">없으면 toofollow이이 방법을 권장 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-135">It is not recommended toofollow this approach.</span></span>

## <a name="azure-blobxfer-utility-details"></a><span data-ttu-id="ebab6-136">Azure blobxfer 유틸리티 정보</span><span class="sxs-lookup"><span data-stu-id="ebab6-136">Azure blobxfer utility details</span></span>

<span data-ttu-id="ebab6-137">CLI 또는 PowerShell을 사용할 수 toostore 디렉터리와 Azure 저장소에 파일 hello 중 하나를 사용 하 여 도구를 개발 하거나 [Azure Sdk](https://azure.microsoft.com/downloads/)합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-137">toostore directories and files on Azure storage, one could use CLI or PowerShell, or develop a tool using one of hello [Azure SDKs](https://azure.microsoft.com/downloads/).</span></span> <span data-ttu-id="ebab6-138">데이터 tooAzure 저장소를 복사 하는 데 즉시 사용할 유틸리티, AzCopy를도 이지만 Windows만 되 고 (참조 [hello AzCopy 명령줄 유틸리티를 사용 하 여 데이터를 전송](../../../storage/common/storage-use-azcopy.md)).</span><span class="sxs-lookup"><span data-stu-id="ebab6-138">There is also a ready-to-use utility, AzCopy, for copying data tooAzure storage, but it is Windows only (see [Transfer data with hello AzCopy Command-Line Utility](../../../storage/common/storage-use-azcopy.md)).</span></span>

<span data-ttu-id="ebab6-139">따라서 blobxfer 유틸리티가 SAP HANA 백업 파일을 복사하는 데 사용되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-139">Therefore blobxfer was used for copying SAP HANA backup files.</span></span> <span data-ttu-id="ebab6-140">이 유틸리티는 프로덕션 환경에서 많은 고객이 사용하는 오픈 소스이며 [GitHub](https://github.com/Azure/blobxfer)에서 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-140">It is open source, used by many customers in production environments, and available on [GitHub](https://github.com/Azure/blobxfer).</span></span> <span data-ttu-id="ebab6-141">이 도구는 tooeither Azure blob 저장소 또는 Azure 파일 공유 직접 하나의 toocopy 데이터를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-141">This tool allows one toocopy data directly tooeither Azure blob storage or Azure file share.</span></span> <span data-ttu-id="ebab6-142">또한 여러 파일이 있는 디렉터리를 복사하는 경우 md5 해시 또는 자동 병렬 처리와 같은 유용한 기능을 다양하게 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-142">It also offers a range of useful features, like md5 hash or automatic parallelism when copying a directory with multiple files.</span></span>

## <a name="sap-hana-backup-performance"></a><span data-ttu-id="ebab6-143">SAP HANA 백업 성능</span><span class="sxs-lookup"><span data-stu-id="ebab6-143">SAP HANA backup performance</span></span>

![SAP HANA Studio hello SAP HANA 백업 콘솔의는이 스크린 샷](media/sap-hana-backup-file-level/image023.png)

<span data-ttu-id="ebab6-145">이 스크린 샷 hello SAP HANA Studio에서 SAP HANA 백업 콘솔입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-145">This screenshot is of hello SAP HANA backup console in SAP HANA Studio.</span></span> <span data-ttu-id="ebab6-146">약 42 분 toodo hello 백업 걸리는 toohello HANA VM을 연결 된 단일 Azure 표준 저장소 디스크에는 230 g B의 hello XFS 파일 시스템을 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-146">It took about 42 minutes toodo hello backup of hello 230 GB on a single Azure standard storage disk attached toohello HANA VM using XFS file system.</span></span>

![이 스크린 샷 YaST hello SAP HANA 테스트 VM에는](media/sap-hana-backup-file-level/image024.png)

<span data-ttu-id="ebab6-148">이 스크린 샷 hello SAP HANA 테스트 VM에 YaST입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-148">This screenshot is of YaST on hello SAP HANA test VM.</span></span> <span data-ttu-id="ebab6-149">앞에서 설명한 것 처럼 SAP HANA 백업에 대 한 hello 1TB 단일 디스크를 볼 수 하나.</span><span class="sxs-lookup"><span data-stu-id="ebab6-149">One can see hello 1-TB single disk for SAP HANA backup as mentioned before.</span></span> <span data-ttu-id="ebab6-150">약 42 분 toobackup 걸리는 230 GB입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-150">It took about 42 minutes toobackup 230 GB.</span></span> <span data-ttu-id="ebab6-151">또한 5개의 200GB 디스크가 연결되었고, 소프트웨어 RAID md0이 만들어졌으며, 이러한 5개의 Azure 데이터 디스크에 스트라이핑이 사용되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-151">In addition, five 200-GB disks were attached and software RAID md0 created, with striping on top of these five Azure data disks.</span></span>

![연결 된 표준 저장소를 Azure 데이터 디스크를 동일한 백업 소프트웨어 RAID에 스트라이프가 있는 5 간에 hello 반복](media/sap-hana-backup-file-level/image025.png)

<span data-ttu-id="ebab6-153">반복 hello 동일한 소프트웨어 RAID에 사용 하 여 백업을 스트라이프 5에서 too10 분 아래로 42 분에서 표준 저장소를 Azure 데이터 디스크 상태가 hello 백업 시간을 연결 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-153">Repeating hello same backup on software RAID with striping across five attached Azure standard storage data disks brought hello backup time from 42 minutes down too10 minutes.</span></span> <span data-ttu-id="ebab6-154">hello 디스크가 toohello VM을 캐시 하지 않고도 연결 된입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-154">hello disks were attached without caching toohello VM.</span></span> <span data-ttu-id="ebab6-155">따라서 것이 얼마나 중요 한지 디스크 쓰기 처리량이 확실 한 hello 백업 시간입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-155">So it is obvious how important disk write throughput is for hello backup time.</span></span> <span data-ttu-id="ebab6-156">하나에 다음 스위치 tooAzure 프리미엄 저장소 toofurther 최적의 성능을 위해 hello 프로세스를 가속화할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-156">One could then switch tooAzure premium storage toofurther accelerate hello process for optimal performance.</span></span> <span data-ttu-id="ebab6-157">일반적으로 프로덕션 시스템에는 Azure 프리미엄 저장소를 사용해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-157">In general, Azure premium storage should be used for production systems.</span></span>

## <a name="copy-sap-hana-backup-files-tooazure-blob-storage"></a><span data-ttu-id="ebab6-158">SAP HANA 백업 파일 tooAzure blob 저장소를 복사 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-158">Copy SAP HANA backup files tooAzure blob storage</span></span>

<span data-ttu-id="ebab6-159">2016 년 12 월의 hello 가장 처럼 옵션 tooquickly SAP HANA 백업 파일을 저장 Azure blob 저장소입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-159">As of December 2016, hello best option tooquickly store SAP HANA backup files is Azure blob storage.</span></span> <span data-ttu-id="ebab6-160">단일 blob 컨테이너 하나에 500tb를 Azure에 충분 한 SAP HANA 백업 tookeep GS5 VM에서 실행 중인 대부분의 SAP HANA 시스템에 충분 한 제한이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-160">One single blob container has a limit of 500 TB, enough for most SAP HANA systems, running in a GS5 VM on Azure, tookeep sufficient SAP HANA backups.</span></span> <span data-ttu-id="ebab6-161">고객의 여지가 hello 간의 &quot;핫&quot; 및 &quot;콜드&quot; blob 저장소 (참조 [Azure Blob 저장소: 핫 및 저장소 계층 냉각용](../../../storage/blobs/storage-blob-storage-tiers.md)).</span><span class="sxs-lookup"><span data-stu-id="ebab6-161">Customers have hello choice between &quot;hot&quot; and &quot;cold&quot; blob storage (see [Azure Blob Storage: Hot and cool storage tiers](../../../storage/blobs/storage-blob-storage-tiers.md)).</span></span>

<span data-ttu-id="ebab6-162">Hello blobxfer 도구 tooAzure blob 저장소에 직접 쉽게 toocopy hello SAP HANA에 대 한 백업 파일은 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-162">With hello blobxfer tool, it is easy toocopy hello SAP HANA backup files directly tooAzure blob storage.</span></span>

![여기서는 전체 SAP HANA 파일 백업의 hello 파일을 볼 수 하나](media/sap-hana-backup-file-level/image026.png)

<span data-ttu-id="ebab6-164">여기서는 전체 SAP HANA 파일 백업의 hello 파일을 볼 수 하나.</span><span class="sxs-lookup"><span data-stu-id="ebab6-164">Here one can see hello files of a full SAP HANA file backup.</span></span> <span data-ttu-id="ebab6-165">4 개의 파일 및 hello 가장 큰 옵션에 230 GB 약 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-165">There are four files and hello biggest one has roughly 230 GB.</span></span>

![3000 초 대략 toocopy hello 230 GB tooan 표준 Azure 저장소 계정 blob 컨테이너를 걸리는](media/sap-hana-backup-file-level/image027.png)

<span data-ttu-id="ebab6-167">Hello 초기 테스트의 md5 해시를 사용 하지, 걸린 3000 초 대략 toocopy hello 230 GB tooan 표준 Azure 저장소 계정 blob 컨테이너입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-167">Not using md5 hash in hello initial test, it took roughly 3000 seconds toocopy hello 230 GB tooan Azure standard storage account blob container.</span></span>

![이 스크린 샷에 하나 수 모양을 확인할 hello Azure 포털에서](media/sap-hana-backup-file-level/image028.png)

<span data-ttu-id="ebab6-169">이 스크린 샷에 hello Azure 포털에 나타나는 하나이 보입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-169">In this screenshot, one can see how it looks on hello Azure portal.</span></span> <span data-ttu-id="ebab6-170">라는 blob 컨테이너 &quot;sap-hana-백업을&quot; 만들어졌으며 hello SAP HANA 백업 파일을 나타내는 4 hello blob을 포함 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-170">A blob container named &quot;sap-hana-backups&quot; was created and includes hello four blobs, which represent hello SAP HANA backup files.</span></span> <span data-ttu-id="ebab6-171">그 중 하나는 약 230GB 크기입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-171">One of them has a size of roughly 230 GB.</span></span>

<span data-ttu-id="ebab6-172">hello HANA Studio 백업 콘솔 HANA 백업 파일의 한 toorestrict hello 최대 파일 크기를 허용 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-172">hello HANA Studio backup console allows one toorestrict hello max file size of HANA backup files.</span></span> <span data-ttu-id="ebab6-173">Hello 샘플 환경에서 하나의 큰 230 GB 파일 대신 작은 여러 백업 파일을 가능한 toohave 함으로써 성능이 향상 됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-173">In hello sample environment, it improved performance by making it possible toohave multiple smaller backup files, instead of one large 230-GB file.</span></span>

![Hello HANA 쪽 대상이 &#39; hello 백업 파일 크기 제한을 설정 t hello 백업 시간 개선](media/sap-hana-backup-file-level/image029.png)

<span data-ttu-id="ebab6-175">Hello HANA 쪽 대상이 &#39; hello 백업 파일 크기 제한을 설정 hello 파일은 다음이 그림에 표시 된 대로 순차적으로 기록 되기 때문에 t hello 백업 시간을 단축 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-175">Setting hello backup file size limit on hello HANA side doesn&#39;t improve hello backup time, because hello files are written sequentially as shown in this figure.</span></span> <span data-ttu-id="ebab6-176">hello 파일 크기 제한 때문에 hello 백업 hello 230 GB 대신 4 개의 큰 데이터 파일을 단일 파일 생성 too60 GB 설정 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-176">hello file size limit was set too60 GB, so hello backup created four large data files instead of hello 230-GB single file.</span></span>

![hello blobxfer 도구의 tootest 병렬 처리 수준, HANA 백업에 대 한 최대 파일 크기 hello 다음 설정한 too15 GB](media/sap-hana-backup-file-level/image030.png)

<span data-ttu-id="ebab6-178">hello blobxfer 도구의 tootest 병렬 처리 수준, HANA 백업에 대 한 최대 파일 크기 hello too15 GB 19 백업 파일은 다음 설정 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-178">tootest parallelism of hello blobxfer tool, hello max file size for HANA backups was then set too15 GB, which resulted in 19 backup files.</span></span> <span data-ttu-id="ebab6-179">이 구성은 too875 초 아래로 3000 초에서 blobxfer toocopy hello 230 GB tooAzure blob 저장소에 대 한 hello 시간으로 전환 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-179">This configuration brought hello time for blobxfer toocopy hello 230 GB tooAzure blob storage from 3000 seconds down too875 seconds.</span></span>

<span data-ttu-id="ebab6-180">이 결과 Azure blob를 작성 하기 위한 60MB/sec의 toohello 제한 때문입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-180">This result is due toohello limit of 60 MB/sec for writing an Azure blob.</span></span> <span data-ttu-id="ebab6-181">여러 blob 통해 병렬 처리 수준 hello 병목 상태가 해결 되지만 한 가지 단점이 있습니다: 많이 있는 새로운 hello blobxfer 도구 toocopy의 이러한 모든 HANA 백업 파일 tooAzure blob 저장소에서는 부하를 HANA VM hello와 hello 네트워크입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-181">Parallelism via multiple blobs solves hello bottleneck, but there is a downside: increasing performance of hello blobxfer tool toocopy all these HANA backup files tooAzure blob storage puts load on both hello HANA VM and hello network.</span></span> <span data-ttu-id="ebab6-182">HANA 시스템의 작업에 영향을 미칩니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-182">Operation of HANA system becomes impacted.</span></span>

## <a name="blob-copy-of-dedicated-azure-data-disks-in-backup-software-raid"></a><span data-ttu-id="ebab6-183">백업 소프트웨어 RAID에서 전용 Azure 데이터 디스크의 Blob 복사</span><span class="sxs-lookup"><span data-stu-id="ebab6-183">Blob copy of dedicated Azure data disks in backup software RAID</span></span>

<span data-ttu-id="ebab6-184">Hello 수동 VM 데이터 디스크 백업 달리 하나이 접근 방식에서 VM toosave hello 전체 SAP 설치에서 HANA 데이터를 비롯 한 모든 hello 데이터 디스크를 백업 HANA 로깅하지 않습니다 파일 및 config 파일.</span><span class="sxs-lookup"><span data-stu-id="ebab6-184">Unlike hello manual VM data disk backup, in this approach one does not back up all hello data disks on a VM toosave hello whole SAP installation, including HANA data, HANA log files, and config files.</span></span> <span data-ttu-id="ebab6-185">대신, hello 개념은 전용 toohave 소프트웨어 RAID 스트라이프가 있는 여러 Azure 데이터 Vhd에 걸쳐 전체 SAP HANA 파일 백업을 저장 하기 위한입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-185">Instead, hello idea is toohave dedicated software RAID with striping across multiple Azure data VHDs for storing a full SAP HANA file backup.</span></span> <span data-ttu-id="ebab6-186">하나에 이러한 디스크를 hello SAP HANA 백업을 복사 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-186">One copies only these disks, which have hello SAP HANA backup.</span></span> <span data-ttu-id="ebab6-187">전용 tooa 연결 또는 전용된 HANA 백업 저장소 계정에 유지할 수 쉽게은 &quot;VM 관리 백업&quot; 추가 처리를 위해 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-187">They could easily be kept in a dedicated HANA backup storage account, or attached tooa dedicated &quot;backup management VM&quot; for further processing.</span></span>

![관련 된 모든 Vhd hello를 사용 하 여 복사 된 * * 시작-azurestorageblobcopy * * PowerShell 명령](media/sap-hana-backup-file-level/image031.png)

<span data-ttu-id="ebab6-189">관련 된 모든 Vhd hello를 사용 하 여 복사 된 hello 백업 toohello 로컬 소프트웨어 RAID 완료 된 후 **시작 azurestorageblobcopy** PowerShell 명령 (참조 [시작 AzureStorageBlobCopy](/powershell/module/azure.storage/start-azurestorageblobcopy)).</span><span class="sxs-lookup"><span data-stu-id="ebab6-189">After hello backup toohello local software RAID was completed, all VHDs involved were copied using hello **start-azurestorageblobcopy** PowerShell command (see [Start-AzureStorageBlobCopy](/powershell/module/azure.storage/start-azurestorageblobcopy)).</span></span> <span data-ttu-id="ebab6-190">Hello 백업 파일을 유지 하는 데 전용 hello 파일 시스템에만 영향을 미칩니다를 hello 디스크에 SAP HANA 데이터 또는 로그 파일 일관성에 대 한 문제 없이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-190">As it only affects hello dedicated file system for keeping hello backup files, there are no concerns about SAP HANA data or log file consistency on hello disk.</span></span> <span data-ttu-id="ebab6-191">이 명령의 이점은 hello VM 온라인 유지 되는 동안 작동 한다는 점입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-191">A benefit of this command is that it works while hello VM stays online.</span></span> <span data-ttu-id="ebab6-192">특정 프로세스가 toohello 백업 스트라이프 세트를 씁니다 toobe 수 있는지 toounmount hello 하기 전에 blob 복사 하 고 나중에 다시 탑재 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-192">toobe certain that no process writes toohello backup stripe set, be sure toounmount it before hello blob copy, and mount it again afterwards.</span></span> <span data-ttu-id="ebab6-193">사용할 수 있는 적절 한 너무 또는&quot;고정&quot; hello 파일 시스템입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-193">Or one could use an appropriate way too&quot;freeze&quot; hello file system.</span></span> <span data-ttu-id="ebab6-194">예를 들어 xfs 통해\_hello XFS 파일 시스템에 대 한 고정 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-194">For example, via xfs\_freeze for hello XFS file system.</span></span>

![이 스크린샷은 hello Azure 포털에 hello vhd 컨테이너의 blob 목록을 hello를 보여 줍니다.](media/sap-hana-backup-file-level/image032.png)

<span data-ttu-id="ebab6-196">이 스크린샷은 hello의 hello blob 목록을 보여 줍니다. &quot;vhd&quot; hello Azure 포털에 대 한 컨테이너입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-196">This screenshot shows hello list of blobs in hello &quot;vhds&quot; container on hello Azure portal.</span></span> <span data-ttu-id="ebab6-197">hello 스크린샷은 hello 5 개의 Vhd hello 소프트웨어 RAID tookeep SAP HANA 백업 파일 toohello SAP HANA 서버 VM tooserve를 연결 된입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-197">hello screenshot shows hello five VHDs, which were attached toohello SAP HANA server VM tooserve as hello software RAID tookeep SAP HANA backup files.</span></span> <span data-ttu-id="ebab6-198">Hello hello blob 복사 명령을 통해 수행 된 복사본이 5 또한 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-198">It also shows hello five copies, which were taken via hello blob copy command.</span></span>

![테스트를 위해 hello SAP HANA 백업 소프트웨어 RAID 디스크의 hello 복사본 된 연결 된 toohello 응용 프로그램 서버 VM](media/sap-hana-backup-file-level/image033.png)

<span data-ttu-id="ebab6-200">테스트를 위해 hello SAP HANA 백업 소프트웨어 RAID 디스크의 hello 복사본에 연결 된 toohello 응용 프로그램 서버 VM 했습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-200">For testing purposes, hello copies of hello SAP HANA backup software RAID disks were attached toohello app server VM.</span></span>

![tooattach hello 디스크 복사본 hello 응용 프로그램 서버 VM 종료](media/sap-hana-backup-file-level/image034.png)

<span data-ttu-id="ebab6-202">hello 응용 프로그램 서버 VM tooattach hello 디스크 복사본 종료 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-202">hello app server VM was shut down tooattach hello disk copies.</span></span> <span data-ttu-id="ebab6-203">VM hello를 시작한 후 hello 개이며, 그 hello RAID 제대로 검색 되었습니다 (UUID을 통해 탑재).</span><span class="sxs-lookup"><span data-stu-id="ebab6-203">After starting hello VM, hello disks and hello RAID were discovered correctly (mounted via UUID).</span></span> <span data-ttu-id="ebab6-204">탑재 지점에만 hello hello YaST 파티 셔 너를 통해 만들어진, 누락 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-204">Only hello mount point was missing, which was created via hello YaST partitioner.</span></span> <span data-ttu-id="ebab6-205">나중에 SAP HANA 백업 파일 복사본을 hello 있게 OS 수준에 표시 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-205">Afterwards hello SAP HANA backup file copies became visible on OS level.</span></span>

## <a name="copy-sap-hana-backup-files-toonfs-share"></a><span data-ttu-id="ebab6-206">SAP HANA tooNFS 공유는 백업 파일을 복사</span><span class="sxs-lookup"><span data-stu-id="ebab6-206">Copy SAP HANA backup files tooNFS share</span></span>

<span data-ttu-id="ebab6-207">hello SAP HANA NFS 공유에 백업 파일을 저장 하는 방법을 고려할 수 toolessen hello에 영향을 줄 hello 성능 또는 디스크 공간 관점에서 SAP HANA 시스템을 하나.</span><span class="sxs-lookup"><span data-stu-id="ebab6-207">toolessen hello potential impact on hello SAP HANA system from a performance or disk space perspective, one might consider storing hello SAP HANA backup files on an NFS share.</span></span> <span data-ttu-id="ebab6-208">작동 기술적으로 하지만 두 번째 Azure VM을 사용 하 여 NFS 공유 hello의 hello 호스트로 것을 의미 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-208">Technically it works, but it means using a second Azure VM as hello host of hello NFS share.</span></span> <span data-ttu-id="ebab6-209">VM 네트워크 대역폭을 toohello 인해 작은 VM 크기, 되지 않아야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-209">It should not be a small VM size, due toohello VM network bandwidth.</span></span> <span data-ttu-id="ebab6-210">의미 다음이 다운 tooshut 하기가 &quot;VM 백업&quot; 만 hello SAP HANA 백업 실행에 대해 상태로 전환 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-210">It would make sense then tooshut down this &quot;backup VM&quot; and only bring it up for executing hello SAP HANA backup.</span></span> <span data-ttu-id="ebab6-211">공유 하면 hello 네트워크에 부하를 추가 및 영향 hello SAP HANA 시스템 하지만 hello에 나중에 백업 파일을 hello 단순히 관리 NFS 대 &quot;VM 백업&quot; 것에 영향을 주지 hello SAP HANA 시스템 전혀 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-211">Writing on an NFS share puts load on hello network and impacts hello SAP HANA system, but merely managing hello backup files afterwards on hello &quot;backup VM&quot; would not influence hello SAP HANA system at all.</span></span>

![Azure VM에서 NFS 공유 된 탑재 된 toohello SAP HANA 서버 VM](media/sap-hana-backup-file-level/image035.png)

<span data-ttu-id="ebab6-213">tooverify hello NFS 대/소문자를 사용 하 여, Azure VM에서 NFS 공유에 탑재 된 toohello SAP HANA 서버 VM 인지 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-213">tooverify hello NFS use case, an NFS share from another Azure VM was mounted toohello SAP HANA server VM.</span></span> <span data-ttu-id="ebab6-214">특별한 NFS 튜닝을 적용하지 않았습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-214">There was no special NFS tuning applied.</span></span>

![직접 toodo hello 백업 1 시간 46 분 소요](media/sap-hana-backup-file-level/image036.png)

<span data-ttu-id="ebab6-216">hello NFS 공유 hello SAP HANA 서버에 하나 hello와 같은 빠른 스트라이프 세트를 했습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-216">hello NFS share was a fast stripe set, like hello one on hello SAP HANA server.</span></span> <span data-ttu-id="ebab6-217">그럼에도 불구 하 고 걸린 1 시간 및 46 분 toodo hello 10 분이 아닌 hello NFS 공유에 직접 백업 tooa 로컬 스트라이프 세트를 작성할 때입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-217">Nevertheless, it took 1 hour and 46 minutes toodo hello backup directly on hello NFS share instead of 10 minutes, when writing tooa local stripe set.</span></span>

![hello 대안 1 시간 43 분에서 속도가 훨씬 빨라집니다 되지 않았습니다.](media/sap-hana-backup-file-level/image037.png)

<span data-ttu-id="ebab6-219">운영 체제 수준에서 백업 tooa 로컬 스트라이프 세트를 수행 하 고 toohello 복사 hello 다른 방법도 NFS 공유 (단순 **cp avr** 명령) 속도가 훨씬 빨라집니다 되지 않았습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-219">hello alternative of doing a backup tooa local stripe set and copying toohello NFS share on OS level (a simple **cp -avr** command) wasn't much quicker.</span></span> <span data-ttu-id="ebab6-220">1시간 43분이 걸렸습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-220">It took 1 hour and 43 minutes.</span></span>

<span data-ttu-id="ebab6-221">따라서 작동 되지만 성능 hello 230 GB 백업 테스트에 대 한 좋은 되지 않았습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-221">So it works, but performance wasn't good for hello 230-GB backup test.</span></span> <span data-ttu-id="ebab6-222">다중 테라바이트 크기로 작업할 경우에는 더욱 나쁠 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-222">It would look even worse for multi terabytes.</span></span>

## <a name="copy-sap-hana-backup-files-tooazure-file-service"></a><span data-ttu-id="ebab6-223">SAP HANA 백업 파일 tooAzure 파일 서비스를 복사 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-223">Copy SAP HANA backup files tooAzure file service</span></span>

<span data-ttu-id="ebab6-224">Azure 파일을 Azure Linux VM 내부 공유 가능 toomount 이며</span><span class="sxs-lookup"><span data-stu-id="ebab6-224">It is possible toomount an Azure file share inside an Azure Linux VM.</span></span> <span data-ttu-id="ebab6-225">hello 문서 [어떻게 toouse Azure 파일 저장소와 Linux](../../../storage/files/storage-how-to-use-files-linux.md) 방법에 자세히 설명 toodo 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-225">hello article [How toouse Azure File storage with Linux](../../../storage/files/storage-how-to-use-files-linux.md) provides details on how toodo it.</span></span> <span data-ttu-id="ebab6-226">현재 Azure 파일 공유당 5TB의 할당량 제한이 있으며, 파일당 파일 크기 제한은 1TB입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-226">Keep in mind that there is currently a 5-TB quota limit of one Azure file share, and a file size limit of 1 TB per file.</span></span> <span data-ttu-id="ebab6-227">저장소 제한에 대한 내용은 [Azure Storage 확장성 및 성능 목표](../../../storage/common/storage-scalability-targets.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebab6-227">See [Azure Storage Scalability and Performance Targets](../../../storage/common/storage-scalability-targets.md) for information on storage limits.</span></span>

<span data-ttu-id="ebab6-228">그러나 테스트한 결과 SAP HANA 백업이 현재 이러한 종류의 CIFS 탑재와 직접 작동하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-228">Tests have shown, however, that SAP HANA backup doesn&#39;t currently work directly with this kind of CIFS mount.</span></span> <span data-ttu-id="ebab6-229">또한 [SAP Note 1820529](https://launchpad.support.sap.com/#/notes/1820529)에서도 CIFS를 권장하지 않는다고 명시하고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-229">It is also stated in [SAP Note 1820529](https://launchpad.support.sap.com/#/notes/1820529) that CIFS is not recommended.</span></span>

![이 그림에서 SAP HANA Studio hello 백업 대화 상자에서 오류를 보여 줍니다.](media/sap-hana-backup-file-level/image038.png)

<span data-ttu-id="ebab6-231">이 그림 tooback 직접 tooa CIFS 탑재 Azure 파일 공유를 시도할 때 hello 백업 대화 상자에서 SAP HANA Studio에서 오류를 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-231">This figure shows an error in hello backup dialog in SAP HANA Studio, when trying tooback up directly tooa CIFS-mounted Azure file share.</span></span> <span data-ttu-id="ebab6-232">하나에 toodo VM 파일 시스템에는 표준 SAP HANA 백업이 첫째, 고 없어 tooAzure에서 복사 hello 백업 파일을 다음 파일 서비스입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-232">So one has toodo a standard SAP HANA backup into a VM file system first, and then copy hello backup files from there tooAzure file service.</span></span>

![이 그림은 929 초 toocopy 19 SAP HANA 백업 파일에 대 한 걸리는 보여 줍니다.](media/sap-hana-backup-file-level/image039.png)

<span data-ttu-id="ebab6-234">이 그림 걸린다는 929 초 toocopy에 대 한 대략 230 GB toohello Azure 파일 공유의 총 크기가 19 SAP HANA 백업 파일입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-234">This figure shows that it took about 929 seconds toocopy 19 SAP HANA backup files with a total size of roughly 230 GB toohello Azure file share.</span></span>

![SAP HANA VM hello에서 hello 소스 디렉터리 구조를 복사한 toohello Azure 파일 공유](media/sap-hana-backup-file-level/image040.png)

<span data-ttu-id="ebab6-236">이 스크린 샷에 하나 볼 수는 hello SAP HANA VM에서 hello 소스 디렉터리 구조가 복사한 toohello Azure 파일 공유: 디렉터리 (hana\_백업\_fsl\_15gb) 및 19 개별 백업 파일입니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-236">In this screenshot, one can see that hello source directory structure on hello SAP HANA VM was copied toohello Azure file share: one directory (hana\_backup\_fsl\_15gb) and 19 individual backup files.</span></span>

<span data-ttu-id="ebab6-237">SAP HANA Azure 파일에 백업 파일을 저장 SAP HANA 파일 백업을 지 원하는 경우 직접 hello 향후에 흥미로운 옵션 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-237">Storing SAP HANA backup files on Azure files could be an interesting option in hello future when SAP HANA file backups support it directly.</span></span> <span data-ttu-id="ebab6-238">또는 가능 하 게 될 때 toomount NFS를 통해 파일을 Azure hello 최대 할당량 한도 5TB 보다 훨씬 높습니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-238">Or when it becomes possible toomount Azure files via NFS and hello maximum quota limit is considerably higher than 5 TB.</span></span>

## <a name="next-steps"></a><span data-ttu-id="ebab6-239">다음 단계</span><span class="sxs-lookup"><span data-stu-id="ebab6-239">Next steps</span></span>
* <span data-ttu-id="ebab6-240">[Azure Virtual Machines의 SAP HANA 백업 가이드](sap-hana-backup-guide.md) - 시작에 대한 개요 및 정보를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-240">[Backup guide for SAP HANA on Azure Virtual Machines](sap-hana-backup-guide.md) gives an overview and information on getting started.</span></span>
* <span data-ttu-id="ebab6-241">[SAP HANA 백업 스냅숏을 기반으로 저장소](sap-hana-backup-storage-snapshots.md) hello 저장소 스냅숏 기반 백업 옵션에 설명 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-241">[SAP HANA backup based on storage snapshots](sap-hana-backup-storage-snapshots.md) describes hello storage snapshot-based backup option.</span></span>
* <span data-ttu-id="ebab6-242">tooestablish 고가용성 및 (대형 인스턴스) Azure에서 SAP HANA의 재해 복구 계획의 참조 toolearn [SAP HANA (대형 인스턴스) 고가용성 및 재해 복구 Azure에서](hana-overview-high-availability-disaster-recovery.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="ebab6-242">toolearn how tooestablish high availability and plan for disaster recovery of SAP HANA on Azure (large instances), see [SAP HANA (large instances) high availability and disaster recovery on Azure](hana-overview-high-availability-disaster-recovery.md).</span></span>
