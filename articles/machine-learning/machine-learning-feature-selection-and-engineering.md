---
title: "Azure Machine Learning의 기능 엔지니어링 및 선택 | Microsoft Docs"
description: "기능 선택 및 기능 엔지니어링의 목적을 설명하고 Machine Learning의 데이터 향상 프로세스에서 수행하는 역할의 예를 제공합니다."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 9ceb524d-842e-4f77-9eae-a18e599442d6
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/18/2017
ms.author: zhangya;bradsev
ROBOTS: NOINDEX
redirect_url: machine-learning-data-science-create-features
redirect_document_id: TRUE
ms.openlocfilehash: 51a5d8fed492cb9301e048c2b6a721e4573a47d9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 07/11/2017
---
# <a name="feature-engineering-and-selection-in-azure-machine-learning"></a><span data-ttu-id="4c532-103">Azure 기계 학습의 기능 엔지니어링 및 선택</span><span class="sxs-lookup"><span data-stu-id="4c532-103">Feature engineering and selection in Azure Machine Learning</span></span>
<span data-ttu-id="4c532-104">이 항목에서는 Machine Learning의 데이터 향상 프로세스에서 기능을 엔지니어링하고 기능을 선택하는 목적에 대해 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-104">This topic explains the purposes of feature engineering and feature selection in the data-enhancement process of machine learning.</span></span> <span data-ttu-id="4c532-105">Azure Machine Learning 스튜디오에서 제공하는 예제를 사용하여 이러한 프로세스와 관련된 내용을 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-105">It illustrates what these processes involve by using examples provided by Azure Machine Learning Studio.</span></span>

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="4c532-106">기계 학습에 사용되는 교육 데이터는 수집된 원시 데이터에서 기능을 선택하거나 추출하여 향상시킬 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-106">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</span></span> <span data-ttu-id="4c532-107">필기 문자의 이미지 분류 방법을 배우는 경우 엔지니어링된 기능의 예로는 원시 비트 분산 데이터에서 구성된 비트 밀도 맵이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-107">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit-density map constructed from the raw bit distribution data.</span></span> <span data-ttu-id="4c532-108">이 맵을 사용하면 원시 분산보다 효율적으로 문자의 경계를 찾을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-108">This map can help locate the edges of the characters more efficiently than the raw distribution.</span></span>

<span data-ttu-id="4c532-109">기능을 엔지니어링하고 선택하면 데이터에 포함된 주요 정보를 추출하려고 하는 학습 프로세스의 효율성이 증가됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-109">Engineered and selected features increase the efficiency of the training process, which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="4c532-110">이러한 모델이 입력 데이터를 정확하게 분류하고 원하는 결과를 더욱 안정적으로 예측하는 기능도 향상됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-110">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="4c532-111">컴퓨터를 통해 학습을 다루기가 더욱 쉽도록 기능 엔지니어링 및 선택을 결합할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-111">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="4c532-112">이 작업은 모델을 보정하거나 학습하는 데 필요한 기능을 향상시키고 해당 수를 줄여 수행합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-112">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="4c532-113">수학적인 관점에서 보자면 모델을 학습하기 위해 선택한 기능은 데이터의 패턴을 설명한 다음 결과를 성공적으로 예측하는 최소한의 독립 변수 집합입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-113">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="4c532-114">기능의 엔지니어링 및 선택은 일반적으로 다음 네 단계로 구성된 대규모 프로세스의 한 가지 부분입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-114">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</span></span>

* <span data-ttu-id="4c532-115">데이터 수집</span><span class="sxs-lookup"><span data-stu-id="4c532-115">Data collection</span></span>
* <span data-ttu-id="4c532-116">데이터 향상</span><span class="sxs-lookup"><span data-stu-id="4c532-116">Data enhancement</span></span>
* <span data-ttu-id="4c532-117">모델 생성</span><span class="sxs-lookup"><span data-stu-id="4c532-117">Model construction</span></span>
* <span data-ttu-id="4c532-118">후처리</span><span class="sxs-lookup"><span data-stu-id="4c532-118">Post-processing</span></span>

<span data-ttu-id="4c532-119">엔지니어링 및 선택은 기계 학습의 데이터 향상 단계를 구성합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-119">Engineering and selection make up the data enhancement step of machine learning.</span></span> <span data-ttu-id="4c532-120">이 프로세스의 세 가지 요소는 목적에 따라 다음과 같이 구별할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-120">Three aspects of this process may be distinguished for our purposes:</span></span>

* <span data-ttu-id="4c532-121">**데이터 사전 처리**: 이 프로세스에서는 수집된 데이터가 정리되어 있으며 일관성이 있는지 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-121">**Data pre-processing**: This process tries to ensure that the collected data is clean and consistent.</span></span> <span data-ttu-id="4c532-122">여러 데이터 집합 통합, 누락된 데이터 처리, 일관되지 않은 데이터 처리 및 데이터 유형 변환과 같은 작업이 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-122">It includes tasks such as integrating multiple data sets, handling missing data, handling inconsistent data, and converting data types.</span></span>
* <span data-ttu-id="4c532-123">**기능 엔지니어링**: 이 프로세스에서는 데이터의 기존 원시 기능에서 추가 관련 기능을 만들고 학습 알고리즘의 예측 능력을 향상시키려 합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-123">**Feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="4c532-124">**선택 기능**: 이 프로세스에서는 학습 문제의 차원 수를 줄이기 위해 원래 데이터 기능의 주요 하위 집합을 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-124">**Feature selection**: This process selects the key subset of original data features to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="4c532-125">이 항목에서는 데이터 향상 프로세스의 기능 엔지니어링 및 기능 선택 측면만 다룹니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-125">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</span></span> <span data-ttu-id="4c532-126">데이터 사전 처리 단계에 대한 자세한 정보는 [Azure Machine Learning 스튜디오의 데이터 사전 처리](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="4c532-126">For more information on the data pre-processing step, see [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/).</span></span>

## <a name="creating-features-from-your-data--feature-engineering"></a><span data-ttu-id="4c532-127">데이터에서 기능 만들기--기능 엔지니어링</span><span class="sxs-lookup"><span data-stu-id="4c532-127">Creating features from your data--feature engineering</span></span>
<span data-ttu-id="4c532-128">학습 데이터는 각각 기능 집합(열에 저장된 변수 또는 필드)이 있는 예제(행에 저장된 레코드 또는 관찰)로 구성된 행렬로 구성됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-128">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span></span> <span data-ttu-id="4c532-129">실험 디자인에 지정된 기능은 데이터에서 패턴의 특징을 나타내야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-129">The features specified in the experimental design are expected to characterize the patterns in the data.</span></span> <span data-ttu-id="4c532-130">많은 원시 데이터 필드를 모델을 학습하는 데 사용하는 선택된 기능 집합에 직접 포함할 수 있지만, 향상된 학습 데이터 집합을 생성하기 위해 원시 데이터의 기능에서 추가(엔지니어링된) 기능을 생성해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-130">Although many of the raw data fields can be directly included in the selected feature set used to train a model, additional engineered features often need to be constructed from the features in the raw data to generate an enhanced training data set.</span></span>

<span data-ttu-id="4c532-131">모델을 학습할 때 데이터 집합을 향상시키기 위해 작성해야 하는 기능의 유형은 무엇인가요?</span><span class="sxs-lookup"><span data-stu-id="4c532-131">What kind of features should be created to enhance the data set when training a model?</span></span> <span data-ttu-id="4c532-132">학습을 향상시키는 엔지니어링된 기능에서는 데이터에서 패턴을 더욱 잘 구분할 수 있는 정보를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-132">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span></span> <span data-ttu-id="4c532-133">새 기능에서는 원래 또는 기존 기능 집합에서는 명확하게 캡처하지 못하거나 쉽게 구분되지 않는 추가 정보를 제공해야 합니다. 그러나 이 프로세스는 정교한 작업입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-133">You expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set, but this process is something of an art.</span></span> <span data-ttu-id="4c532-134">안정되고 생산성이 있는 결정을 내리려면 도메인에 대한 전문 지식이 필요한 경우가 많습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-134">Sound and productive decisions often require some domain expertise.</span></span>

<span data-ttu-id="4c532-135">Azure Machine Learning을 시작할 때 Machine Learning 스튜디오에 제공된 샘플을 사용하면 이 프로세스를 구체적으로 파악하기가 쉽습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-135">When starting with Azure Machine Learning, it is easiest to grasp this process concretely by using samples provided in Machine Learning Studio.</span></span> <span data-ttu-id="4c532-136">다음은 제공되는 두 가지 예입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-136">Two examples are presented here:</span></span>

* <span data-ttu-id="4c532-137">대상 값이 알려진 감독된 실험에서의 회귀 예제 [자전거 대여 수 예측](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)</span><span class="sxs-lookup"><span data-stu-id="4c532-137">A regression example ([Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) in a supervised experiment where the target values are known</span></span>
* <span data-ttu-id="4c532-138">[기능 해싱][feature-hashing]을 사용하는 텍스트 마이닝 분류 예제</span><span class="sxs-lookup"><span data-stu-id="4c532-138">A text-mining classification example using [Feature Hashing][feature-hashing]</span></span>

### <a name="example-1-adding-temporal-features-for-a-regression-model"></a><span data-ttu-id="4c532-139">예 1: 회귀 모델을 위해 시간 기능 추가</span><span class="sxs-lookup"><span data-stu-id="4c532-139">Example 1: Adding temporal features for a regression model</span></span>
<span data-ttu-id="4c532-140">Azure Machine Learning Studio의 “자전거 수요 예측" 실험을 사용하여 회귀 작업을 위해 기능을 엔지니어링 하는 방법을 설명해 보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-140">To demonstrate how to engineer features for a regression task, let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio.</span></span> <span data-ttu-id="4c532-141">이 실험의 목표는 자전거 수요, 즉 특정 월,일 또는 시간에 자전거 대여 수를 예측하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-141">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month, day, or hour.</span></span> <span data-ttu-id="4c532-142">**자전거 대여 UCI 데이터 집합** 데이터 집합을 원시 입력 데이터로 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-142">The data set **Bike Rental UCI data set** is used as the raw input data.</span></span>

<span data-ttu-id="4c532-143">이 데이터 집합은 미국, 워싱턴 DC에서 자전거 임대망을 유지 관리하는 Capital Bikeshare 회사의 실제 데이터를 기반으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-143">This data set is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span></span> <span data-ttu-id="4c532-144">이 데이터 집합에는 2011년부터 2012년까지, 하루의 특정 시간 대에 자전거 대여 수가 표시되고, 17379 행과 17열이 포함되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-144">The data set represents the number of bike rentals within a specific hour of a day, from 2011 to 2012, and it contains 17379 rows and 17 columns.</span></span> <span data-ttu-id="4c532-145">원시 기능 집합에는 날씨 조건(온도, 습도, 풍속) 및 날의 유형(휴일 또는 주중)이 포함되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-145">The raw feature set contains weather conditions (temperature, humidity, wind speed) and the type of the day (holiday or weekday).</span></span> <span data-ttu-id="4c532-146">예측할 수 있는 필드는 **cnt**로서, 특정 시간 대의 자전거 대여 수를 나타내는 수이고, 범위는 1~977입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-146">The field to predict is **cnt**, a count that represents the bike rentals within a specific hour and that ranges from 1 to 977.</span></span>

<span data-ttu-id="4c532-147">학습 데이터에 효율적인 기능을 생성하려면 알고리즘은 동일하지만 네 개의 서로 다른 학습 데이터 집합이 있는 네 개의 회귀 모델을 빌드합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-147">To construct effective features in the training data, four regression models are built by using the same algorithm, but with four different training data sets.</span></span> <span data-ttu-id="4c532-148">네 개의 데이터 집합에서는 동일한 원시 입력 데이터를 표시하지만 기능 집합의 수는 증가합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-148">The four data sets represent the same raw input data, but with an increasing number of features set.</span></span> <span data-ttu-id="4c532-149">이러한 기능은 다음 네 가지 범주로 그룹화됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-149">These features are grouped into four categories:</span></span>

1. <span data-ttu-id="4c532-150">A = 예측 날에 대한 날씨 + 휴일 + 주중 + 주말 기능</span><span class="sxs-lookup"><span data-stu-id="4c532-150">A = weather + holiday + weekday + weekend features for the predicted day</span></span>
2. <span data-ttu-id="4c532-151">B = 지난 12시간마다 대여된 자전거 대수</span><span class="sxs-lookup"><span data-stu-id="4c532-151">B = number of bikes that were rented in each of the previous 12 hours</span></span>
3. <span data-ttu-id="4c532-152">C= 지난 12일마다 같은 시간에 대여된 자전거 대수</span><span class="sxs-lookup"><span data-stu-id="4c532-152">C = number of bikes that were rented in each of the previous 12 days at the same hour</span></span>
4. <span data-ttu-id="4c532-153">D = 지난 12주마다 같은 시간, 같은 요일에 대여된 자전거 대수</span><span class="sxs-lookup"><span data-stu-id="4c532-153">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span></span>

<span data-ttu-id="4c532-154">이미 원래 원시 데이터에 있던 기능 집합 A를 제외하고 나머지 세 개의 기능 집합은 기능 엔지니어링 프로세스를 통해 생성됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-154">Besides feature set A, which already exists in the original raw data, the other three sets of features are created through the feature engineering process.</span></span> <span data-ttu-id="4c532-155">기능 집합 B에서는 자전거의 최신 수요를 파악합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-155">Feature set B captures the recent demand for the bikes.</span></span> <span data-ttu-id="4c532-156">기능 집합 C에서는 특정 시간의 자전거 수요를 파악합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-156">Feature set C captures the demand for bikes at a particular hour.</span></span> <span data-ttu-id="4c532-157">기능 집합 D에서는 특정 시간 및 특정 요일에 자전거에 대한 수요를 파악합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-157">Feature set D captures demand for bikes at particular hour and particular day of the week.</span></span> <span data-ttu-id="4c532-158">네 개의 학습 데이터 집합 각각에는 A, A+B, A+B+C 및 A+B+C+D가 포함되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-158">Each of the four training data sets includes feature sets A, A+B, A+B+C, and A+B+C+D, respectively.</span></span>

<span data-ttu-id="4c532-159">Azure Machine Learning 실험에서는 사전 처리된 입력 데이터 집합에서 4개의 분기를 통해 이러한 4개의 학습 데이터 집합을 구성합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-159">In the Azure Machine Learning experiment, these four training data sets are formed via four branches from the pre-processed input data set.</span></span> <span data-ttu-id="4c532-160">가장 왼쪽 분기를 제외한 각 분기에는 [Execute R Script][execute-r-script] 모듈이 포함되어 있습니다. 이 모듈에는 파생 기능 집합(기능 집합 B, C 및 D)이 각각 구성되어 있고, 가져온 데이터 집합에 추가되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-160">Except for the leftmost branch, each of these branches contains an [Execute R Script][execute-r-script] module in which a set of derived features (feature sets B, C, and D) is respectively constructed and appended to the imported data set.</span></span> <span data-ttu-id="4c532-161">다음 그림에서는 두 번째 왼쪽 분기에서 기능 집합 B를 생성하는 데 사용하는 R 스크립트를 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-161">The following figure demonstrates the R script used to create feature set B in the second left branch.</span></span>

![기능 집합 만들기](./media/machine-learning-feature-selection-and-engineering/addFeature-Rscripts.png)

<span data-ttu-id="4c532-163">다음 테이블에는 네 가지 모델의 성능 결과의 비교가 요약되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-163">The following table summarizes the comparison of the performance results of the four models.</span></span> <span data-ttu-id="4c532-164">최상의 결과는 A+B+C 기능으로 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-164">The best results are shown by features A+B+C.</span></span> <span data-ttu-id="4c532-165">학습 데이터에 추가 기능 집합이 포함되면 오류 비율이 감소됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-165">Note that the error rate decreases when additional feature sets are included in the training data.</span></span> <span data-ttu-id="4c532-166">따라서 기능 집합 B와 C에서 회귀 작업을 위한 추가 관련 정보를 제공한다는 가정이 검증됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-166">This verifies our presumption that the feature sets B and C provide additional relevant information for the regression task.</span></span> <span data-ttu-id="4c532-167">D 기능 집합을 추가하면 오류 비율을 추가적으로 감소시키지 않는 것으로 보입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-167">Adding the D feature set does not seem to provide any additional reduction in the error rate.</span></span>

![성능 결과 비교](./media/machine-learning-feature-selection-and-engineering/result1.png)

### <span data-ttu-id="4c532-169"><a name="example2"></a> 예 2: 텍스트 마이닝에 기능 만들기</span><span class="sxs-lookup"><span data-stu-id="4c532-169"><a name="example2"></a> Example 2: Creating features in text mining</span></span>
<span data-ttu-id="4c532-170">기능 엔지니어링은 문서 분류 및 감성 분석 등의 텍스트 마이닝 관련 작업에 광범위하게 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-170">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span></span> <span data-ttu-id="4c532-171">예를 들어 문서를 여러 범주로 분류하려는 경우, 일반적으로 한 문서 범주에 포함된 단어 또는 문구가 다른 문서 범주에서 발생할 가능성이 적다고 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-171">For example, when you want to classify documents into several categories, a typical assumption is that the words or phrases included in one document category are less likely to occur in another document category.</span></span> <span data-ttu-id="4c532-172">즉, 단어 또는 문구 분포 빈도를 통해 서로 다른 문서 범주의 특징을 결정할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-172">In other words, the frequency of the word or phrase distribution is able to characterize different document categories.</span></span> <span data-ttu-id="4c532-173">텍스트 마이닝 응용 프로그램에서는 개별 텍스트 내용이 일반적으로 입력 데이터로 제공되므로, 단어 또는 문구 빈도와 관련된 기능을 생성하려면 기능 엔지니어링 프로세스가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-173">In text mining applications, the feature engineering process is needed to create the features involving word or phrase frequencies because individual pieces of text-contents usually serve as the input data.</span></span>

<span data-ttu-id="4c532-174">이 작업을 수행하기 위해 *기능 해싱*이라는 기술을 적용하여 임의의 텍스트 기능을 인덱스로 전환합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-174">To achieve this task, a technique called *feature hashing* is applied to efficiently turn arbitrary text features into indices.</span></span> <span data-ttu-id="4c532-175">각 텍스트 기능(단어 또는 문구)을 특정 인덱스에 연관시키는 대신, 이 메서드에서는 해시 함수를 기능에 적용하고 해시 값을 인덱스로 직접 사용하여 작동합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-175">Instead of associating each text feature (words or phrases) to a particular index, this method functions by applying a hash function to the features and by using their hash values as indices directly.</span></span>

<span data-ttu-id="4c532-176">Azure Machine Learning에는 이러한 단어 또는 문구 기능을 편리하게 생성하는 [기능 해싱][feature-hashing] 모듈이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-176">In Azure Machine Learning, there is a [Feature Hashing][feature-hashing] module that creates these word or phrase features.</span></span> <span data-ttu-id="4c532-177">다음 그림에서는 이 모듈을 사용하는 예를 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-177">The following figure shows an example of using this module.</span></span> <span data-ttu-id="4c532-178">입력 데이터 집합에는 두 개의 열, 즉 1~5의 서적 등급과 실제 검토 내용이 들어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-178">The input data set contains two columns: the book rating ranging from 1 to 5 and the actual review content.</span></span> <span data-ttu-id="4c532-179">이 [기능 해싱][feature-hashing] 모듈의 목표는 특정 서적 검토에서 해당하는 단어 또는 문구의 발생 빈도를 표시하는 여러 새 기능을 검색하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-179">The goal of this [Feature Hashing][feature-hashing] module is to retrieve new features that show the occurrence frequency of the corresponding words or phrases within the particular book review.</span></span> <span data-ttu-id="4c532-180">이 모듈을 사용하려면 다음 단계를 완료해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-180">To use this module, you need to complete the following steps:</span></span>

1. <span data-ttu-id="4c532-181">입력 텍스트를 포함하는 열을 선택합니다(이 예에서는 **Col2**).</span><span class="sxs-lookup"><span data-stu-id="4c532-181">Select the column that contains the input text (**Col2** in this example).</span></span>
2. <span data-ttu-id="4c532-182">*Hashing bitsize*를 8로 설정합니다. 즉, 2^8=256개의 기능이 생성됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-182">Set *Hashing bitsize* to 8, which means 2^8=256 features are created.</span></span> <span data-ttu-id="4c532-183">텍스트의 단어 또는 문구가 256개의 인덱스로 해시됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-183">The word or phrase in the text is then hashed to 256 indices.</span></span> <span data-ttu-id="4c532-184">*Hashing bitsize* 매개 변수의 범위는 1~31입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-184">The parameter *Hashing bitsize* ranges from 1 to 31.</span></span> <span data-ttu-id="4c532-185">매개 변수를 큰 숫자로 설정한 경우 단어 또는 구는 동일한 인덱스에 해시될 가능성이 적습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-185">If the parameter is set to a larger number, the words or phrases are less likely to be hashed into the same index.</span></span>
3. <span data-ttu-id="4c532-186">*N-grams* 매개 변수를 2로 설정합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-186">Set the parameter *N-grams* to 2.</span></span> <span data-ttu-id="4c532-187">그러면 입력 텍스트에서 유니그램(단일 단어마다 하나의 기능) 및 바이그램(인접한 단어 쌍마다 하나의 기능) 발생 빈도를 가져옵니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-187">This retrieves the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span></span> <span data-ttu-id="4c532-188">*N-grams* 매개 변수의 범위는 0~10입니다. 즉, 순차 단어의 최대 수가 기능에 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-188">The parameter *N-grams* ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span></span>  

![기능 해싱 모듈](./media/machine-learning-feature-selection-and-engineering/feature-Hashing1.png)

<span data-ttu-id="4c532-190">다음 그림에서는 이러한 새 기능이 어떻게 표시되는지 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-190">The following figure shows what these new features look like.</span></span>

![기능 해싱 예제](./media/machine-learning-feature-selection-and-engineering/feature-Hashing2.png)

## <a name="filtering-features-from-your-data--feature-selection"></a><span data-ttu-id="4c532-192">데이터에서 기능 필터링--기능 선택</span><span class="sxs-lookup"><span data-stu-id="4c532-192">Filtering features from your data--feature selection</span></span>
<span data-ttu-id="4c532-193">*기능 선택*은 분류 또는 회귀 작업과 같은 예측 모델링 작업의 학습 데이터 집합을 생성하는 데 일반적으로 적용되는 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-193">*Feature selection* is a process that is commonly applied to the construction of training data sets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="4c532-194">최소한의 기능 집합을 사용하여 데이터의 최대 분산 크기를 표시함으로써 차원수를 줄이는 원래 데이터 집합의 하위 집합을 선택하기 위해 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-194">The goal is to select a subset of the features from the original data set that reduces its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="4c532-195">이 기능 하위 집합은 모델을 학습하기 위해 포함되는 기능만 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-195">This subset of features contains the only features to be included to train the model.</span></span> <span data-ttu-id="4c532-196">기능 선택은 두 가지 기본 용도로 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-196">Feature selection serves two main purposes:</span></span>

* <span data-ttu-id="4c532-197">기능 선택을 수행하면 관련이 없는 중복된 기능이나 고도로 상관된 기능을 제거하여 분류 정확도를 높입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-197">Feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="4c532-198">기능 선택은 기능 수를 줄여서 모델 학습 프로세스의 효율성을 높입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-198">Feature selection decreases the number of features, which makes the model training process more efficient.</span></span> <span data-ttu-id="4c532-199">지원 벡터 컴퓨터와 같이 학습하는 데 비용이 많이 드는 학습자에게 특히 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-199">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="4c532-200">기능 선택에서 모델을 학습시키는 데 사용하는 데이터 집합의 기능 수를 줄이려고 하지만, 일반적으로 *차원 수 감소*라는 용어로 부르지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-200">Although feature selection seeks to reduce the number of features in the data set used to train the model, it is not usually referred to by the term *dimensionality reduction.*</span></span> <span data-ttu-id="4c532-201">기능 선택 메서드에서는 원래 기능을 변경하지 않고 데이터에서 원래 기능의 하위 집합을 추출합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-201">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="4c532-202">차원 수 감소 메서드에서는 원래 기능을 변환하므로 기능을 수정할 수 있는 엔지니어링된 기능을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-202">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="4c532-203">차원 수 감소 메서드의 예로는 주성분 분석, 표준 상관 분석 및 특이값 분해가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-203">Examples of dimensionality reduction methods include principal component analysis, canonical correlation analysis, and singular value decomposition.</span></span>

<span data-ttu-id="4c532-204">감독된 컨텍스트에서 가장 널리 적용되는 기능 선택 메서드 범주는 필터 기반 기능 선택입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-204">One widely applied category of feature selection methods in a supervised context is filter-based feature selection.</span></span> <span data-ttu-id="4c532-205">이러한 메서드에서는 각 기능과 대상 특성 사이의 상관 관계를 평가하여, 각 기능에 점수를 할당하기 위해 통계 측정값을 적용합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-205">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="4c532-206">그런 다음 특정 기능을 보유하거나 제거하는 임계값을 설정하는 데 사용할 수 있는 점수별로 기능에 순위가 지정됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-206">The features are then ranked by the score, which you can use to set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="4c532-207">이러한 메서드에서 사용하는 통계 측정값의 예로는 Pearson 상관, 상호 정보 및 카이 제곱 테스트가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-207">Examples of the statistical measures used in these methods include Pearson Correlation, mutual information, and the Chi-squared test.</span></span>

<span data-ttu-id="4c532-208">Azure Machine Learning 스튜디오에서는 기능 선택의 모듈을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-208">Azure Machine Learning Studio provides modules for feature selection.</span></span> <span data-ttu-id="4c532-209">다음 그림에 표시된 대로 이러한 모듈에는 [필터 기반 기능 선택][filter-based-feature-selection] 및 [피셔 선형 판별식 분석][fisher-linear-discriminant-analysis]이 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-209">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![기능 선택 예](./media/machine-learning-feature-selection-and-engineering/feature-Selection.png)

<span data-ttu-id="4c532-211">예를 들어 앞부분에 설명한 텍스트 마이닝 예제로 [필터 기반 기능 선택][filter-based-feature-selection] 모듈을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-211">For example, use the [Filter-Based Feature Selection][filter-based-feature-selection] module with the text mining example outlined previously.</span></span> <span data-ttu-id="4c532-212">[기능 해싱][feature-hashing] 모듈을 통해 256개의 기능 집합을 생성한 후 회귀 모델을 빌드하려고 하며, 응답 변수는 **Col1**이고 1~ 5 범위의 서적 검토 등급을 나타낸다고 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-212">Assume that you want to build a regression model after a set of 256 features is created through the [Feature Hashing][feature-hashing] module, and that the response variable is **Col1** and represents a book review rating ranging from 1 to 5.</span></span> <span data-ttu-id="4c532-213">**기능 점수 매기기 메서드**를 **Pearson 상관**으로 설정하고 **대상 열**은 **Col1**로 설정하며 **원하는 기능 수**는 **50**으로 설정합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-213">Set **Feature scoring method** to **Pearson Correlation**, **Target column** to **Col1**, and **Number of desired features** to **50**.</span></span> <span data-ttu-id="4c532-214">그러면 [필터 기반 기능 선택][filter-based-feature-selection] 모듈에서 대상 특성이 **Col1**과 함께 50개의 기능이 포함된 데이터 집합을 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-214">The module [Filter-Based Feature Selection][filter-based-feature-selection] then produces a data set containing 50 features together with the target attribute **Col1**.</span></span> <span data-ttu-id="4c532-215">다음 그림에서는 입력 매개 변수와 이 실험의 흐름을 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-215">The following figure shows the flow of this experiment and the input parameters.</span></span>

![기능 선택 예](./media/machine-learning-feature-selection-and-engineering/feature-Selection1.png)

<span data-ttu-id="4c532-217">다음 그림에서는 결과 데이터 집합을 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-217">The following figure shows the resulting data sets.</span></span> <span data-ttu-id="4c532-218">각 기능과 대상 특성 **Col1** 사이의 Pearson 상관 관계에 따라 기능의 점수를 매깁니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-218">Each feature is scored based on the Pearson Correlation between itself and the target attribute **Col1**.</span></span> <span data-ttu-id="4c532-219">점수가 가장 높은 기능이 유지됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-219">The features with top scores are kept.</span></span>

![필터 기반 기능 선택 데이터 집합](./media/machine-learning-feature-selection-and-engineering/feature-Selection2.png)

<span data-ttu-id="4c532-221">다음 그림에서는 선택한 기능의 해당 점수를 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-221">The following figure shows the corresponding scores of the selected features.</span></span>

![선택한 기능 점수](./media/machine-learning-feature-selection-and-engineering/feature-Selection3.png)

<span data-ttu-id="4c532-223">이 [필터 기반 기능 선택][filter-based-feature-selection] 모듈을 적용하면, 256개의 기능 중 50개가 선택됩니다. 이 50개에는 **Pearson 상관** 점수 매기기 메서드에 따라 대상 변수 **Col1**과 상관 관계가 있는 대부분의 기능이 있기 때문입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-223">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most features correlated with the target variable **Col1** based on the scoring method **Pearson Correlation**.</span></span>

## <a name="conclusion"></a><span data-ttu-id="4c532-224">결론</span><span class="sxs-lookup"><span data-stu-id="4c532-224">Conclusion</span></span>
<span data-ttu-id="4c532-225">기능 엔지니어링과 기능 선택은 Machine Learning 모델을 빌드할 때 학습 데이터를 준비하기 위해 일반적으로 수행되는 두 가지 단계입니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-225">Feature engineering and feature selection are two steps commonly performed to prepare the training data when building a machine learning model.</span></span> <span data-ttu-id="4c532-226">일반적으로 추가 기능을 생성하기 위해 기능 엔지니어링을 먼저 적용한 다음, 관련이 없는 중복 기능이나 고도로 상관된 기능을 제거하기 위해 기능 선택 단계가 수행됩니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-226">Normally, feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

<span data-ttu-id="4c532-227">기능 엔지니어링이나 기능 선택을 반드시 항상 수행할 필요는 없습니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-227">It is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="4c532-228">이러한 기능의 필요 여부는 보유하거나 수집한 데이터, 선택한 알고리즘 및 실험 목적에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="4c532-228">Whether it is needed depends on the data you have or collect, the algorithm you pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[execute-r-script]: https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/
