---
title: "hello 팀 데이터 과학 프로세스의에서 aaaFeature 선택 | Microsoft Docs"
description: "기능 선택의 hello 용도 설명 하 고 기계 학습의 hello 데이터 향상 프로세스에서 해당 역할의 예를 제공 합니다."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 878541f5-1df8-4368-889a-ced6852aba47
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: 54af93c83e4cc6a3670b3ad62490e0f74082b4ee
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="feature-selection-in-hello-team-data-science-process-tdsp"></a><span data-ttu-id="57d82-103">Hello 팀 데이터 과학 프로세스 (TDSP)의 기능 선택</span><span class="sxs-lookup"><span data-stu-id="57d82-103">Feature selection in hello Team Data Science Process (TDSP)</span></span>
<span data-ttu-id="57d82-104">이 문서는 기능 선택의 hello 목적에 설명 하 고 기계 학습의 hello 데이터 향상 프로세스에서 해당 역할의 예제를 제공 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-104">This article explains hello purposes of feature selection and provides examples of its role in hello data enhancement process of machine learning.</span></span> <span data-ttu-id="57d82-105">이들 예는 Azure 기계 학습 스튜디오에서 가져온 것입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-105">These examples are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="57d82-106">기능 선택은 hello 팀 데이터 과학 프로세스 (TDSP)이 설명에 한 부분 및 엔지니어링 hello [hello 팀 데이터 과학 프로세스는 무엇입니까?](data-science-process-overview.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-106">hello engineering and selection of features is one part of hello Team Data Science Process (TDSP) outlined in [What is hello Team Data Science Process?](data-science-process-overview.md).</span></span> <span data-ttu-id="57d82-107">기능 엔지니어링 및 선택 hello의 일부는 **기능 개발** hello TDSP의 단계입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-107">Feature engineering and selection are parts of hello **Develop features** step of hello TDSP.</span></span>

* <span data-ttu-id="57d82-108">**엔지니어링 기능**:이 프로세스에서 hello 데이터 및 tooincrease 예측 력이 toohello 학습 알고리즘의 기존 원시 기능 hello toocreate 추가 관련 기능을 시도 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-108">**feature engineering**: This process attempts toocreate additional relevant features from hello existing raw features in hello data, and tooincrease predictive power toohello learning algorithm.</span></span>
* <span data-ttu-id="57d82-109">**기능 선택**:이 프로세스 hello 학습 문제의 시도 tooreduce hello 차원에서 원래 데이터 기능의 hello 키 하위 집합을 선택 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-109">**feature selection**: This process selects hello key subset of original data features in an attempt tooreduce hello dimensionality of hello training problem.</span></span>

<span data-ttu-id="57d82-110">일반적으로 **엔지니어링 기능** 적용 된 첫 번째 toogenerate 추가 기능을 고 hello **기능 선택** 단계는 수행된 된 tooeliminate 관련이 없는, 중복 또는 밀접 기능입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-110">Normally **feature engineering** is applied first toogenerate additional features, and then hello **feature selection** step is performed tooeliminate irrelevant, redundant, or highly correlated features.</span></span>

## <a name="filtering-features-from-your-data---feature-selection"></a><span data-ttu-id="57d82-111">데이터에서 기능 필터링 - 기능 선택</span><span class="sxs-lookup"><span data-stu-id="57d82-111">Filtering Features from Your Data - Feature Selection</span></span>
<span data-ttu-id="57d82-112">기능 선택은 일반적으로 분류 또는 회귀 작업 같은 예측 모델링 태스크에 대 한 학습 데이터 집합의 hello 구축을 위한 적용 되는 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-112">Feature selection is a process that is commonly applied for hello construction of training datasets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="57d82-113">hello 목표 tooselect hello 데이터에서 기능 toorepresent hello 분산의 최대 크기의 최소 집합을 사용 하 여 크기를 축소 하는 hello 원래 데이터 집합에서 hello 기능 하위 집합입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-113">hello goal is tooselect a subset of hello features from hello original dataset that reduce its dimensions by using a minimal set of features toorepresent hello maximum amount of variance in hello data.</span></span> <span data-ttu-id="57d82-114">이 기능의 하위이 집합은 다음 hello 유일한 기능 toobe tootrain hello 모델을 포함 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-114">This subset of features are, then, hello only features toobe included tootrain hello model.</span></span> <span data-ttu-id="57d82-115">기능 선택은 두 가지 기본 용도로 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-115">Feature selection serves two main purposes.</span></span>

* <span data-ttu-id="57d82-116">첫째 기능 선택을 수행하면 관련이 없는 중복된 기능이나 고도로 상관된 기능을 제거하여 분류 정확도를 높입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-116">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="57d82-117">둘째, 모델 학습 프로세스를 보다 효율적으로 수 있는 기능의 hello 수를 줄입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-117">Second, it decreases hello number of features which makes model training process more efficient.</span></span> <span data-ttu-id="57d82-118">지원 벡터 컴퓨터 같은 비용이 많이 드는 tootrain 않은 학습자에 특히 유용 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-118">This is particularly important for learners that are expensive tootrain such as support vector machines.</span></span>

<span data-ttu-id="57d82-119">기능 선택 hello 사용 된 데이터 집합 tootrain hello 모델의 기능 tooreduce hello 수 찾기지 않습니다 이지만 하지 일반적으로 hello 용어 "차원 감소" tooby 참조 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-119">Although feature selection does seek tooreduce hello number of features in hello dataset used tootrain hello model, it is not usually referred tooby hello term "dimensionality reduction".</span></span> <span data-ttu-id="57d82-120">기능 선택 방법을 변경 하지 않은 hello 데이터의 원래 기능 하위 집합을 추출 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-120">Feature selection methods extract a subset of original features in hello data without changing them.</span></span>  <span data-ttu-id="57d82-121">차원성 감소 방법을 hello 초기 기능을 변환 하 고 있으므로 수정할 수 있는 엔지니어링된 기능을 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-121">Dimensionality reduction methods employ engineered features that can transform hello original features and thus modify them.</span></span> <span data-ttu-id="57d82-122">차원 수 감소 메서드의 예로는 주성분 분석, 표준 상관 분석 및 특이값 분해가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-122">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</span></span>

<span data-ttu-id="57d82-123">그중에서도 감독된 컨텍스트에서 가장 널리 적용되는 기능 선택 메서드 범주는 “필터 기반 기능 선택"이라고 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-123">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</span></span> <span data-ttu-id="57d82-124">각 기능 및 hello 대상 특성 간의 hello 상관 관계를 평가 하 여 이러한 메서드는 통계 측정값 tooassign 점수 tooeach 기능을 적용 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-124">By evaluating hello correlation between each feature and hello target attribute, these methods apply a statistical measure tooassign a score tooeach feature.</span></span> <span data-ttu-id="57d82-125">hello 기능에서는 다음 유지 하거나 특정 기능 제거에 대 한 사용된 toohelp 집합 hello 임계값 수 있는 hello 점수에 의해 순위를 지정 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-125">hello features are then ranked by hello score, which may be used toohelp set hello threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="57d82-126">이러한 방법에 사용 된 hello 통계 측정값의 예로 사용자 상관 관계, 상호 정보 및 카이 제곱된 테스트 hello 들 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-126">Examples of hello statistical measures used in these methods include Person correlation, mutual information, and hello Chi squared test.</span></span>

<span data-ttu-id="57d82-127">Azure 기계 학습 스튜디오에서는 기능 선택에 제공되는 모듈이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-127">In Azure Machine Learning Studio, there are modules provided for feature selection.</span></span> <span data-ttu-id="57d82-128">이러한 모듈에는 hello 다음 그림에에서 나와 있는 것 처럼 [필터 기반 기능 선택] [ filter-based-feature-selection] 및 [피셔 선형 판별 분석] [ fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="57d82-128">As shown in hello following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![기능 선택 예](./media/machine-learning-data-science-select-features/feature-Selection.png)

<span data-ttu-id="57d82-130">예를 들어, hello hello 사용 [필터 기반 기능 선택] [ filter-based-feature-selection] 모듈입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-130">Consider, for example, hello use of hello [Filter-Based Feature Selection][filter-based-feature-selection] module.</span></span> <span data-ttu-id="57d82-131">Hello 편의 위해 위에서 설명한 toouse hello 텍스트 마이닝 예제를 계속 실행 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-131">For hello purpose of convenience, we continue toouse hello text mining example outlined above.</span></span> <span data-ttu-id="57d82-132">회귀 모델 256 기능 집합을 이후 hello 통해 만드는 toobuild 한다고 가정 [기능 해시] [ feature-hashing] 모듈 및 해당 hello 응답 변수는 "Col1" hello 및 책을 나타냅니다. 1 too5에서 사이인 등급을 검토 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-132">Assume that we want toobuild a regression model after a set of 256 features are created through hello [Feature Hashing][feature-hashing] module, and that hello response variable is hello "Col1" and represents a book review ratings ranging from 1 too5.</span></span> <span data-ttu-id="57d82-133">"기능 점수 매기기 방법" toobe "피어슨 상관 관계"를 설정 하 여 "대상 열" toobe "Col1" 및 "원하는 기능의 수" too50 hello hello 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-133">By setting "Feature scoring method" toobe "Pearson Correlation", hello "Target column" toobe "Col1", and hello "Number of desired features" too50.</span></span> <span data-ttu-id="57d82-134">다음 hello 모듈 [필터 기반 기능 선택] [ filter-based-feature-selection] hello 대상 특성 "Col1"와 함께 50 기능을 포함 하는 데이터 집합을 생성 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-134">Then hello module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with hello target attribute "Col1".</span></span> <span data-ttu-id="57d82-135">hello 다음이 실험에서는 hello 흐름 알아보고 hello 지금까지 언급 된 입력된 매개 변수.</span><span class="sxs-lookup"><span data-stu-id="57d82-135">hello following figure shows hello flow of this experiment and hello input parameters we just described.</span></span>

![기능 선택 예](./media/machine-learning-data-science-select-features/feature-Selection1.png)

<span data-ttu-id="57d82-137">hello 다음 그림은 hello 결과 데이터 집합.</span><span class="sxs-lookup"><span data-stu-id="57d82-137">hello following figure shows hello resulting datasets.</span></span> <span data-ttu-id="57d82-138">각 기능은 점수를 기준으로 hello 자체 간의 피어슨 상관 관계에 있으며 대상 특성 "Col1" hello 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-138">Each feature is scored based on hello Pearson Correlation between itself and hello target attribute "Col1".</span></span> <span data-ttu-id="57d82-139">최고 점수와 hello 기능이 유지 됩니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-139">hello features with top scores are kept.</span></span>

![기능 선택 예](./media/machine-learning-data-science-select-features/feature-Selection2.png)

<span data-ttu-id="57d82-141">선택한 hello 기능의 hello 해당 점수는 hello 다음 그림에에서 표시 됩니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-141">hello corresponding scores of hello selected features are shown in hello following figure.</span></span>

![기능 선택 예](./media/machine-learning-data-science-select-features/feature-Selection3.png)

<span data-ttu-id="57d82-143">이 적용 하 여 [필터 기반 기능 선택] [ filter-based-feature-selection] 모듈을 50의 256 hello 대상 변수로 "Col1" 상관 관계가 지정 된 기능에 가장를 hello 있어야 하기 때문에 기능을 선택한 hello 점수 매기기에 따라 "피어슨 상관 관계" 메서드입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-143">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have hello most correlated features with hello target variable "Col1", based on hello scoring method "Pearson Correlation".</span></span>

## <a name="conclusion"></a><span data-ttu-id="57d82-144">결론</span><span class="sxs-lookup"><span data-stu-id="57d82-144">Conclusion</span></span>
<span data-ttu-id="57d82-145">기능 엔지니어링 및 기능 선택은 일반적으로 엔지니어링 및 선택한 기능 hello 데이터에 포함 된 tooextract hello 키 정보를 시도 하는 프로세스를 학습 하는 hello의 hello 효율성을 높입니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-145">Feature engineering and feature selection are two commonly Engineered and selected features increase hello efficiency of hello training process which attempts tooextract hello key information contained in hello data.</span></span> <span data-ttu-id="57d82-146">이러한 모델 tooclassify hello 입력된 데이터의 hello 거듭제곱을 정확 하 게 향상 시킬 및 toopredict 결과 관심 더 강력 하 게 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-146">They also improve hello power of these models tooclassify hello input data accurately and toopredict outcomes of interest more robustly.</span></span> <span data-ttu-id="57d82-147">기능 엔지니어링 및 선택 toomake hello 학습 더 계산 tractable 결합할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-147">Feature engineering and selection can also combine toomake hello learning more computationally tractable.</span></span> <span data-ttu-id="57d82-148">이 작업을 수행 하 여 향상 하 고 toocalibrate 또는 모델을 학습 필요 hello 기능 수를 줄이면 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-148">It does so by enhancing and then reducing hello number of features needed toocalibrate or train a model.</span></span> <span data-ttu-id="57d82-149">Hello 기능 선택한 tootrain hello 모델에 최소한의 hello 데이터의 hello 패턴을 설명 하 고 다음 결과 성공적으로 예측 하는 독립 변수 집합은 수학적으로 말하자면, 있습니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-149">Mathematically speaking, hello features selected tootrain hello model are a minimal set of independent variables that explain hello patterns in hello data and then predict outcomes successfully.</span></span>

<span data-ttu-id="57d82-150">Note 아닌지 항상 반드시 tooperform 기능 엔지니어링 이나 기능 선택 합니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-150">Note that it is not always necessarily tooperform feature engineering or feature selection.</span></span> <span data-ttu-id="57d82-151">또는 필요한 지 여부 또는 수집 hello 알고리즘을 선택 했으며 hello 실험의 hello hello 데이터에 따라 달라 집니다.</span><span class="sxs-lookup"><span data-stu-id="57d82-151">Whether it is needed or not depends on hello data we have or collect, hello algorithm we pick, and hello objective of hello experiment.</span></span>

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/

