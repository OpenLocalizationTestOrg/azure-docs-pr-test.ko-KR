---
title: "aaaHow toochoose 기계 학습 알고리즘 | Microsoft Docs"
description: "어떻게 클러스터링 감독 및 자율 학습을 위한 Azure 기계 학습 알고리즘 toochoose, 분류 또는 회귀 실험 합니다."
services: machine-learning
documentationcenter: 
author: garyericson
manager: jhubbard
editor: cgronlun
tags: 
ms.assetid: a3b23d7f-f083-49c4-b6b1-3911cd69f1b4
ms.service: machine-learning
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: data-services
ms.date: 04/25/2017
ms.author: garye
ms.openlocfilehash: 367b2278acc2435f27f9d24ead8199db58aca283
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="how-toochoose-algorithms-for-microsoft-azure-machine-learning"></a><span data-ttu-id="f9536-103">어떻게 toochoose Microsoft Azure 기계 학습 알고리즘</span><span class="sxs-lookup"><span data-stu-id="f9536-103">How toochoose algorithms for Microsoft Azure Machine Learning</span></span>
<span data-ttu-id="f9536-104">hello toohello 란 질문의 대답 "어떤 기계 학습 알고리즘을 사용 해야 합니까?"</span><span class="sxs-lookup"><span data-stu-id="f9536-104">hello answer toohello question "What machine learning algorithm should I use?"</span></span> <span data-ttu-id="f9536-105">항상 "상황마다 다릅니다."입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-105">is always "It depends."</span></span> <span data-ttu-id="f9536-106">Hello 크기, 품질 및 hello 데이터의 특성에 따라 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-106">It depends on hello size, quality, and nature of hello data.</span></span> <span data-ttu-id="f9536-107">대상에 따라 달라 집니다 toodo hello 답으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-107">It depends on what you want toodo with hello answer.</span></span> <span data-ttu-id="f9536-108">Hello 수학 hello 알고리즘의 사용 중인 hello 컴퓨터에 대 한 지침으로 변환 되었으면 방식에 따라 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-108">It depends on how hello math of hello algorithm was translated into instructions for hello computer you are using.</span></span> <span data-ttu-id="f9536-109">시간이 얼마나 있는지에 따라 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-109">And it depends on how much time you have.</span></span> <span data-ttu-id="f9536-110">알고리즘을 시도 하기 전에 최적의 성능을 발휘 합니다 가장 발생 하는 hello 데이터 과학자도 알 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-110">Even hello most experienced data scientists can't tell which algorithm will perform best before trying them.</span></span>

## <a name="hello-machine-learning-algorithm-cheat-sheet"></a><span data-ttu-id="f9536-111">hello 컴퓨터 학습 알고리즘 치트 시트</span><span class="sxs-lookup"><span data-stu-id="f9536-111">hello Machine Learning Algorithm Cheat Sheet</span></span>
<span data-ttu-id="f9536-112">hello **Microsoft Azure 컴퓨터 학습 알고리즘 치트 시트** hello 오른쪽 선택 기계 학습 알고리즘 hello Microsoft Azure 기계 학습 라이브러리 알고리즘에서 예측 분석 솔루션에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-112">hello **Microsoft Azure Machine Learning Algorithm Cheat Sheet** helps you choose hello right machine learning algorithm for your predictive analytics solutions from hello Microsoft Azure Machine Learning library of algorithms.</span></span>
<span data-ttu-id="f9536-113">이 문서는 방법을 단계별로 설명 toouse 것입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-113">This article walks you through how toouse it.</span></span>

> [!NOTE]
> <span data-ttu-id="f9536-114">toodownload hello 치트 시트를 서버와이 문서와 하면서 진행 너무 라인[알고리즘 치트 시트를 Microsoft Azure 기계 학습 스튜디오에 대 한 학습 컴퓨터](machine-learning-algorithm-cheat-sheet.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-114">toodownload hello cheat sheet and follow along with this article, go too[Machine learning algorithm cheat sheet for Microsoft Azure Machine Learning Studio](machine-learning-algorithm-cheat-sheet.md).</span></span>
> 
> 

<span data-ttu-id="f9536-115">이 치트 시트를 염두에 대 한 매우 특정 대상을: 시작 부분 데이터 과학자와 undergraduate 수준 기계 학습 toochoose Azure 기계 학습 스튜디오에서 사용 된 알고리즘 toostart 시도 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-115">This cheat sheet has a very specific audience in mind: a beginning data scientist with undergraduate-level machine learning, trying toochoose an algorithm toostart with in Azure Machine Learning Studio.</span></span> <span data-ttu-id="f9536-116">따라서 일반화되고 지나치게 단순화되었을 수 있으나 안전한 방향으로 사용자를 안내합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-116">That means that it makes some generalizations and oversimplifications, but it points you in a safe direction.</span></span> <span data-ttu-id="f9536-117">여기에 나와 있는 것 외에도 수많은 알고리즘이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-117">It also means that there are lots of algorithms not listed here.</span></span> <span data-ttu-id="f9536-118">Azure 기계 학습에 사용할 수 있는 방법의 전체 집합이 tooencompass 증가 추가 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-118">As Azure Machine Learning grows tooencompass a more complete set of available methods, we'll add them.</span></span>

<span data-ttu-id="f9536-119">이러한 권장 사항은 많은 데이터 과학자 및 기계 학습 전문가로부터 수집한 피드백 및 팁입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-119">These recommendations are compiled feedback and tips from many data scientists and machine learning experts.</span></span> <span data-ttu-id="f9536-120">모든 항목에 동의 하지 않은 것 이지만 시도 tooharmonize 서로의 의견 대략적인 합의를 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-120">We didn't agree on everything, but I've tried tooharmonize our opinions into a rough consensus.</span></span> <span data-ttu-id="f9536-121">대부분의 반대 의견이의 hello 문 "종속..."로 시작</span><span class="sxs-lookup"><span data-stu-id="f9536-121">Most of hello statements of disagreement begin with "It depends…"</span></span>

### <a name="how-toouse-hello-cheat-sheet"></a><span data-ttu-id="f9536-122">어떻게 toouse hello 치트 시트</span><span class="sxs-lookup"><span data-stu-id="f9536-122">How toouse hello cheat sheet</span></span>
<span data-ttu-id="f9536-123">로 hello 차트에 대 한 hello 경로 및 알고리즘 레이블이 읽기 "에 대 한  *&lt;경로 레이블&gt;*를 사용 하 여  *&lt;알고리즘&gt;*."</span><span class="sxs-lookup"><span data-stu-id="f9536-123">Read hello path and algorithm labels on hello chart as "For *&lt;path label&gt;*, use *&lt;algorithm&gt;*."</span></span> <span data-ttu-id="f9536-124">예를 들어, "*속도*의 경우 *두 개의 클래스 로지스틱 회귀*를 사용하세요."와 같습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-124">For example, "For *speed*, use *two class logistic regression*."</span></span> <span data-ttu-id="f9536-125">경우에 따라 두 개 이상의 분기가 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-125">Sometimes more than one branch applies.</span></span>
<span data-ttu-id="f9536-126">완벽하게 맞는 것이 없는 경우도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-126">Sometimes none of them are a perfect fit.</span></span> <span data-ttu-id="f9536-127">의도 한 toobe 경험의 권장 하는, 따라서 정확한 것에 대 한 걱정 하지 마십시오.</span><span class="sxs-lookup"><span data-stu-id="f9536-127">They're intended toobe rule-of-thumb recommendations, so don't worry about it being exact.</span></span>
<span data-ttu-id="f9536-128">여러 데이터 과학자와 알아보았습니다.만 있는지 hello 최상의 알고리즘은 tootry 찾을를 해당 hello 라고 모두 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-128">Several data scientists I talked with said that hello only sure way to find hello very best algorithm is tootry all of them.</span></span>

<span data-ttu-id="f9536-129">다음은 예제 hello에서 [Cortana 인텔리전스 갤러리](http://gallery.cortanaintelligence.com/) 동일한 데이터와 비교 하 여 hello 결과 hello에 대 한 여러 가지 알고리즘을 실험의: [다중 클래스 분류자 비교: 문자 인식 ](http://gallery.cortanaintelligence.com/Details/a635502fc98b402a890efe21cec65b92).</span><span class="sxs-lookup"><span data-stu-id="f9536-129">Here's an example from hello [Cortana Intelligence Gallery](http://gallery.cortanaintelligence.com/) of an experiment that tries several algorithms against hello same data and compares hello results: [Compare Multi-class Classifiers: Letter recognition](http://gallery.cortanaintelligence.com/Details/a635502fc98b402a890efe21cec65b92).</span></span>

> [!TIP]
> <span data-ttu-id="f9536-130">기계 학습 스튜디오의 hello 기능의 개요 다이어그램 참조 toodownload 및 인쇄 [Azure 기계 학습 스튜디오 기능의 개요 다이어그램](machine-learning-studio-overview-diagram.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-130">toodownload and print a diagram that gives an overview of hello capabilities of Machine Learning Studio, see [Overview diagram of Azure Machine Learning Studio capabilities](machine-learning-studio-overview-diagram.md).</span></span>
> 
> 

## <a name="flavors-of-machine-learning"></a><span data-ttu-id="f9536-131">기계 학습의 다양한 특징</span><span class="sxs-lookup"><span data-stu-id="f9536-131">Flavors of machine learning</span></span>
### <a name="supervised"></a><span data-ttu-id="f9536-132">감독</span><span class="sxs-lookup"><span data-stu-id="f9536-132">Supervised</span></span>
<span data-ttu-id="f9536-133">감독 학습 알고리즘은 예제 집합을 토대로 예측합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-133">Supervised learning algorithms make predictions based on a set of examples.</span></span> <span data-ttu-id="f9536-134">예를 들어, 기록 주가 향후 가격에 사용한 toohazard 추측 될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-134">For instance, historical stock prices can be used toohazard guesses at future prices.</span></span> <span data-ttu-id="f9536-135">학습에 사용 되는 각 예제는 hello 값으로 표시 되어-이 경우 주가 hello 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-135">Each example used for training is labeled with hello value of interest—in this case hello stock price.</span></span> <span data-ttu-id="f9536-136">감독 학습 알고리즘은 이러한 값 레이블에서 패턴을 찾습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-136">A supervised learning algorithm looks for patterns in those value labels.</span></span> <span data-ttu-id="f9536-137">관련 될 수 있는 모든 정보를 사용할 수 있습니다-hello 요일을 hello 주, hello 시즌, hello 회사의 재무 데이터, 업계의 hello 유형으로, 지정 학적 사건과 같은 중단의 hello 존재 여부 — 고 각 알고리즘 다양 한 유형의 패턴을 찾습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-137">It can use any information that might be relevant—hello day of hello week, hello season, hello company's financial data, hello type of industry, hello presence of disruptive geopolitical events—and each algorithm looks for different types of patterns.</span></span> <span data-ttu-id="f9536-138">Hello 알고리즘 hello 가장 효율적인 패턴 수를 찾으면 사용 하 여 테스트 데이터에 대 한 패턴 toomake 예측에 레이블 없음는-내일의 가격입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-138">After hello algorithm has found hello best pattern it can, it uses that pattern toomake predictions for unlabeled testing data—tomorrow's prices.</span></span>

<span data-ttu-id="f9536-139">감독 학습은 널리 사용되고 유용한 기계 학습 유형입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-139">Supervised learning is a popular and useful type of machine learning.</span></span> <span data-ttu-id="f9536-140">단, Azure 기계 학습에서 모든 hello 모듈 학습 알고리즘 감독 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-140">With one exception, all hello modules in Azure Machine Learning are supervised learning algorithms.</span></span> <span data-ttu-id="f9536-141">Azure 기계 학습 내에서는 분류, 회귀 및 이상 감지의 여러 특정 유형의 감독 학습이 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-141">There are several specific types of supervised learning that are represented within Azure Machine Learning: classification, regression, and anomaly detection.</span></span>

* <span data-ttu-id="f9536-142">**분류**.</span><span class="sxs-lookup"><span data-stu-id="f9536-142">**Classification**.</span></span> <span data-ttu-id="f9536-143">Hello 데이터에 사용 되는 toopredict 범주 되는, 분류 지도 학습 호출 됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-143">When hello data are being used toopredict a category, supervised learning is also called classification.</span></span> <span data-ttu-id="f9536-144">이 경우 hello 'cat' 또는 'dog'의 그림으로 이미지를 할당할 때 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-144">This is hello case when assigning an image as a picture of either a 'cat' or a 'dog'.</span></span> <span data-ttu-id="f9536-145">선택 항목이 두 가지뿐인 경우 **2클래스** 또는 **이항 분류**라고 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-145">When there are only two choices, it's called **two-class** or **binomial classification**.</span></span> <span data-ttu-id="f9536-146">이 문제는으로 알려져 있는 경우 더 많은 범주도 hello NCAA 년 3 월 목표 토너먼트의 hello 승자 예측 하는 경우, **다중 클래스 분류**합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-146">When there are more categories, as when predicting hello winner of hello NCAA March Madness tournament, this problem is known as **multi-class classification**.</span></span>
* <span data-ttu-id="f9536-147">**회귀**</span><span class="sxs-lookup"><span data-stu-id="f9536-147">**Regression**.</span></span> <span data-ttu-id="f9536-148">주가와 같은 값을 예측하는 경우 감독 학습을 회귀라고 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-148">When a value is being predicted, as with stock prices, supervised learning is called regression.</span></span>
* <span data-ttu-id="f9536-149">**이상 감지**.</span><span class="sxs-lookup"><span data-stu-id="f9536-149">**Anomaly detection**.</span></span> <span data-ttu-id="f9536-150">경우에 따라 hello 목표는 단순히 일반적이 지 않습니다 tooidentify 데이터 요소입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-150">Sometimes hello goal is tooidentify data points that are simply unusual.</span></span> <span data-ttu-id="f9536-151">예를 들어 이상 금융 거래 감지에서 매우 비정상적인 신용 카드 지출 패턴이 의심 대상입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-151">In fraud detection, for example, any highly unusual credit card spending patterns are suspect.</span></span> <span data-ttu-id="f9536-152">hello 가능한 변형을 하므로 다양 한 되며 hello 학습 예제가 거의 없으므로, 인지 가능 하지 않은 toolearn 사기성 어떤 활동이 같습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-152">hello possible variations are so numerous and hello training examples so few, that it's not feasible toolearn what fraudulent activity looks like.</span></span> <span data-ttu-id="f9536-153">이상 탐지 사용 방법은 toosimply 어떤 정상적인 작업 (기록 비 사기성 트랜잭션을 사용 하 여) 처럼 보이는 자세히 알아보고와 비슷한 되는 모든 항목을 식별 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-153">The approach that anomaly detection takes is toosimply learn what normal activity looks like (using a history non-fraudulent transactions) and identify anything that is significantly different.</span></span>

### <a name="unsupervised"></a><span data-ttu-id="f9536-154">자율</span><span class="sxs-lookup"><span data-stu-id="f9536-154">Unsupervised</span></span>
<span data-ttu-id="f9536-155">자율 학습에서는 데이터 요소에 연결된 레이블이 없습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-155">In unsupervised learning, data points have no labels associated with them.</span></span> <span data-ttu-id="f9536-156">대신, hello 목표는 감독 되지 않은 알고리즘 몇 가지 방식으로 또는 toodescribe hello 데이터 구조를 구성 하는 알 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-156">Instead, hello goal of an unsupervised learning algorithm is to organize hello data in some way or toodescribe its structure.</span></span> <span data-ttu-id="f9536-157">이는 클러스터로 그룹화하거나 복잡한 데이터가 보다 단순하게 또는 조직화되어 표시되도록 데이터를 바라보는 다양한 방법을 찾는 것을 의미할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-157">This can mean grouping it into clusters or finding different ways of looking at complex data so that it appears simpler or more organized.</span></span>

### <a name="reinforcement-learning"></a><span data-ttu-id="f9536-158">보충 학습</span><span class="sxs-lookup"><span data-stu-id="f9536-158">Reinforcement learning</span></span>
<span data-ttu-id="f9536-159">보충 학습 hello 알고리즘이 가져옵니다 toochoose 작업을 응답 tooeach 데이터 요소에.</span><span class="sxs-lookup"><span data-stu-id="f9536-159">In reinforcement learning, hello algorithm gets toochoose an action in response tooeach data point.</span></span> <span data-ttu-id="f9536-160">hello 학습 알고리즘 짧은 시간 나중 얼마나 양호한 hello 의사 결정을 나타내는 보상 신호를 받습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-160">hello learning algorithm also receives a reward signal a short time later, indicating how good hello decision was.</span></span>
<span data-ttu-id="f9536-161">이에 따라 hello 알고리즘 순서 tooachieve hello에 대 한 가장 높은 보상에서 해당 전략을 수정 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-161">Based on this, hello algorithm modifies its strategy in order tooachieve hello highest reward.</span></span> <span data-ttu-id="f9536-162">현재 Azure 기계 학습에는 보충 학습 알고리즘 모듈이 없습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-162">Currently there are no reinforcement learning algorithm modules in Azure Machine Learning.</span></span> <span data-ttu-id="f9536-163">보충 학습 로봇, 여기서 hello 시간에 한 지점 센서 판독값의 집합은 데이터 요소 고 hello 알고리즘 hello 로봇 다음 작업을 선택 해야에서 일반적입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-163">Reinforcement learning is common in robotics, where hello set of sensor readings at one point in time is a data point, and hello algorithm must choose hello robot's next action.</span></span> <span data-ttu-id="f9536-164">사물 인터넷의 응용 프로그램에 적합한 학습이기도 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-164">It is also a natural fit for Internet of Things applications.</span></span>

## <a name="considerations-when-choosing-an-algorithm"></a><span data-ttu-id="f9536-165">알고리즘 선택 시 고려 사항</span><span class="sxs-lookup"><span data-stu-id="f9536-165">Considerations when choosing an algorithm</span></span>
### <a name="accuracy"></a><span data-ttu-id="f9536-166">정확도</span><span class="sxs-lookup"><span data-stu-id="f9536-166">Accuracy</span></span>
<span data-ttu-id="f9536-167">항상 가능한 hello 가장 정확 하 게 응답을 가져오는 필요가 없습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-167">Getting hello most accurate answer possible isn't always necessary.</span></span>
<span data-ttu-id="f9536-168">용도에 따라 근사치가 적절한 경우도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-168">Sometimes an approximation is adequate, depending on what you want to use it for.</span></span> <span data-ttu-id="f9536-169">않은 hello 경우 수 toocut 것일 수 있습니다 더 많은 대략적인 방법으로 생각해 여 크게 처리 시간입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-169">If that's hello case, you may be able toocut your processing time dramatically by sticking with more approximate methods.</span></span> <span data-ttu-id="f9536-170">보다 근접한 방법의 다른 이점은 기본적으로 [과잉 맞춤](https://youtu.be/DQWI1kvmwRg)경향을 피할 수 있다는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-170">Another advantage of more approximate methods is that they naturally tend to avoid [overfitting](https://youtu.be/DQWI1kvmwRg).</span></span>

### <a name="training-time"></a><span data-ttu-id="f9536-171">학습 시간</span><span class="sxs-lookup"><span data-stu-id="f9536-171">Training time</span></span>
<span data-ttu-id="f9536-172">안녕 수가 분 단위 또는 시간 필요한 tootrain 모델 알고리즘 간에 크게 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-172">hello number of minutes or hours necessary tootrain a model varies a great deal between algorithms.</span></span> <span data-ttu-id="f9536-173">학습 시간이 종종 밀접 한 관련이 정확도-다른 hello 하나 일반적으로 포함 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-173">Training time is often closely tied to accuracy—one typically accompanies hello other.</span></span> <span data-ttu-id="f9536-174">또한 일부 알고리즘은 데이터 요소 다른 항목 보다 더 중요 한 toohello 수입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-174">In addition, some algorithms are more sensitive toohello number of data points than others.</span></span>
<span data-ttu-id="f9536-175">시간이 제한 되는 경우에 특히 hello 데이터 집합이 큰 경우 hello 선택한 알고리즘 운영할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-175">When time is limited it can drive hello choice of algorithm, especially when hello data set is large.</span></span>

### <a name="linearity"></a><span data-ttu-id="f9536-176">선형성</span><span class="sxs-lookup"><span data-stu-id="f9536-176">Linearity</span></span>
<span data-ttu-id="f9536-177">많은 기계 학습 알고리즘에서 선형성을 활용합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-177">Lots of machine learning algorithms make use of linearity.</span></span> <span data-ttu-id="f9536-178">선형 분류 알고리즘은 클래스를 직선(또는 고차원의 아날로그)으로 구분할 수 있다고 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-178">Linear classification algorithms assume that classes can be separated by a straight line (or its higher-dimensional analog).</span></span> <span data-ttu-id="f9536-179">여기에는 로지스틱 회귀 및 지원 벡터 컴퓨터가 포함됩니다(Azure 기계 학습에 구현됨).</span><span class="sxs-lookup"><span data-stu-id="f9536-179">These include logistic regression and support vector machines (as implemented in Azure Machine Learning).</span></span>
<span data-ttu-id="f9536-180">선형 회귀 알고리즘은 데이터가 직선을 따르는 경향이 있다고 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-180">Linear regression algorithms assume that data trends follow a straight line.</span></span> <span data-ttu-id="f9536-181">이러한 가정은 일부 문제에 대해서는 그다지 나쁘지 않지만 어떤 면에서는 정확도가 떨어질 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-181">These assumptions aren't bad for some problems, but on others they bring accuracy down.</span></span>

![비선형 클래스 경계][1]

<span data-ttu-id="f9536-183">***비선형 클래스 경계*** *- 선형 분류 알고리즘에 의존하며 결과의 정확도가 떨어짐*</span><span class="sxs-lookup"><span data-stu-id="f9536-183">***Non-linear class boundary*** *- relying on a linear classification algorithm would result in low accuracy*</span></span>

![비선형 추세 반영 데이터][2]

<span data-ttu-id="f9536-185">***비선형 추세 반영 데이터*** *- 선형 회귀 방법을 사용하면 필요한 것보다 훨씬 많은 오류가 생성됨*</span><span class="sxs-lookup"><span data-stu-id="f9536-185">***Data with a nonlinear trend*** *- using a linear regression method would generate much larger errors than necessary*</span></span>

<span data-ttu-id="f9536-186">위험성에도 불구하고, 선형 알고리즘은 공격에 대한 최전선으로 매우 널리 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-186">Despite their dangers, linear algorithms are very popular as a first line of attack.</span></span> <span data-ttu-id="f9536-187">이들은 toobe 알고리즘 방식으로 간단 하 고 학습에 빠르게는 경향이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-187">They tend toobe algorithmically simple and fast to train.</span></span>

### <a name="number-of-parameters"></a><span data-ttu-id="f9536-188">매개 변수 수</span><span class="sxs-lookup"><span data-stu-id="f9536-188">Number of parameters</span></span>
<span data-ttu-id="f9536-189">매개 변수는 hello 손잡이가 하는 데이터 과학자는 알고리즘을 설정할 때 tooturn를 가져옵니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-189">Parameters are hello knobs a data scientist gets tooturn when setting up an algorithm.</span></span> <span data-ttu-id="f9536-190">숫자 허용 오차 또는, 반복 또는 hello 알고리즘의 동작 방식을 변형의 여러 옵션의 수와 같은 hello 알고리즘의 동작에 영향을 주는 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-190">They are numbers that affect hello algorithm's behavior, such as error tolerance or number of iterations, or options between variants of how hello algorithm behaves.</span></span> <span data-ttu-id="f9536-191">hello 학습 시간 및 hello 알고리즘의 정확도 매우 중요 한 toogetting 정당한 hello 올바른 설정을 경우가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-191">hello training time and accuracy of hello algorithm can sometimes be quite sensitive toogetting just hello right settings.</span></span> <span data-ttu-id="f9536-192">일반적으로 많은 수의 매개 변수를 사용 하는 알고리즘 가장 평가판 hello 및 오류 toofind 좋은 조합이 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-192">Typically, algorithms with large numbers parameters require hello most trial and error toofind a good combination.</span></span>

<span data-ttu-id="f9536-193">또는 Azure 기계 학습에는 [매개 변수 스위핑](machine-learning-algorithm-parameters-optimize.md) 모듈 블록이 있어 선택한 세분성에서 모든 매개 변수 조합을 자동으로 시도합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-193">Alternatively, there is a [parameter sweeping](machine-learning-algorithm-parameters-optimize.md) module block in Azure Machine Learning that automatically tries all parameter combinations at whatever granularity you choose.</span></span> <span data-ttu-id="f9536-194">이 좋은 방법 toomake 있는지 hello 매개 변수 공간의 스팬 했습니다, 그리고 hello 필요한 시간 tootrain 모델 급격 하 게 증가 hello 수의 매개 변수를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-194">While this is a great way toomake sure you've spanned hello parameter space, hello time required tootrain a model increases exponentially with hello number of parameters.</span></span>

<span data-ttu-id="f9536-195">hello 거꾸로는 일반적으로 많은 매개 변수가 있는 의미는 알고리즘에는 유연 하 게 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-195">hello upside is that having many parameters typically indicates that an algorithm has greater flexibility.</span></span> <span data-ttu-id="f9536-196">또한 정확도도 매우 뛰어날 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-196">It can often achieve very good accuracy.</span></span> <span data-ttu-id="f9536-197">제공 hello 잘 조합 매개 변수 설정 찾을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-197">Provided you can find hello right combination of parameter settings.</span></span>

### <a name="number-of-features"></a><span data-ttu-id="f9536-198">기능 수</span><span class="sxs-lookup"><span data-stu-id="f9536-198">Number of features</span></span>
<span data-ttu-id="f9536-199">특정 유형의 데이터에 대 한 hello 다양 한 기능에는 데이터 요소의 매우 큰 비교 toohello 숫자일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-199">For certain types of data, hello number of features can be very large compared toohello number of data points.</span></span> <span data-ttu-id="f9536-200">이 경우 종종 hello와 genetics 또는 텍스트 데이터입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-200">This is often hello case with genetics or textual data.</span></span> <span data-ttu-id="f9536-201">hello 많은 수의 기능 일부 학습 알고리즘을 학습 unfeasibly 긴 시간을 만드는 성능이 저하 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-201">hello large number of features can bog down some learning algorithms, making training time unfeasibly long.</span></span> <span data-ttu-id="f9536-202">지원 벡터 컴퓨터는 특히 적합된 toothis 대/소문자 (아래 참조).</span><span class="sxs-lookup"><span data-stu-id="f9536-202">Support Vector Machines are particularly well suited toothis case (see below).</span></span>

### <a name="special-cases"></a><span data-ttu-id="f9536-203">특수 사례</span><span class="sxs-lookup"><span data-stu-id="f9536-203">Special cases</span></span>
<span data-ttu-id="f9536-204">일부 학습 알고리즘 hello 데이터 또는 원하는 hello 결과의 hello 구조에 대 한 특정 가정을 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-204">Some learning algorithms make particular assumptions about hello structure of hello data or hello desired results.</span></span> <span data-ttu-id="f9536-205">사용자 요구에 적합한 것을 찾을 수 있는 경우 보다 유용한 결과, 보다 정확한 예측 또는 단축된 교육 시간을 제공할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-205">If you can find one that fits your needs, it can give you more useful results, more accurate predictions, or faster training times.</span></span>

| <span data-ttu-id="f9536-206">**알고리즘**</span><span class="sxs-lookup"><span data-stu-id="f9536-206">**Algorithm**</span></span> | <span data-ttu-id="f9536-207">**정확도**</span><span class="sxs-lookup"><span data-stu-id="f9536-207">**Accuracy**</span></span> | <span data-ttu-id="f9536-208">**학습 시간**</span><span class="sxs-lookup"><span data-stu-id="f9536-208">**Training time**</span></span> | <span data-ttu-id="f9536-209">**선형성**</span><span class="sxs-lookup"><span data-stu-id="f9536-209">**Linearity**</span></span> | <span data-ttu-id="f9536-210">**매개 변수**</span><span class="sxs-lookup"><span data-stu-id="f9536-210">**Parameters**</span></span> | <span data-ttu-id="f9536-211">**참고 사항**</span><span class="sxs-lookup"><span data-stu-id="f9536-211">**Notes**</span></span> |
| --- |:---:|:---:|:---:|:---:| --- |
| <span data-ttu-id="f9536-212">**2클래스 분류**</span><span class="sxs-lookup"><span data-stu-id="f9536-212">**Two-class classification**</span></span> | | | | | |
| [<span data-ttu-id="f9536-213">로지스틱 회귀</span><span class="sxs-lookup"><span data-stu-id="f9536-213">logistic regression</span></span>](https://msdn.microsoft.com/library/azure/dn905994.aspx) | |<span data-ttu-id="f9536-214">●</span><span class="sxs-lookup"><span data-stu-id="f9536-214">●</span></span> |<span data-ttu-id="f9536-215">●</span><span class="sxs-lookup"><span data-stu-id="f9536-215">●</span></span> |<span data-ttu-id="f9536-216">5</span><span class="sxs-lookup"><span data-stu-id="f9536-216">5</span></span> | |
| [<span data-ttu-id="f9536-217">의사 결정 포리스트</span><span class="sxs-lookup"><span data-stu-id="f9536-217">decision forest</span></span>](https://msdn.microsoft.com/library/azure/dn906008.aspx) |<span data-ttu-id="f9536-218">●</span><span class="sxs-lookup"><span data-stu-id="f9536-218">●</span></span> |<span data-ttu-id="f9536-219">○</span><span class="sxs-lookup"><span data-stu-id="f9536-219">○</span></span> | |<span data-ttu-id="f9536-220">6</span><span class="sxs-lookup"><span data-stu-id="f9536-220">6</span></span> | |
| [<span data-ttu-id="f9536-221">의사 결정 정글</span><span class="sxs-lookup"><span data-stu-id="f9536-221">decision jungle</span></span>](https://msdn.microsoft.com/library/azure/dn905976.aspx) |<span data-ttu-id="f9536-222">●</span><span class="sxs-lookup"><span data-stu-id="f9536-222">●</span></span> |<span data-ttu-id="f9536-223">○</span><span class="sxs-lookup"><span data-stu-id="f9536-223">○</span></span> | |<span data-ttu-id="f9536-224">6</span><span class="sxs-lookup"><span data-stu-id="f9536-224">6</span></span> |<span data-ttu-id="f9536-225">적은 메모리 공간</span><span class="sxs-lookup"><span data-stu-id="f9536-225">Low memory footprint</span></span> |
| [<span data-ttu-id="f9536-226">향상된 의사 결정 트리</span><span class="sxs-lookup"><span data-stu-id="f9536-226">boosted decision tree</span></span>](https://msdn.microsoft.com/library/azure/dn906025.aspx) |<span data-ttu-id="f9536-227">●</span><span class="sxs-lookup"><span data-stu-id="f9536-227">●</span></span> |<span data-ttu-id="f9536-228">○</span><span class="sxs-lookup"><span data-stu-id="f9536-228">○</span></span> | |<span data-ttu-id="f9536-229">6</span><span class="sxs-lookup"><span data-stu-id="f9536-229">6</span></span> |<span data-ttu-id="f9536-230">큰 메모리 공간</span><span class="sxs-lookup"><span data-stu-id="f9536-230">Large memory footprint</span></span> |
| [<span data-ttu-id="f9536-231">신경망</span><span class="sxs-lookup"><span data-stu-id="f9536-231">neural network</span></span>](https://msdn.microsoft.com/library/azure/dn905947.aspx) |<span data-ttu-id="f9536-232">●</span><span class="sxs-lookup"><span data-stu-id="f9536-232">●</span></span> | | |<span data-ttu-id="f9536-233">9</span><span class="sxs-lookup"><span data-stu-id="f9536-233">9</span></span> |[<span data-ttu-id="f9536-234">추가 사용자 지정 가능</span><span class="sxs-lookup"><span data-stu-id="f9536-234">Additional customization is possible</span></span>](http://go.microsoft.com/fwlink/?LinkId=402867) |
| [<span data-ttu-id="f9536-235">평균 퍼셉트론</span><span class="sxs-lookup"><span data-stu-id="f9536-235">averaged perceptron</span></span>](https://msdn.microsoft.com/library/azure/dn906036.aspx) |<span data-ttu-id="f9536-236">○</span><span class="sxs-lookup"><span data-stu-id="f9536-236">○</span></span> |<span data-ttu-id="f9536-237">○</span><span class="sxs-lookup"><span data-stu-id="f9536-237">○</span></span> |<span data-ttu-id="f9536-238">●</span><span class="sxs-lookup"><span data-stu-id="f9536-238">●</span></span> |<span data-ttu-id="f9536-239">4</span><span class="sxs-lookup"><span data-stu-id="f9536-239">4</span></span> | |
| [<span data-ttu-id="f9536-240">지원 벡터 컴퓨터</span><span class="sxs-lookup"><span data-stu-id="f9536-240">support vector machine</span></span>](https://msdn.microsoft.com/library/azure/dn905835.aspx) | |<span data-ttu-id="f9536-241">○</span><span class="sxs-lookup"><span data-stu-id="f9536-241">○</span></span> |<span data-ttu-id="f9536-242">●</span><span class="sxs-lookup"><span data-stu-id="f9536-242">●</span></span> |<span data-ttu-id="f9536-243">5</span><span class="sxs-lookup"><span data-stu-id="f9536-243">5</span></span> |<span data-ttu-id="f9536-244">큰 기능 집합의 적합</span><span class="sxs-lookup"><span data-stu-id="f9536-244">Good for large feature sets</span></span> |
| [<span data-ttu-id="f9536-245">로컬 심층 지원 벡터 컴퓨터</span><span class="sxs-lookup"><span data-stu-id="f9536-245">locally deep support vector machine</span></span>](https://msdn.microsoft.com/library/azure/dn913070.aspx) |<span data-ttu-id="f9536-246">○</span><span class="sxs-lookup"><span data-stu-id="f9536-246">○</span></span> | | |<span data-ttu-id="f9536-247">8</span><span class="sxs-lookup"><span data-stu-id="f9536-247">8</span></span> |<span data-ttu-id="f9536-248">큰 기능 집합의 적합</span><span class="sxs-lookup"><span data-stu-id="f9536-248">Good for large feature sets</span></span> |
| [<span data-ttu-id="f9536-249">Bayes 지점 컴퓨터</span><span class="sxs-lookup"><span data-stu-id="f9536-249">Bayes’ point machine</span></span>](https://msdn.microsoft.com/library/azure/dn905930.aspx) | |<span data-ttu-id="f9536-250">○</span><span class="sxs-lookup"><span data-stu-id="f9536-250">○</span></span> |<span data-ttu-id="f9536-251">●</span><span class="sxs-lookup"><span data-stu-id="f9536-251">●</span></span> |<span data-ttu-id="f9536-252">3</span><span class="sxs-lookup"><span data-stu-id="f9536-252">3</span></span> | |
| <span data-ttu-id="f9536-253">**다중 클래스 분류**</span><span class="sxs-lookup"><span data-stu-id="f9536-253">**Multi-class classification**</span></span> | | | | | |
| [<span data-ttu-id="f9536-254">로지스틱 회귀</span><span class="sxs-lookup"><span data-stu-id="f9536-254">logistic regression</span></span>](https://msdn.microsoft.com/library/azure/dn905853.aspx) | |<span data-ttu-id="f9536-255">●</span><span class="sxs-lookup"><span data-stu-id="f9536-255">●</span></span> |<span data-ttu-id="f9536-256">●</span><span class="sxs-lookup"><span data-stu-id="f9536-256">●</span></span> |<span data-ttu-id="f9536-257">5</span><span class="sxs-lookup"><span data-stu-id="f9536-257">5</span></span> | |
| [<span data-ttu-id="f9536-258">의사 결정 포리스트</span><span class="sxs-lookup"><span data-stu-id="f9536-258">decision forest</span></span>](https://msdn.microsoft.com/library/azure/dn906015.aspx) |<span data-ttu-id="f9536-259">●</span><span class="sxs-lookup"><span data-stu-id="f9536-259">●</span></span> |<span data-ttu-id="f9536-260">○</span><span class="sxs-lookup"><span data-stu-id="f9536-260">○</span></span> | |<span data-ttu-id="f9536-261">6</span><span class="sxs-lookup"><span data-stu-id="f9536-261">6</span></span> | |
| [<span data-ttu-id="f9536-262">의사 결정 정글 </span><span class="sxs-lookup"><span data-stu-id="f9536-262">decision jungle </span></span>](https://msdn.microsoft.com/library/azure/dn905963.aspx) |<span data-ttu-id="f9536-263">●</span><span class="sxs-lookup"><span data-stu-id="f9536-263">●</span></span> |<span data-ttu-id="f9536-264">○</span><span class="sxs-lookup"><span data-stu-id="f9536-264">○</span></span> | |<span data-ttu-id="f9536-265">6</span><span class="sxs-lookup"><span data-stu-id="f9536-265">6</span></span> |<span data-ttu-id="f9536-266">적은 메모리 공간</span><span class="sxs-lookup"><span data-stu-id="f9536-266">Low memory footprint</span></span> |
| [<span data-ttu-id="f9536-267">신경망</span><span class="sxs-lookup"><span data-stu-id="f9536-267">neural network</span></span>](https://msdn.microsoft.com/library/azure/dn906030.aspx) |<span data-ttu-id="f9536-268">●</span><span class="sxs-lookup"><span data-stu-id="f9536-268">●</span></span> | | |<span data-ttu-id="f9536-269">9</span><span class="sxs-lookup"><span data-stu-id="f9536-269">9</span></span> |[<span data-ttu-id="f9536-270">추가 사용자 지정 가능</span><span class="sxs-lookup"><span data-stu-id="f9536-270">Additional customization is possible</span></span>](http://go.microsoft.com/fwlink/?LinkId=402867) |
| [<span data-ttu-id="f9536-271">one-v-all</span><span class="sxs-lookup"><span data-stu-id="f9536-271">one-v-all</span></span>](https://msdn.microsoft.com/library/azure/dn905887.aspx) |- |- |- |- |<span data-ttu-id="f9536-272">속성을 선택 하는 hello 2 클래스 메서드를 참조 하십시오.</span><span class="sxs-lookup"><span data-stu-id="f9536-272">See properties of hello two-class method selected</span></span> |
| <span data-ttu-id="f9536-273">**회귀**</span><span class="sxs-lookup"><span data-stu-id="f9536-273">**Regression**</span></span> | | | | | |
| [<span data-ttu-id="f9536-274">선형</span><span class="sxs-lookup"><span data-stu-id="f9536-274">linear</span></span>](https://msdn.microsoft.com/library/azure/dn905978.aspx) | |<span data-ttu-id="f9536-275">●</span><span class="sxs-lookup"><span data-stu-id="f9536-275">●</span></span> |<span data-ttu-id="f9536-276">●</span><span class="sxs-lookup"><span data-stu-id="f9536-276">●</span></span> |<span data-ttu-id="f9536-277">4</span><span class="sxs-lookup"><span data-stu-id="f9536-277">4</span></span> | |
| [<span data-ttu-id="f9536-278">베이지언 선형</span><span class="sxs-lookup"><span data-stu-id="f9536-278">Bayesian linear</span></span>](https://msdn.microsoft.com/library/azure/dn906022.aspx) | |<span data-ttu-id="f9536-279">○</span><span class="sxs-lookup"><span data-stu-id="f9536-279">○</span></span> |<span data-ttu-id="f9536-280">●</span><span class="sxs-lookup"><span data-stu-id="f9536-280">●</span></span> |<span data-ttu-id="f9536-281">2</span><span class="sxs-lookup"><span data-stu-id="f9536-281">2</span></span> | |
| [<span data-ttu-id="f9536-282">의사 결정 포리스트</span><span class="sxs-lookup"><span data-stu-id="f9536-282">decision forest</span></span>](https://msdn.microsoft.com/library/azure/dn905862.aspx) |<span data-ttu-id="f9536-283">●</span><span class="sxs-lookup"><span data-stu-id="f9536-283">●</span></span> |<span data-ttu-id="f9536-284">○</span><span class="sxs-lookup"><span data-stu-id="f9536-284">○</span></span> | |<span data-ttu-id="f9536-285">6</span><span class="sxs-lookup"><span data-stu-id="f9536-285">6</span></span> | |
| [<span data-ttu-id="f9536-286">향상된 의사 결정 트리</span><span class="sxs-lookup"><span data-stu-id="f9536-286">boosted decision tree</span></span>](https://msdn.microsoft.com/library/azure/dn905801.aspx) |<span data-ttu-id="f9536-287">●</span><span class="sxs-lookup"><span data-stu-id="f9536-287">●</span></span> |<span data-ttu-id="f9536-288">○</span><span class="sxs-lookup"><span data-stu-id="f9536-288">○</span></span> | |<span data-ttu-id="f9536-289">5</span><span class="sxs-lookup"><span data-stu-id="f9536-289">5</span></span> |<span data-ttu-id="f9536-290">큰 메모리 공간</span><span class="sxs-lookup"><span data-stu-id="f9536-290">Large memory footprint</span></span> |
| [<span data-ttu-id="f9536-291">빠른 포리스트 분위수</span><span class="sxs-lookup"><span data-stu-id="f9536-291">fast forest quantile</span></span>](https://msdn.microsoft.com/library/azure/dn913093.aspx) |<span data-ttu-id="f9536-292">●</span><span class="sxs-lookup"><span data-stu-id="f9536-292">●</span></span> |<span data-ttu-id="f9536-293">○</span><span class="sxs-lookup"><span data-stu-id="f9536-293">○</span></span> | |<span data-ttu-id="f9536-294">9</span><span class="sxs-lookup"><span data-stu-id="f9536-294">9</span></span> |<span data-ttu-id="f9536-295">지점 예측이 아닌 배포</span><span class="sxs-lookup"><span data-stu-id="f9536-295">Distributions rather than point predictions</span></span> |
| [<span data-ttu-id="f9536-296">신경망</span><span class="sxs-lookup"><span data-stu-id="f9536-296">neural network</span></span>](https://msdn.microsoft.com/library/azure/dn905924.aspx) |<span data-ttu-id="f9536-297">●</span><span class="sxs-lookup"><span data-stu-id="f9536-297">●</span></span> | | |<span data-ttu-id="f9536-298">9</span><span class="sxs-lookup"><span data-stu-id="f9536-298">9</span></span> |[<span data-ttu-id="f9536-299">추가 사용자 지정 가능</span><span class="sxs-lookup"><span data-stu-id="f9536-299">Additional customization is possible</span></span>](http://go.microsoft.com/fwlink/?LinkId=402867) |
| [<span data-ttu-id="f9536-300">포아송 </span><span class="sxs-lookup"><span data-stu-id="f9536-300">Poisson</span></span>](https://msdn.microsoft.com/library/azure/dn905988.aspx) | | |<span data-ttu-id="f9536-301">●</span><span class="sxs-lookup"><span data-stu-id="f9536-301">●</span></span> |<span data-ttu-id="f9536-302">5</span><span class="sxs-lookup"><span data-stu-id="f9536-302">5</span></span> |<span data-ttu-id="f9536-303">기술적으로 로그 선형입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-303">Technically log-linear.</span></span> <span data-ttu-id="f9536-304">개수를 예측하는 경우</span><span class="sxs-lookup"><span data-stu-id="f9536-304">For predicting counts</span></span> |
| [<span data-ttu-id="f9536-305">서수</span><span class="sxs-lookup"><span data-stu-id="f9536-305">ordinal</span></span>](https://msdn.microsoft.com/library/azure/dn906029.aspx) | | | |<span data-ttu-id="f9536-306">0</span><span class="sxs-lookup"><span data-stu-id="f9536-306">0</span></span> |<span data-ttu-id="f9536-307">순위 순서를 예측하는 경우</span><span class="sxs-lookup"><span data-stu-id="f9536-307">For predicting rank-ordering</span></span> |
| <span data-ttu-id="f9536-308">**이상 감지**</span><span class="sxs-lookup"><span data-stu-id="f9536-308">**Anomaly detection**</span></span> | | | | | |
| [<span data-ttu-id="f9536-309">지원 벡터 컴퓨터</span><span class="sxs-lookup"><span data-stu-id="f9536-309">support vector machine</span></span>](https://msdn.microsoft.com/library/azure/dn913103.aspx) |<span data-ttu-id="f9536-310">○</span><span class="sxs-lookup"><span data-stu-id="f9536-310">○</span></span> |<span data-ttu-id="f9536-311">○</span><span class="sxs-lookup"><span data-stu-id="f9536-311">○</span></span> | |<span data-ttu-id="f9536-312">2</span><span class="sxs-lookup"><span data-stu-id="f9536-312">2</span></span> |<span data-ttu-id="f9536-313">특히 큰 기능 집합의 적합</span><span class="sxs-lookup"><span data-stu-id="f9536-313">Especially good for large feature sets</span></span> |
| [<span data-ttu-id="f9536-314">PCA 기반 이상 감지</span><span class="sxs-lookup"><span data-stu-id="f9536-314">PCA-based anomaly detection</span></span>](https://msdn.microsoft.com/library/azure/dn913102.aspx) | |<span data-ttu-id="f9536-315">○</span><span class="sxs-lookup"><span data-stu-id="f9536-315">○</span></span> |<span data-ttu-id="f9536-316">●</span><span class="sxs-lookup"><span data-stu-id="f9536-316">●</span></span> |<span data-ttu-id="f9536-317">3</span><span class="sxs-lookup"><span data-stu-id="f9536-317">3</span></span> | |
| [<span data-ttu-id="f9536-318">K-means</span><span class="sxs-lookup"><span data-stu-id="f9536-318">K-means</span></span>](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/) | |<span data-ttu-id="f9536-319">○</span><span class="sxs-lookup"><span data-stu-id="f9536-319">○</span></span> |<span data-ttu-id="f9536-320">●</span><span class="sxs-lookup"><span data-stu-id="f9536-320">●</span></span> |<span data-ttu-id="f9536-321">4</span><span class="sxs-lookup"><span data-stu-id="f9536-321">4</span></span> |<span data-ttu-id="f9536-322">클러스터링 알고리즘</span><span class="sxs-lookup"><span data-stu-id="f9536-322">A clustering algorithm</span></span> |

<span data-ttu-id="f9536-323">**알고리즘 속성:**</span><span class="sxs-lookup"><span data-stu-id="f9536-323">**Algorithm properties:**</span></span>

<span data-ttu-id="f9536-324">**●** -정확성, 빠른 학습 시간 및 선형성 hello 사용을 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-324">**●** - shows excellent accuracy, fast training times, and hello use of linearity</span></span>

<span data-ttu-id="f9536-325">**○** - 양호한 정확도 및 보통의 교육 시간 나타냄</span><span class="sxs-lookup"><span data-stu-id="f9536-325">**○** - shows good accuracy and moderate training times</span></span>

## <a name="algorithm-notes"></a><span data-ttu-id="f9536-326">알고리즘 참고 사항</span><span class="sxs-lookup"><span data-stu-id="f9536-326">Algorithm notes</span></span>
### <a name="linear-regression"></a><span data-ttu-id="f9536-327">선형 회귀</span><span class="sxs-lookup"><span data-stu-id="f9536-327">Linear regression</span></span>
<span data-ttu-id="f9536-328">앞에서 설명한 대로 [선형 회귀](https://msdn.microsoft.com/library/azure/dn905978.aspx) 선 (또는 평면 또는 평면이) toohello 데이터 집합에 적합 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-328">As mentioned previously, [linear regression](https://msdn.microsoft.com/library/azure/dn905978.aspx) fits a line (or plane, or hyperplane) toohello data set.</span></span> <span data-ttu-id="f9536-329">자주 사용되고 간단하며 빠르지만 일부 문제에 대해서는 지나치게 단순화될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-329">It's a workhorse, simple and fast, but it may be overly simplistic for some problems.</span></span>
<span data-ttu-id="f9536-330">[선형 회귀 자습서](machine-learning-linear-regression-in-azure.md)는 여기를 확인하세요.</span><span class="sxs-lookup"><span data-stu-id="f9536-330">Check here for a [linear regression tutorial](machine-learning-linear-regression-in-azure.md).</span></span>

![선형 추세 반영 데이터][3]

<span data-ttu-id="f9536-332">***선형 추세 반영 데이터***</span><span class="sxs-lookup"><span data-stu-id="f9536-332">***Data with a linear trend***</span></span>

### <a name="logistic-regression"></a><span data-ttu-id="f9536-333">로지스틱 회귀</span><span class="sxs-lookup"><span data-stu-id="f9536-333">Logistic regression</span></span>
<span data-ttu-id="f9536-334">로지스틱 회귀는 위한 강력한 도구로 실제로 사용 '회귀' hello 이름에 포함 되어, 있지만 [2 클래스](https://msdn.microsoft.com/library/azure/dn905994.aspx) 및 [다중 클래스](https://msdn.microsoft.com/library/azure/dn905853.aspx) 분류 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-334">Although it confusingly includes 'regression' in hello name, logistic regression is actually a powerful tool for [two-class](https://msdn.microsoft.com/library/azure/dn905994.aspx) and [multiclass](https://msdn.microsoft.com/library/azure/dn905853.aspx) classification.</span></span> <span data-ttu-id="f9536-335">빠르고 단순합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-335">It's fast and simple.</span></span> <span data-ttu-id="f9536-336">사용 하 여 팩트 hello는 '-직선 대신 모양의 곡선 하면 적합 한 데이터를 그룹으로 분할 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-336">hello fact that it uses an 'S'-shaped curve instead of a straight line makes it a natural fit for dividing data into groups.</span></span> <span data-ttu-id="f9536-337">로지스틱 회귀는 선형 클래스 경계를 제공하므로 이를 사용할 때는 선형 근사값이 수락할 수 있는 것인지 확인해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-337">Logistic regression gives linear class boundaries, so when you use it, make sure a linear approximation is something you can live with.</span></span>

![한 기능을 사용 하 여 로지스틱 회귀 tootwo 클래스 데이터][4]

<span data-ttu-id="f9536-339">***한 기능을 사용 하 여 로지스틱 회귀 tootwo 클래스 데이터*** *-클래스 경계는 hello 항목이 요소를 어떤 hello 로지스틱 곡선 방금 가까운 tooboth 클래스*</span><span class="sxs-lookup"><span data-stu-id="f9536-339">***A logistic regression tootwo-class data with just one feature*** *- the class boundary is hello point at which hello logistic curve is just as close tooboth classes*</span></span>

### <a name="trees-forests-and-jungles"></a><span data-ttu-id="f9536-340">트리, 포리스트 및 정글</span><span class="sxs-lookup"><span data-stu-id="f9536-340">Trees, forests, and jungles</span></span>
<span data-ttu-id="f9536-341">의사 결정 포리스트([회귀](https://msdn.microsoft.com/library/azure/dn905862.aspx), [2클래스](https://msdn.microsoft.com/library/azure/dn906008.aspx) 및 [다중 클래스](https://msdn.microsoft.com/library/azure/dn906015.aspx)), 의사 결정 정글([2클래스](https://msdn.microsoft.com/library/azure/dn905976.aspx) 및 [다중 클래스](https://msdn.microsoft.com/library/azure/dn905963.aspx)) 및 향상된 의사 결정 트리([회귀](https://msdn.microsoft.com/library/azure/dn905801.aspx) 및 [2클래스](https://msdn.microsoft.com/library/azure/dn906025.aspx))는 모두 의사 결정 트리, 기본적인 Machine Learning 개념을 기반으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-341">Decision forests ([regression](https://msdn.microsoft.com/library/azure/dn905862.aspx), [two-class](https://msdn.microsoft.com/library/azure/dn906008.aspx), and [multiclass](https://msdn.microsoft.com/library/azure/dn906015.aspx)), decision jungles ([two-class](https://msdn.microsoft.com/library/azure/dn905976.aspx) and [multiclass](https://msdn.microsoft.com/library/azure/dn905963.aspx)), and boosted decision trees ([regression](https://msdn.microsoft.com/library/azure/dn905801.aspx) and [two-class](https://msdn.microsoft.com/library/azure/dn906025.aspx)) are all based on decision trees, a foundational machine learning concept.</span></span> <span data-ttu-id="f9536-342">에 의사 결정 트리의 여러 변형이 모두 동일한 작업을 수행 하지만-hello 기능 공간 대부분 hello 있는 영역으로 세분화 동일한 레이블.</span><span class="sxs-lookup"><span data-stu-id="f9536-342">There are many variants of decision trees, but they all do the same thing—subdivide hello feature space into regions with mostly hello same label.</span></span> <span data-ttu-id="f9536-343">분류 또는 회귀를 수행 중인지 여부에 따라 일관된 범주 또는 상수 값의 하위 지역일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-343">These can be regions of consistent category or of constant value, depending on whether you are doing classification or regression.</span></span>

![의사 결정 트리가 기능 공간 세분화][5]

<span data-ttu-id="f9536-345">***의사 결정 트리는 기능 공간을 대략적으로 균일한 값의 하위 지역으로 세분화함***</span><span class="sxs-lookup"><span data-stu-id="f9536-345">***A decision tree subdivides a feature space into regions of roughly uniform values***</span></span>

<span data-ttu-id="f9536-346">기능 공간을 임의로 작은 영역으로 분할할 수 있습니다, 되므로 쉽게 tooimagine 정밀 하 게 충분 한 toohave 단일 데이터 요소 / 지역당 분할 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-346">Because a feature space can be subdivided into arbitrarily small regions, it's easy tooimagine dividing it finely enough toohave one data point per region.</span></span> <span data-ttu-id="f9536-347">이는 극단적인 과잉 맞춤의 예입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-347">This is an extreme example of overfitting.</span></span> <span data-ttu-id="f9536-348">순서 tooavoid에이 다양 한 트리 생성 됩니다 수행 하는 특별 한 수학 주의 하 여 hello 트리는 상호 관련 되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-348">In order tooavoid this, a large set of trees are constructed with special mathematical care taken that hello trees are not correlated.</span></span> <span data-ttu-id="f9536-349">이 "의사 결정 포리스트"의 평균을 hello는 과잉 맞춤을 방지 하는 트리입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-349">hello average of this "decision forest" is a tree that avoids overfitting.</span></span> <span data-ttu-id="f9536-350">의사 결정 포리스트는 많은 양의 메모리를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-350">Decision forests can use a lot of memory.</span></span> <span data-ttu-id="f9536-351">의사 결정 정글은 학습 시간이 약간 초과의 hello 부담 적은 메모리를 사용 하는 variant입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-351">Decision jungles are a variant that consumes less memory at hello expense of a slightly longer training time.</span></span>

<span data-ttu-id="f9536-352">향상된 의사 결정 트리는 세분화할 수 있는 횟수 및 각 하위 지역에 허용되는 데이터 요소 수를 제한하여 과잉 맞춤을 피합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-352">Boosted decision trees avoid overfitting by limiting how many times they can subdivide and how few data points are allowed in each region.</span></span> <span data-ttu-id="f9536-353">알고리즘 하기 전에 hello 트리 남긴 hello 오류에 대 한 보정을 위해 학습 있으며 각 트리의 시퀀스를 생성 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-353">The algorithm constructs a sequence of trees, each of which learns to compensate for hello error left by hello tree before.</span></span> <span data-ttu-id="f9536-354">hello 결과 toouse 메모리가 매우 많이 느껴질 수 있는 매우 정확 하 게 학습자입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-354">hello result is a very accurate learner that tends toouse a lot of memory.</span></span> <span data-ttu-id="f9536-355">Hello 전체 기술 설명에 대 한 체크 아웃 [Friedman의 원래 자료](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf)합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-355">For hello full technical description, check out [Friedman's original paper](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf).</span></span>

<span data-ttu-id="f9536-356">[빠른 포리스트 변 위치 회귀](https://msdn.microsoft.com/library/azure/dn913093.aspx) 은 뿐만 아니라 hello 일반적인 ()의 중앙값 hello 데이터 영역, 뿐만 아니라 hello 형태의 변 위치에서 해당 배포 내에서 알아야 하는 hello 특별 한 경우에 대 한 의사 결정 트리의 변형입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-356">[Fast forest quantile regression](https://msdn.microsoft.com/library/azure/dn913093.aspx) is a variation of decision trees for hello special case where you want to know not only hello typical (median) value of hello data within a region, but also its distribution in hello form of quantiles.</span></span>

### <a name="neural-networks-and-perceptrons"></a><span data-ttu-id="f9536-357">신경망 및 퍼셉트론</span><span class="sxs-lookup"><span data-stu-id="f9536-357">Neural networks and perceptrons</span></span>
<span data-ttu-id="f9536-358">신경망은 [다중 클래스](https://msdn.microsoft.com/library/azure/dn906030.aspx), [2클래스](https://msdn.microsoft.com/library/azure/dn905947.aspx) 및 [회귀](https://msdn.microsoft.com/library/azure/dn905924.aspx)를 반영한 두뇌 영감 학습 알고리즘입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-358">Neural networks are brain-inspired learning algorithms covering [multiclass](https://msdn.microsoft.com/library/azure/dn906030.aspx), [two-class](https://msdn.microsoft.com/library/azure/dn905947.aspx), and [regression](https://msdn.microsoft.com/library/azure/dn905924.aspx) problems.</span></span> <span data-ttu-id="f9536-359">무한 한에서 나올 있지만 hello 신경망 내 Azure 기계 학습에서 사용 약관은 모두 hello 형태의 방향이 있는 비순환 그래프.</span><span class="sxs-lookup"><span data-stu-id="f9536-359">They come in an infinite variety, but hello neural networks within Azure Machine Learning are all of hello form of directed acyclic graphs.</span></span> <span data-ttu-id="f9536-360">따라서 입력 기능은 출력으로 전환되기 전에 계층 시퀀스를 통해 앞으로(뒤로는 불가능) 전달됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-360">That means that input features are passed forward (never backward) through a sequence of layers before being turned into outputs.</span></span> <span data-ttu-id="f9536-361">각 계층의 입력에 가중치를 적용을 다양 한 조합 한 hello 다음 계층에 전달 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-361">In each layer, inputs are weighted in various combinations, summed, and passed on to hello next layer.</span></span> <span data-ttu-id="f9536-362">기능 toolearn에서 간단한 계산 결과 이러한 조합은 클래스 경계 및 데이터 추세 매직 여 겉보기 정교한 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-362">This combination of simple calculations results in the ability toolearn sophisticated class boundaries and data trends, seemingly by magic.</span></span> <span data-ttu-id="f9536-363">이러한 종류의 네트워크 다 계층화 된 많은 양의 기술 보고 뒷받침하는 "심층 학습" hello 및 과학 소설 수행 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-363">Many-layered networks of this sort perform hello "deep learning" that fuels so much tech reporting and science fiction.</span></span>

<span data-ttu-id="f9536-364">하지만 이러한 높은 성능을 위해서는 대가가 따릅니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-364">This high performance doesn't come for free, though.</span></span> <span data-ttu-id="f9536-365">신경망은 기능이 많이 포함 된 큰 데이터 집합에 대 한 특히는 오랜 시간이 tootrain을 걸릴 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-365">Neural networks can take a long time tootrain, particularly for large data sets with lots of features.</span></span> <span data-ttu-id="f9536-366">즉, 매개 변수 비우기를 수행 hello 학습 시간이 크게 확장 대부분 알고리즘 보다 더 많은 매개 변수 갖습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-366">They also have more parameters than most algorithms, which means that parameter sweeping expands hello training time a great deal.</span></span>
<span data-ttu-id="f9536-367">및 너무 누구나 해당 overachievers[자신의 네트워크 구조를 지정](http://go.microsoft.com/fwlink/?LinkId=402867),이 속성은 exhaustible 되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-367">And for those overachievers who wish too[specify their own network structure](http://go.microsoft.com/fwlink/?LinkId=402867), the possibilities are inexhaustible.</span></span>

<span data-ttu-id="f9536-368">![신경망을 통해 경계가 학습][6]
***복잡 하 고 불규칙 한 신경망을 통해 학습 하는 hello 경계 수***</span><span class="sxs-lookup"><span data-stu-id="f9536-368">![Boundaries learned by neural networks][6]
***hello boundaries learned by neural networks can be complex and irregular***</span></span>

<span data-ttu-id="f9536-369">hello [2 클래스 평균 퍼셉트론](https://msdn.microsoft.com/library/azure/dn906036.aspx) 신경망의 응답 tooskyrocketing 학습 시간이 됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-369">hello [two-class averaged perceptron](https://msdn.microsoft.com/library/azure/dn906036.aspx) is neural networks' answer tooskyrocketing training times.</span></span> <span data-ttu-id="f9536-370">2클래스 평균 퍼셉트론은 선형 클래스 경계를 제공하는 네트워크 구조를 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-370">It uses a network structure that gives linear class boundaries.</span></span> <span data-ttu-id="f9536-371">오늘날의 기준으로 기본 거의 있지만 강력 하 게 작업은 오랜 전통을 개이고 만큼 작지 않아서 toolearn를 신속 하 게 됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-371">It is almost primitive by today's standards, but it has a long history of working robustly and is small enough toolearn quickly.</span></span>

### <a name="svms"></a><span data-ttu-id="f9536-372">SVM</span><span class="sxs-lookup"><span data-stu-id="f9536-372">SVMs</span></span>
<span data-ttu-id="f9536-373">지원 벡터 컴퓨터 Svm ()는 최대한 광범위 한 여백으로 여 클래스를 구분 하는 hello 경계를 찾습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-373">Support vector machines (SVMs) find hello boundary that separates classes by as wide a margin as possible.</span></span> <span data-ttu-id="f9536-374">Hello 두 개의 클래스를 명확 하 게 분리 될 수 없으며 hello 알고리즘 hello 최상의 경계를 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-374">When hello two classes can't be clearly separated, hello algorithms find hello best boundary they can.</span></span> <span data-ttu-id="f9536-375">Azure 기계 학습에서 작성 된 대로 hello [2 클래스 SVM](https://msdn.microsoft.com/library/azure/dn905835.aspx) 직선만으로이 작업을 수행 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-375">As written in Azure Machine Learning, hello [two-class SVM](https://msdn.microsoft.com/library/azure/dn905835.aspx) does this with a straight line only.</span></span> <span data-ttu-id="f9536-376">(SVM-speak에서 선형 커널을 사용합니다.) 이 선형 근사값을 사용 하면 되므로 수 toorun 매우 신속 하 게 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-376">(In SVM-speak, it uses a linear kernel.) Because it makes this linear approximation, it is able toorun fairly quickly.</span></span> <span data-ttu-id="f9536-377">텍스트 또는 유전자처럼 기능 중심의 데이터에서 특히 유용합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-377">Where it really shines is with feature-intense data, like text or genomic.</span></span> <span data-ttu-id="f9536-378">이러한 경우 Svm는 덜 과잉 맞춤이 대부분의 다른 알고리즘 보다 또한 toorequiring 많지 메모리으로 보다 신속 하 게 수 tooseparate 클래스입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-378">In these cases SVMs are able tooseparate classes more quickly and with less overfitting than most other algorithms, in addition toorequiring only a modest amount of memory.</span></span>

![지원 벡터 컴퓨터 클래스 경계][7]

<span data-ttu-id="f9536-380">***일반적인 지원 벡터 컴퓨터 클래스 경계는 두 개의 클래스를 구분 하는 hello 여백 최대화***</span><span class="sxs-lookup"><span data-stu-id="f9536-380">***A typical support vector machine class boundary maximizes hello margin separating two classes***</span></span>

<span data-ttu-id="f9536-381">다른 제품 Microsoft Research의 hello [2 클래스 로컬 심층 SVM](https://msdn.microsoft.com/library/azure/dn913070.aspx) 비선형 SVM 대부분의 hello 선형 버전의 hello 속도 메모리 효율성을 유지 하면서의 변형입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-381">Another product of Microsoft Research, hello [two-class locally deep SVM](https://msdn.microsoft.com/library/azure/dn913070.aspx) is a non-linear variant of SVM that retains most of hello speed and memory efficiency of hello linear version.</span></span> <span data-ttu-id="f9536-382">Hello 선형 접근 방식을 충분히 정확한 답을 제공 하지 않습니다 사례에 대 한 이상적입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-382">It is ideal for cases where hello linear approach doesn't give accurate enough answers.</span></span> <span data-ttu-id="f9536-383">hello 개발자 하 게 hello 문제 많은 작은 선형 SVM 문제를 구분 하 여 빠른 유지 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-383">hello developers kept it fast by breaking down hello problem into a bunch of small linear SVM problems.</span></span> <span data-ttu-id="f9536-384">읽기 hello [설명 전체](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf) hello에 대 한 내용은이 트릭 어떻게 삽입 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-384">Read hello [full description](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf) for hello details on how they pulled off this trick.</span></span>

<span data-ttu-id="f9536-385">Hello 비선형 Svm의 효과적인 확장을 사용 하 여, [1 클래스 SVM](https://msdn.microsoft.com/library/azure/dn913103.aspx) 밀접 하 게 hello 전체 데이터 집합에 간략하게 설명 하는 경계를 그립니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-385">Using a clever extension of nonlinear SVMs, hello [one-class SVM](https://msdn.microsoft.com/library/azure/dn913103.aspx) draws a boundary that tightly outlines hello entire data set.</span></span> <span data-ttu-id="f9536-386">이것은 이상 감지에 유용합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-386">It is useful for anomaly detection.</span></span> <span data-ttu-id="f9536-387">모든 새 데이터까지 해당 경계에 해당 하지 않는 요소가 주목할 만한 충분 한 비정상적인 toobe 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-387">Any new data points that fall far outside that boundary are unusual enough toobe noteworthy.</span></span>

### <a name="bayesian-methods"></a><span data-ttu-id="f9536-388">Bayesian 메서드</span><span class="sxs-lookup"><span data-stu-id="f9536-388">Bayesian methods</span></span>
<span data-ttu-id="f9536-389">Bayesian 메서드는 과잉 맞춤을 방지하는 매우 뛰어난 품질을 자랑합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-389">Bayesian methods have a highly desirable quality: they avoid overfitting.</span></span> <span data-ttu-id="f9536-390">Hello 대답의 hello 가능성이 분포에 대 한 미리 몇 가지 사항을 가정 하 여이 수행 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-390">They do this by making some assumptions beforehand about hello likely distribution of hello answer.</span></span> <span data-ttu-id="f9536-391">이 방법에 대한 다른 부산물은 매우 적은 매개 변수입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-391">Another byproduct of this approach is that they have very few parameters.</span></span> <span data-ttu-id="f9536-392">Azure Machine Learning은 분류([2클래스 Bayes 지점 컴퓨터](https://msdn.microsoft.com/library/azure/dn905930.aspx)) 및 회귀([베이지언 선형 회귀](https://msdn.microsoft.com/library/azure/dn906022.aspx)) 모두에 대해 두 베이지언 알고리즘을 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-392">Azure Machine Learning has both Bayesian algorithms for both classification ([Two-class Bayes' point machine](https://msdn.microsoft.com/library/azure/dn905930.aspx)) and regression ([Bayesian linear regression](https://msdn.microsoft.com/library/azure/dn906022.aspx)).</span></span>
<span data-ttu-id="f9536-393">Hello 데이터를 분할 하거나 직선에 맞게 수는 이러한 가정 하는 참고 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-393">Note that these assume that hello data can be split or fit with a straight line.</span></span>

<span data-ttu-id="f9536-394">역사를 거슬러 보면, Bayes 지점 컴퓨터는 Microsoft Research에서 개발되었습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-394">On a historical note, Bayes' point machines were developed at Microsoft Research.</span></span> <span data-ttu-id="f9536-395">그 이면에는 매우 뛰어난 이론적인 작업을 포함하고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-395">They have some exceptionally beautiful theoretical work behind them.</span></span> <span data-ttu-id="f9536-396">hello 관심된 학생은 방향이 지정 된 toohello [JMLR의 문서를 원래](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf) 및 [Chris bishop이 작성 하 여 통찰력 블로그](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx)합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-396">hello interested student is directed toohello [original article in JMLR](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf) and an [insightful blog by Chris Bishop](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx).</span></span>

### <a name="specialized-algorithms"></a><span data-ttu-id="f9536-397">특수화된 알고리즘</span><span class="sxs-lookup"><span data-stu-id="f9536-397">Specialized algorithms</span></span>
<span data-ttu-id="f9536-398">매우 구체적인 목표가 있다면 운이 좋을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-398">If you have a very specific goal you may be in luck.</span></span> <span data-ttu-id="f9536-399">Hello Azure 기계 학습 컬렉션 내에서 특수화 하는 알고리즘 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-399">Within hello Azure Machine Learning collection, there are algorithms that specialize in:</span></span>

- <span data-ttu-id="f9536-400">순위 예측([서수 회귀](https://msdn.microsoft.com/library/azure/dn906029.aspx)),</span><span class="sxs-lookup"><span data-stu-id="f9536-400">rank prediction ([ordinal regression](https://msdn.microsoft.com/library/azure/dn906029.aspx)),</span></span>
- <span data-ttu-id="f9536-401">수 예측([포아송 회귀](https://msdn.microsoft.com/library/azure/dn905988.aspx)),</span><span class="sxs-lookup"><span data-stu-id="f9536-401">count prediction ([Poisson regression](https://msdn.microsoft.com/library/azure/dn905988.aspx)),</span></span>
- <span data-ttu-id="f9536-402">변칙 검색([주요 구성 요소 분석](https://msdn.microsoft.com/library/azure/dn913102.aspx) 기준 및 [지원 벡터 컴퓨터](https://msdn.microsoft.com/library/azure/dn913103.aspx) 기준)</span><span class="sxs-lookup"><span data-stu-id="f9536-402">anomaly detection (one based on [principal components analysis](https://msdn.microsoft.com/library/azure/dn913102.aspx) and one based on [support vector machine](https://msdn.microsoft.com/library/azure/dn913103.aspx)s)</span></span>
- <span data-ttu-id="f9536-403">클러스터링([K-means](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/))</span><span class="sxs-lookup"><span data-stu-id="f9536-403">clustering ([K-means](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/))</span></span>

![PCA 기반 이상 감지][8]

<span data-ttu-id="f9536-405">***PCA 기반 비정상 검색*** *-hello 포함 된 대부분의 hello 데이터 stereotypical 배포에 해당 되며 다음 해당 배포에서 크게 벗어난 포인트 주의 대상*</span><span class="sxs-lookup"><span data-stu-id="f9536-405">***PCA-based anomaly detection*** *- hello vast majority of hello data falls into a stereotypical distribution; points deviating dramatically from that distribution are suspect*</span></span>

![K-means를 사용하여 그룹화된 데이터 집합][9]

<span data-ttu-id="f9536-407">***K-means를 사용하여 데이터 집합을 5개 클러스터로 그룹화합니다.***</span><span class="sxs-lookup"><span data-stu-id="f9536-407">***A data set is grouped into five clusters using K-means***</span></span>

<span data-ttu-id="f9536-408">앙상블 이기도 [중 하나-v-모든 다중 클래스 분류자](https://msdn.microsoft.com/library/azure/dn905887.aspx), 나누기 hello N 클래스 분류 문제가 N-1 2 클래스 분류 문제가 발생 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-408">There is also an ensemble [one-v-all multiclass classifier](https://msdn.microsoft.com/library/azure/dn905887.aspx), which breaks hello N-class classification problem into N-1 two-class classification problems.</span></span> <span data-ttu-id="f9536-409">hello 정확도, 교육 시간 및 선형성 속성 hello 2 클래스 분류자를 사용 하 여 결정 됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-409">hello accuracy, training time, and linearity properties are determined by hello two-class classifiers used.</span></span>

![2 클래스 분류자 tooform 세 클래스 분류자 결합][10]

<span data-ttu-id="f9536-411">***2 클래스 분류자의 쌍 tooform 세 클래스 분류자 결합***</span><span class="sxs-lookup"><span data-stu-id="f9536-411">***A pair of two-class classifiers combine tooform a three-class classifier***</span></span>

<span data-ttu-id="f9536-412">액세스 tooa 강력한 기계 학습 프레임 워크의 hello 제목 아래에 포함 되어 azure 기계 학습 [Vowpal Wabbit](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf)합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-412">Azure Machine Learning also includes access tooa powerful machine learning framework under hello title of [Vowpal Wabbit](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf).</span></span>
<span data-ttu-id="f9536-413">VW는 분류 및 회귀를 모두 학습할 수 있으며 부분적으로 레이블이 지정되지 않은 데이터에서도 학습이 가능하므로 여기에서 분류를 거부합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-413">VW defies categorization here, since it can learn both classification and regression problems and can even learn from partially unlabeled data.</span></span> <span data-ttu-id="f9536-414">다양 한 알고리즘, 손실 함수 및 최적화 알고리즘을 학습 중 하나는 toouse를 구성할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-414">You can configure it toouse any one of a number of learning algorithms, loss functions, and optimization algorithms.</span></span> <span data-ttu-id="f9536-415">효율적이 고, 매우 빠른 toobe를 접지 hello에서 설계 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-415">It was designed from hello ground up toobe efficient, parallel, and extremely fast.</span></span> <span data-ttu-id="f9536-416">적은 작업으로 엄청나게 큰 기능 집합을 처리합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-416">It handles ridiculously large feature sets with little apparent effort.</span></span>
<span data-ttu-id="f9536-417">Microsoft Research의 John Langford가 시작하여 진행한 VW는 스톡 카 알고리즘 분야에서 포뮬러 원(Formula One) 엔트리입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-417">Started and led by Microsoft Research's own John Langford, VW is a Formula One entry in a field of stock car algorithms.</span></span> <span data-ttu-id="f9536-418">모든 문제 VW, 맞지만 던 수도 없습니다 tooclimb 인터페이스에 대해 학습 합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-418">Not every problem fits VW, but if yours does, it may be worth your while tooclimb the learning curve on its interface.</span></span> <span data-ttu-id="f9536-419">또한 여러 언어로 된 [독립 실행형 오픈 소스 코드](https://github.com/JohnLangford/vowpal_wabbit) 도 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-419">It's also available as [stand-alone open source code](https://github.com/JohnLangford/vowpal_wabbit) in several languages.</span></span>

## <a name="more-help-with-algorithms"></a><span data-ttu-id="f9536-420">알고리즘에 대한 자세한 도움말</span><span class="sxs-lookup"><span data-stu-id="f9536-420">More help with algorithms</span></span>
* <span data-ttu-id="f9536-421">알고리즘을 설명하고 예제를 제공하는 다운로드 가능한 인포그래픽은 [다운로드 가능한 인포그래픽: 알고리즘 예제를 포함한 Machine Learning 기본 사항](machine-learning-basics-infographic-with-algorithm-examples.md)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="f9536-421">For a downloadable infographic that describes algorithms and provides examples, see [Downloadable Infographic: Machine learning basics with algorithm examples](machine-learning-basics-infographic-with-algorithm-examples.md).</span></span>
* <span data-ttu-id="f9536-422">Azure 기계 학습 스튜디오에서 사용할 수 있는 모든 hello 기계 학습 알고리즘의 범주별 목록을 보려면 참조 [모델 초기화] [ initialize-model] hello Studio 기계 학습 알고리즘 및 모듈 도움말입니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-422">For a list by category of all hello machine learning algorithms available in Azure Machine Learning Studio, see [Initialize Model][initialize-model] in hello Machine Learning Studio Algorithm and Module Help.</span></span>
* <span data-ttu-id="f9536-423">Azure Machine Learning Studio의 전체 알고리즘 및 모듈에 대한 알파벳 순서 목록은 Machine Learning Studio 알고리즘 및 모듈 도움말에서 [Machine Learning Studio 모듈의 A-Z 목록][a-z-list]을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="f9536-423">For a complete alphabetical list of algorithms and modules in Azure Machine Learning Studio, see [A-Z list of Machine Learning Studio modules][a-z-list] in Machine Learning Studio Algorithm and Module Help.</span></span>
* <span data-ttu-id="f9536-424">Azure 기계 학습 스튜디오의 hello 기능의 개요 다이어그램 참조 toodownload 및 인쇄 [Azure 기계 학습 스튜디오 기능의 개요 다이어그램](machine-learning-studio-overview-diagram.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="f9536-424">toodownload and print a diagram that gives an overview of hello capabilities of Azure Machine Learning Studio, see [Overview diagram of Azure Machine Learning Studio capabilities](machine-learning-studio-overview-diagram.md).</span></span>


<!-- Reference links -->
[initialize-model]: https://msdn.microsoft.com/library/azure/dn905812.aspx
[a-z-list]: https://msdn.microsoft.com/library/azure/dn906033.aspx

<!-- Media -->

[1]: ./media/machine-learning-algorithm-choice/image1.png
[2]: ./media/machine-learning-algorithm-choice/image2.png
[3]: ./media/machine-learning-algorithm-choice/image3.png
[4]: ./media/machine-learning-algorithm-choice/image4.png
[5]: ./media/machine-learning-algorithm-choice/image5.png
[6]: ./media/machine-learning-algorithm-choice/image6.png
[7]: ./media/machine-learning-algorithm-choice/image7.png
[8]: ./media/machine-learning-algorithm-choice/image8.png
[9]: ./media/machine-learning-algorithm-choice/image9.png
[10]: ./media/machine-learning-algorithm-choice/image10.png
