---
title: "Azure HDInsight에서 Spark를 사용하는 데이터 과학 개요 | Microsoft Docs"
description: "Spark MLlib 도구 키트를 통해 많은 기계 학습 모델링 기능을 이 분산 HDInsight 환경에서 사용할 수 있습니다."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: a4e1de99-a554-4240-9647-2c6d669593c8
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/15/2017
ms.author: deguhath;bradsev;gokuma
ms.openlocfilehash: 379b32f4e533f48f1593a97e73737a0c5bfb9135
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 07/11/2017
---
# <a name="overview-of-data-science-using-spark-on-azure-hdinsight"></a><span data-ttu-id="ebf53-103">Azure HDInsight에서 Spark를 사용하는 데이터 과학 개요</span><span class="sxs-lookup"><span data-stu-id="ebf53-103">Overview of data science using Spark on Azure HDInsight</span></span>
[!INCLUDE [machine-learning-spark-modeling](../../includes/machine-learning-spark-modeling.md)]

<span data-ttu-id="ebf53-104">이 항목 모음에서는 HDInsight Spark를 사용하여 데이터 수집, 기능 엔지니어링, 모델링 및 모델 평가와 같은 일반적인 데이터 과학 작업을 완료하는 방법을 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-104">This suite of topics shows how to use HDInsight Spark to complete common data science tasks such as data ingestion, feature engineering, modeling, and model evaluation.</span></span> <span data-ttu-id="ebf53-105">사용되는 데이터는 2013 NYC Taxi Trip 및 요금 데이터 집합의 샘플입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-105">The data used is a sample of the 2013 NYC taxi trip and fare dataset.</span></span> <span data-ttu-id="ebf53-106">작성된 모델은 로지스틱 및 선형 회귀, 임의 포리스트 및 그라데이션 향상된 트리를 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-106">The models built include logistic and linear regression, random forests, and gradient boosted trees.</span></span> <span data-ttu-id="ebf53-107">또한 이 항목은 이러한 모델을 Azure Blob Storage(WASB)에 저장하고 예측 성능의 점수를 매기며 평가하는 방법도 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-107">The topics also show how to store these models in Azure blob storage (WASB) and how to score and evaluate their predictive performance.</span></span> <span data-ttu-id="ebf53-108">고급 항목에서는 교차 유효성 검사 및 하이퍼 매개 변수 스위핑을 사용하여 모델을 학습할 수 있는 방법을 다룹니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-108">More advanced topics cover how models can be trained using cross-validation and hyper-parameter sweeping.</span></span> <span data-ttu-id="ebf53-109">이 개요 토픽은 제공된 연습의 단계를 완료하는 데 필요한 Spark 클러스터를 설정하는 방법을 설명하는 토픽도 참조합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-109">This overview topic also references the topics that describe how to set up the Spark cluster that you need to complete the steps in the walkthroughs provided.</span></span> 

## <a name="spark-and-mllib"></a><span data-ttu-id="ebf53-110">Spark 및 MLlib</span><span class="sxs-lookup"><span data-stu-id="ebf53-110">Spark and MLlib</span></span>
<span data-ttu-id="ebf53-111">[Spark](http://spark.apache.org/) 는 메모리 내 처리를 지원하여 빅데이터 분석 응용 프로그램의 성능을 향상하는 오픈 소스 병렬 처리 프레임워크입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-111">[Spark](http://spark.apache.org/) is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</span></span> <span data-ttu-id="ebf53-112">속도, 간편한 사용 및 정교한 분석을 위해 Spark 처리 엔진이 빌드되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-112">The Spark processing engine is built for speed, ease of use, and sophisticated analytics.</span></span> <span data-ttu-id="ebf53-113">Spark는 메모리 내 분산형 계산 기능을 지원하여 기계 학습 및 그래프 계산에 사용된 반복 알고리즘에 적합합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-113">Spark's in-memory distributed computation capabilities make it a good choice for the iterative algorithms used in machine learning and graph computations.</span></span> <span data-ttu-id="ebf53-114">[MLlib](http://spark.apache.org/mllib/)는 Spark의 확장형 기계 학습 라이브러리로, 분산형 환경에서 알고리즘 모델링 기능을 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-114">[MLlib](http://spark.apache.org/mllib/) is Spark's scalable machine learning library that brings the algorithmic modeling capabilities to this distributed environment.</span></span> 

## <a name="hdinsight-spark"></a><span data-ttu-id="ebf53-115">HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="ebf53-115">HDInsight Spark</span></span>
<span data-ttu-id="ebf53-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) 는 Azure에서 호스트하는 오픈 소스 Spark의 제품입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) is the Azure hosted offering of open-source Spark.</span></span> <span data-ttu-id="ebf53-117">또한 Azure Blob(WASB)에 저장된 데이터를 변환, 필터링 및 시각화하기 위해 Spark SQL 대화형 쿼리를 실행할 수 있는 Spark 클러스터상의 **Jupyter PySpark Notebook**에 대한 지원도 포함하고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-117">It also includes support for **Jupyter PySpark notebooks** on the Spark cluster that can run Spark SQL interactive queries for transforming, filtering, and visualizing data stored in Azure Blobs (WASB).</span></span> <span data-ttu-id="ebf53-118">PySpark는 Spark용 Python API입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-118">PySpark is the Python API for Spark.</span></span> <span data-ttu-id="ebf53-119">솔루션을 제공하고 데이터 시각화를 위해 관련 플롯을 여기에 보여 주는 코드 조각은 Spark 클러스터에 설치된 Jupyter Notebook에서 실행됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-119">The code snippets that provide the solutions and show the relevant plots to visualize the data here run in Jupyter notebooks installed on the Spark clusters.</span></span> <span data-ttu-id="ebf53-120">이러한 항목의 모델링 단계는 각 모델 유형을 학습, 평가, 저장 및 사용하는 방법을 보여 주는 코드를 포함하고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-120">The modeling steps in these topics contain code that shows how to train, evaluate, save, and consume each type of model.</span></span> 

## <a name="setup-spark-clusters-and-jupyter-notebooks"></a><span data-ttu-id="ebf53-121">설정: Spark 클러스터 및 Jupyter Notebook</span><span class="sxs-lookup"><span data-stu-id="ebf53-121">Setup: Spark clusters and Jupyter notebooks</span></span>
<span data-ttu-id="ebf53-122">설치 단계와 코드는 HDInsight Spark 1.6을 사용하는 이 연습에 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-122">Setup steps and code are provided in this walkthrough for using an HDInsight Spark 1.6.</span></span> <span data-ttu-id="ebf53-123">하지만 Jupyter Notebook은 HDInsight Spark 1.6과 Spark 2.0 클러스터 둘 다에 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-123">But Jupyter notebooks are provided for both HDInsight Spark 1.6 and Spark 2.0 clusters.</span></span> <span data-ttu-id="ebf53-124">노트북과 이에 연결된 링크의 설명은 이들을 포함하는 GitHub 리포지토리의 [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md)에 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-124">A description of the notebooks and links to them are provided in the [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) for the GitHub repository containing them.</span></span> <span data-ttu-id="ebf53-125">그뿐 아니라 여기에 있는 코드와 연결된 Notebook에 있는 코드는 일반적이므로 아무 Spark 클러스터에서나 작동할 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-125">Moreover, the code here and in the linked notebooks is generic and should work on any Spark cluster.</span></span> <span data-ttu-id="ebf53-126">HDInsight Spark를 사용하지 않는 경우 클러스터 설치 및 관리 단계가 여기에 나오는 내용과 약간 다를 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-126">If you are not using HDInsight Spark, the cluster setup and management steps may be slightly different from what is shown here.</span></span> <span data-ttu-id="ebf53-127">편의를 위해, Jupyter Notebook 서버의 pySpark 커널에서 실행되는 Spark 1.6 및 Jupyter Notebook 서버의 pySpark3 커널에서 실행되는 Spark 2.0용 Jupyter Notebook에 연결된 링크는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-127">For convenience, here are the links to the Jupyter notebooks for Spark 1.6 (to be run in the pySpark kernel of the Jupyter Notebook server) and  Spark 2.0 (to be run in the pySpark3 kernel of the Jupyter Notebook server):</span></span>

### <a name="spark-16-notebooks"></a><span data-ttu-id="ebf53-128">Spark 1.6 노트북</span><span class="sxs-lookup"><span data-stu-id="ebf53-128">Spark 1.6 notebooks</span></span>
<span data-ttu-id="ebf53-129">이러한 Notebook은 Jupyter Notebook 서버의 pySpark 커널에서 실행됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-129">These notebooks are to be run in the pySpark kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="ebf53-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): 데이터 탐색, 모델링, 그리고 몇 가지 알고리즘으로 점수 매기기 등을 수행하는 방법에 대한 정보를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): Provides information on how to perform data exploration, modeling, and scoring with several different algorithms.</span></span>
- <span data-ttu-id="ebf53-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): 노트북 #1의 토픽과 하이퍼 매개 변수 조정 및 교차 유효성 검사를 사용하는 모델 개발을 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): Includes topics in notebook #1, and model development using hyperparameter tuning and cross-validation.</span></span>
- <span data-ttu-id="ebf53-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): HDInsight 클러스터에서 Python을 사용하여 저장된 모델을 운용하는 방법을 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): Shows how to operationalize a saved model using Python on HDInsight clusters.</span></span>

### <a name="spark-20-notebooks"></a><span data-ttu-id="ebf53-133">Spark 2.0 노트북</span><span class="sxs-lookup"><span data-stu-id="ebf53-133">Spark 2.0 notebooks</span></span>
<span data-ttu-id="ebf53-134">이러한 Notebook은 Jupyter Notebook 서버의 pySpark3 커널에서 실행됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-134">These notebooks are to be run in the pySpark3 kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="ebf53-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): 이 파일은 NYC Taxi Trip 및 [여기](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data) 설명된 데이터 집합을 사용하여 Spark 2.0 클러스터에서 데이터 탐색, 모델링, 점수 매기기를 수행하는 방법에 대한 정보를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): This file provides information on how to perform data exploration, modeling, and scoring in Spark 2.0 clusters using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span> <span data-ttu-id="ebf53-136">이 Notebook은 Spark 2.0에 대해 제공했던 코드를 신속하게 탐색하기 위한 좋은 시작점일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-136">This notebook may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> <span data-ttu-id="ebf53-137">NYC Taxi 데이터를 분석하는 Notebook 상세 정보는 이 목록에서 다음 Notebook을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-137">For a more detailed notebook analyzes the NYC Taxi data, see the next notebook in this list.</span></span> <span data-ttu-id="ebf53-138">이러한 Notebook을 비교하는 목록 다음의 참고 사항을 확인하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-138">See the notes following this list that compare these notebooks.</span></span> 
- <span data-ttu-id="ebf53-139">[Spark2.0 pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): 이 파일은 [여기](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data)에 설명된 NYC 택시 여정 및 요금 데이터 집합을 사용한 데이터 랭글링(Spark SQL 및 데이터 프레임 작업), 탐색, 모델링 및 점수 매기기를 수행하는 방법을 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span>
- <span data-ttu-id="ebf53-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): 이 파일은 2011년 및 2012년의 유명 항공사 정시 출발 데이터 집합을 사용한 데이터 랭글링(Spark SQL 및 데이터 프레임 작업), 탐색, 모델링 및 점수 매기기를 수행하는 방법을 보여줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the well-known Airline On-time departure dataset from 2011 and 2012.</span></span> <span data-ttu-id="ebf53-141">날씨 요소를 모델에 포함시킬 수 있도록 모델링하기 전에 항공사 데이터 집합과 공항 날씨 데이터(예: 풍속, 온도, 고도 등)를 통합하였습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-141">We integrated the airline dataset with the airport weather data (e.g. windspeed, temperature, altitude etc.) prior to modeling, so these weather features can be included in the model.</span></span>

<!-- -->

> [!NOTE]
> <span data-ttu-id="ebf53-142">항공사 데이터 집합은 분류 알고리즘의 사용 이해를 돕기 위해 Spark 2.0 Notebook에 추가되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-142">The airline dataset was added to the Spark 2.0 notebooks to better illustrate the use of classification algorithms.</span></span> <span data-ttu-id="ebf53-143">항공사 정시 출발 데이터 집합 및 날씨 데이터 집합에 대한 내용은 다음 링크를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-143">See the following links for information about airline on-time departure dataset and weather dataset:</span></span>

>- <span data-ttu-id="ebf53-144">항공사 정시 출발 데이터: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span><span class="sxs-lookup"><span data-stu-id="ebf53-144">Airline on-time departure data: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span></span>

>- <span data-ttu-id="ebf53-145">공항 날씨 데이터: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span><span class="sxs-lookup"><span data-stu-id="ebf53-145">Airport weather data: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span></span> 
> 
> 

<!-- -->

<!-- -->

> [!NOTE]
<span data-ttu-id="ebf53-146">NYC taxi의 Spark 2.0 Notebook 및 항공사 비행 지연 데이터 집합은 실행하는 데 10분 이상이 소요될 수 있습니다(HDI 클러스터의 크기에 따라 다름).</span><span class="sxs-lookup"><span data-stu-id="ebf53-146">The Spark 2.0 notebooks on the NYC taxi and airline flight delay data-sets can take 10 mins or more to run (depending on the size of your HDI cluster).</span></span> <span data-ttu-id="ebf53-147">위 목록에서 첫 번째 Notebook은 샘플 수를 줄인 NYC 데이터 집합으로 실행 시간을 줄인 Notebook에서 데이터 탐색, 시각화 및 ML 모델 학습의 다양한 측면을 보여 줍니다. 여기서는 택시 및 요금 파일을 사전에 조인했습니다. [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) 이 Notebook은 완료하는 데 훨씬 짧은 시간(2-3분)이 소요되며 Spark 2.0에 대해 제공된 코드를 신속하게 탐색하기 위한 좋은 시작점일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-147">The first notebook in the above list shows many aspects of the data exploration, visualization and ML model training in a notebook that takes less time to run with down-sampled NYC data set, in which the taxi and fare files have been pre-joined: [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) This notebook takes a much shorter time to finish (2-3 mins) and may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> 

<!-- -->

<span data-ttu-id="ebf53-148">점수 매기기를 위한 Spark 2.0 모델 및 모델 사용량의 운용에 대한 지침은 [사용량에 관한 Spark 1.6 문서](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb)에서 해당 단계를 설명하는 예제를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-148">For guidance on the operationalization of a Spark 2.0 model and model consumption for scoring, see the [Spark 1.6 document on consumption](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) for an example outlining the steps required.</span></span> <span data-ttu-id="ebf53-149">Spark 2.0에서 이를 사용하려면 Python 코드 파일을 [이 파일](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py)로 대체합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-149">To use this on Spark 2.0, replace the Python code file with [this file](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span></span>

### <a name="prerequisites"></a><span data-ttu-id="ebf53-150">필수 조건</span><span class="sxs-lookup"><span data-stu-id="ebf53-150">Prerequisites</span></span>
<span data-ttu-id="ebf53-151">다음 절차는 Spark 1.6에 대한 내용입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-151">The following procedures are related to Spark 1.6.</span></span> <span data-ttu-id="ebf53-152">Spark 2.0 버전의 경우 이전에 설명 및 링크된 Notebook을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-152">For  the Spark 2.0 version, use the notebooks described and linked to previously.</span></span> 

<span data-ttu-id="ebf53-153">1. Azure 구독이 있어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-153">1.You must have an Azure subscription.</span></span> <span data-ttu-id="ebf53-154">아직 가지고 있지 않은 경우 [Azure 평가판](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-154">If you do not already have one, see [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span></span>

<span data-ttu-id="ebf53-155">2. 이 연습을 완료하는 데는 Spark 1.6 클러스터가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-155">2.You need a Spark 1.6 cluster to complete this walkthrough.</span></span> <span data-ttu-id="ebf53-156">만드는 방법은 [시작: Azure HDInsight에서 Apache Spark 만들기](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-156">To create one, see the instructions provided in [Get started: create Apache Spark on Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span></span> <span data-ttu-id="ebf53-157">클러스터 유형 및 버전은 **클러스터 유형 선택** 메뉴에서 지정됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-157">The cluster type and version is specified from the **Select Cluster Type** menu.</span></span> 

![클러스터 구성](./media/machine-learning-data-science-spark-overview/spark-cluster-on-portal.png)

<!-- -->

> [!NOTE]
> <span data-ttu-id="ebf53-159">Python이 아닌 Scala를 사용하여 종단 간 데이터 과학 프로세스에 대한 작업을 완료하는 방법을 보여 주는 항목에 대해서는 [Azure에서 Spark를 사용하는 데이터 과학](machine-learning-data-science-process-scala-walkthrough.md)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-159">For a topic that shows how to use Scala rather than Python to complete tasks for an end-to-end data science process, see the [Data Science using Scala with Spark on Azure](machine-learning-data-science-process-scala-walkthrough.md).</span></span>
> 
> 

<!-- -->

> [!INCLUDE [delete-cluster-warning](../../includes/hdinsight-delete-cluster-warning.md)]
> 
> 

## <a name="the-nyc-2013-taxi-data"></a><span data-ttu-id="ebf53-160">NYC 2013 Taxi 데이터</span><span class="sxs-lookup"><span data-stu-id="ebf53-160">The NYC 2013 Taxi data</span></span>
<span data-ttu-id="ebf53-161">NYC Taxi Trip 데이터는 1억 7,300만 개가 넘는 개별 여정 및 각 여정의 요금으로 구성된 약 20GB의 압축된 CSV(쉼표로 구분된 값) 파일(압축되지 않은 경우 약 48GB)입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-161">The NYC Taxi Trip data is about 20 GB of compressed comma-separated values (CSV) files (~48 GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</span></span> <span data-ttu-id="ebf53-162">각 여정 레코드는 승차 및 하차 위치와 시간, 익명 처리된 hack(기사) 면허증 번호 및 medallion(택시의 고유 ID) 번호를 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-162">Each trip record includes the pick up and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.</span></span> <span data-ttu-id="ebf53-163">데이터는 2013년의 모든 여정을 포괄하며, 매월 다음 두 개의 데이터 집합으로 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-163">The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</span></span>

1. <span data-ttu-id="ebf53-164">'trip_data' CSV 파일은 승객 수, 승차 및 하차 지점, 여정 기간, 여정 거리 등 여정 세부 정보를 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-164">The 'trip_data' CSV files contain trip details, such as number of passengers, pick up and dropoff points, trip duration, and trip length.</span></span> <span data-ttu-id="ebf53-165">다음은 몇 가지 샘플 레코드입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-165">Here are a few sample records:</span></span>
   
        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868
2. <span data-ttu-id="ebf53-166">'trip_fare' CSV 파일은 지불 유형, 금액, 추가 요금 및 세금, 팁 및 통행료, 총 지불 금액 등 각 여정의 요금에 대한 세부 정보를 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-166">The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</span></span> <span data-ttu-id="ebf53-167">다음은 몇 가지 샘플 레코드입니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-167">Here are a few sample records:</span></span>
   
        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5

<span data-ttu-id="ebf53-168">이러한 파일 중 0.1% 샘플을 선택하고 trip\_data 및 trip\_fare CVS 파일을 이 연습의 입력 데이터 집합으로 사용할 단일 데이터 집합에 조인했습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-168">We have taken a 0.1% sample of these files and joined the trip\_data and trip\_fare CVS files into a single dataset to use as the input dataset for this walkthrough.</span></span> <span data-ttu-id="ebf53-169">trip\_data와 trip\_fare를 조인할 고유 키는 medallion, hack\_licence 및 pickup\_datetime 필드로 구성됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-169">The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</span></span> <span data-ttu-id="ebf53-170">데이터 집합의 각 레코드는 NYC Taxi 여정을 나타내는 다음과 같은 특성을 포함하고 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-170">Each record of the dataset contains the following attributes representing a NYC Taxi trip:</span></span>

| <span data-ttu-id="ebf53-171">필드</span><span class="sxs-lookup"><span data-stu-id="ebf53-171">Field</span></span> | <span data-ttu-id="ebf53-172">간략한 설명</span><span class="sxs-lookup"><span data-stu-id="ebf53-172">Brief Description</span></span> |
| --- | --- |
| <span data-ttu-id="ebf53-173">medallion</span><span class="sxs-lookup"><span data-stu-id="ebf53-173">medallion</span></span> |<span data-ttu-id="ebf53-174">익명 처리된 택시 medallion(고유 택시 id)</span><span class="sxs-lookup"><span data-stu-id="ebf53-174">Anonymized taxi medallion (unique taxi id)</span></span> |
| <span data-ttu-id="ebf53-175">hack_license</span><span class="sxs-lookup"><span data-stu-id="ebf53-175">hack_license</span></span> |<span data-ttu-id="ebf53-176">익명 처리된 Hackney 운전 면허 번호</span><span class="sxs-lookup"><span data-stu-id="ebf53-176">Anonymized Hackney Carriage License number</span></span> |
| <span data-ttu-id="ebf53-177">vendor_id</span><span class="sxs-lookup"><span data-stu-id="ebf53-177">vendor_id</span></span> |<span data-ttu-id="ebf53-178">택시 공급 업체 id</span><span class="sxs-lookup"><span data-stu-id="ebf53-178">Taxi vendor id</span></span> |
| <span data-ttu-id="ebf53-179">rate_code</span><span class="sxs-lookup"><span data-stu-id="ebf53-179">rate_code</span></span> |<span data-ttu-id="ebf53-180">NYC 택시 요율</span><span class="sxs-lookup"><span data-stu-id="ebf53-180">NYC taxi rate of fare</span></span> |
| <span data-ttu-id="ebf53-181">store_and_fwd_flag</span><span class="sxs-lookup"><span data-stu-id="ebf53-181">store_and_fwd_flag</span></span> |<span data-ttu-id="ebf53-182">저장소 및 전달 플래그</span><span class="sxs-lookup"><span data-stu-id="ebf53-182">Store and forward flag</span></span> |
| <span data-ttu-id="ebf53-183">pickup_datetime</span><span class="sxs-lookup"><span data-stu-id="ebf53-183">pickup_datetime</span></span> |<span data-ttu-id="ebf53-184">승차 날짜 및 시간</span><span class="sxs-lookup"><span data-stu-id="ebf53-184">Pick up date & time</span></span> |
| <span data-ttu-id="ebf53-185">dropoff_datetime</span><span class="sxs-lookup"><span data-stu-id="ebf53-185">dropoff_datetime</span></span> |<span data-ttu-id="ebf53-186">내린 날짜 및 시간</span><span class="sxs-lookup"><span data-stu-id="ebf53-186">Dropoff date & time</span></span> |
| <span data-ttu-id="ebf53-187">pickup_hour</span><span class="sxs-lookup"><span data-stu-id="ebf53-187">pickup_hour</span></span> |<span data-ttu-id="ebf53-188">승차 시간</span><span class="sxs-lookup"><span data-stu-id="ebf53-188">Pick up hour</span></span> |
| <span data-ttu-id="ebf53-189">pickup_week</span><span class="sxs-lookup"><span data-stu-id="ebf53-189">pickup_week</span></span> |<span data-ttu-id="ebf53-190">연간 누적 승차 주 수</span><span class="sxs-lookup"><span data-stu-id="ebf53-190">Pick up week of the year</span></span> |
| <span data-ttu-id="ebf53-191">weekday</span><span class="sxs-lookup"><span data-stu-id="ebf53-191">weekday</span></span> |<span data-ttu-id="ebf53-192">요일(범위 1-7)</span><span class="sxs-lookup"><span data-stu-id="ebf53-192">Weekday (range 1-7)</span></span> |
| <span data-ttu-id="ebf53-193">passenger_count</span><span class="sxs-lookup"><span data-stu-id="ebf53-193">passenger_count</span></span> |<span data-ttu-id="ebf53-194">택시 여정의 승객 수</span><span class="sxs-lookup"><span data-stu-id="ebf53-194">Number of passengers in a taxi trip</span></span> |
| <span data-ttu-id="ebf53-195">trip_time_in_secs</span><span class="sxs-lookup"><span data-stu-id="ebf53-195">trip_time_in_secs</span></span> |<span data-ttu-id="ebf53-196">여정 시간(초)</span><span class="sxs-lookup"><span data-stu-id="ebf53-196">Trip time in seconds</span></span> |
| <span data-ttu-id="ebf53-197">trip_distance</span><span class="sxs-lookup"><span data-stu-id="ebf53-197">trip_distance</span></span> |<span data-ttu-id="ebf53-198">주행한 여정 거리(마일 단위)</span><span class="sxs-lookup"><span data-stu-id="ebf53-198">Trip distance traveled in miles</span></span> |
| <span data-ttu-id="ebf53-199">pickup_longitude</span><span class="sxs-lookup"><span data-stu-id="ebf53-199">pickup_longitude</span></span> |<span data-ttu-id="ebf53-200">승차 경도</span><span class="sxs-lookup"><span data-stu-id="ebf53-200">Pick up longitude</span></span> |
| <span data-ttu-id="ebf53-201">pickup_latitude</span><span class="sxs-lookup"><span data-stu-id="ebf53-201">pickup_latitude</span></span> |<span data-ttu-id="ebf53-202">승차 위도</span><span class="sxs-lookup"><span data-stu-id="ebf53-202">Pick up latitude</span></span> |
| <span data-ttu-id="ebf53-203">dropoff_longitude</span><span class="sxs-lookup"><span data-stu-id="ebf53-203">dropoff_longitude</span></span> |<span data-ttu-id="ebf53-204">내린 경도</span><span class="sxs-lookup"><span data-stu-id="ebf53-204">Dropoff longitude</span></span> |
| <span data-ttu-id="ebf53-205">dropoff_latitude</span><span class="sxs-lookup"><span data-stu-id="ebf53-205">dropoff_latitude</span></span> |<span data-ttu-id="ebf53-206">내린 위도</span><span class="sxs-lookup"><span data-stu-id="ebf53-206">Dropoff latitude</span></span> |
| <span data-ttu-id="ebf53-207">direct_distance</span><span class="sxs-lookup"><span data-stu-id="ebf53-207">direct_distance</span></span> |<span data-ttu-id="ebf53-208">승차 및 하차 위치 사이의 직접 거리</span><span class="sxs-lookup"><span data-stu-id="ebf53-208">Direct distance between pick up and dropoff locations</span></span> |
| <span data-ttu-id="ebf53-209">payment_type</span><span class="sxs-lookup"><span data-stu-id="ebf53-209">payment_type</span></span> |<span data-ttu-id="ebf53-210">지불 유형(cas, 신용 카드 등)</span><span class="sxs-lookup"><span data-stu-id="ebf53-210">Payment type (cas, credit-card etc.)</span></span> |
| <span data-ttu-id="ebf53-211">fare_amount</span><span class="sxs-lookup"><span data-stu-id="ebf53-211">fare_amount</span></span> |<span data-ttu-id="ebf53-212">요금 금액</span><span class="sxs-lookup"><span data-stu-id="ebf53-212">Fare amount in</span></span> |
| <span data-ttu-id="ebf53-213">surcharge</span><span class="sxs-lookup"><span data-stu-id="ebf53-213">surcharge</span></span> |<span data-ttu-id="ebf53-214">추가 요금</span><span class="sxs-lookup"><span data-stu-id="ebf53-214">Surcharge</span></span> |
| <span data-ttu-id="ebf53-215">mta_tax</span><span class="sxs-lookup"><span data-stu-id="ebf53-215">mta_tax</span></span> |<span data-ttu-id="ebf53-216">Mta 세금</span><span class="sxs-lookup"><span data-stu-id="ebf53-216">Mta tax</span></span> |
| <span data-ttu-id="ebf53-217">tip_amount</span><span class="sxs-lookup"><span data-stu-id="ebf53-217">tip_amount</span></span> |<span data-ttu-id="ebf53-218">팁 금액</span><span class="sxs-lookup"><span data-stu-id="ebf53-218">Tip amount</span></span> |
| <span data-ttu-id="ebf53-219">tolls_amount</span><span class="sxs-lookup"><span data-stu-id="ebf53-219">tolls_amount</span></span> |<span data-ttu-id="ebf53-220">통행료 금액</span><span class="sxs-lookup"><span data-stu-id="ebf53-220">Tolls amount</span></span> |
| <span data-ttu-id="ebf53-221">total_amount</span><span class="sxs-lookup"><span data-stu-id="ebf53-221">total_amount</span></span> |<span data-ttu-id="ebf53-222">총 금액</span><span class="sxs-lookup"><span data-stu-id="ebf53-222">Total amount</span></span> |
| <span data-ttu-id="ebf53-223">tipped</span><span class="sxs-lookup"><span data-stu-id="ebf53-223">tipped</span></span> |<span data-ttu-id="ebf53-224">팁 지불 여부(아니요 또는 예에 대해 0/1 지정)</span><span class="sxs-lookup"><span data-stu-id="ebf53-224">Tipped (0/1 for no or yes)</span></span> |
| <span data-ttu-id="ebf53-225">tip_class</span><span class="sxs-lookup"><span data-stu-id="ebf53-225">tip_class</span></span> |<span data-ttu-id="ebf53-226">팁 클래스(0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span><span class="sxs-lookup"><span data-stu-id="ebf53-226">Tip class (0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span></span> |

## <a name="execute-code-from-a-jupyter-notebook-on-the-spark-cluster"></a><span data-ttu-id="ebf53-227">Spark 클러스터의 Jupyter Notebook에서 코드 실행</span><span class="sxs-lookup"><span data-stu-id="ebf53-227">Execute code from a Jupyter notebook on the Spark cluster</span></span>
<span data-ttu-id="ebf53-228">Azure 포털에서 Jupyter Notebook을 시작할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-228">You can launch the Jupyter Notebook from the Azure portal.</span></span> <span data-ttu-id="ebf53-229">대시보드에서 Spark 클러스터를 찾아 클릭하여 클러스터에 대한 관리 페이지로 들어갑니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-229">Find your Spark cluster on your dashboard and click it to enter management page for your cluster.</span></span> <span data-ttu-id="ebf53-230">Spark 클러스터와 연결된 Notebook을 열려면 **Cluster 대시보드** -> **Jupyter Notebook**을 클릭합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-230">To open the notebook associated with the Spark cluster, click **Cluster Dashboards** -> **Jupyter Notebook** .</span></span>

![클러스터 대시보드](./media/machine-learning-data-science-spark-overview/spark-jupyter-on-portal.png)

<span data-ttu-id="ebf53-232">***https://CLUSTERNAME.azurehdinsight.net/jupyter***로 이동하여 Jupyter Notebook에 액세스할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-232">You can also browse to ***https://CLUSTERNAME.azurehdinsight.net/jupyter*** to access the Jupyter Notebooks.</span></span> <span data-ttu-id="ebf53-233">이 URL의 CLUSTERNAME 부분을 사용자 고유의 클러스터 이름으로 바꿉니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-233">Replace the CLUSTERNAME part of this URL with the name of your own cluster.</span></span> <span data-ttu-id="ebf53-234">Notebook에 액세스하려면 관리자 계정에 대한 암호가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-234">You need the password for your admin account to access the notebooks.</span></span>

![Jupyter 노트북 찾아보기](./media/machine-learning-data-science-spark-overview/spark-jupyter-notebook.png)

<span data-ttu-id="ebf53-236">PySpark를 선택하여 PySpark API를 사용하는 미리 패키지된 Notebook에 대한 몇 가지 예가 들어 있는 디렉터리를 표시합니다. 이 Spark 항목 모음에 대한 코드 샘플이 포함된 Notebook은 [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)에서 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-236">Select PySpark to see a directory that contains a few examples of pre-packaged notebooks that use the PySpark API.The notebooks that contain the code samples for this suite of Spark topic are available at [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)</span></span>

<span data-ttu-id="ebf53-237">노트북을 [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)에서 Spark 클러스터의 Jupyter Notebook 서버에 직접 업로드할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-237">You can upload the notebooks directly from [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) to the Jupyter notebook server on your Spark cluster.</span></span> <span data-ttu-id="ebf53-238">Jupyter의 홈 페이지에서 화면 오른쪽의 **업로드** 버튼을 클릭합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-238">On the home page of your Jupyter, click the **Upload** button on the right part of the screen.</span></span> <span data-ttu-id="ebf53-239">파일 탐색기가 열립니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-239">It opens a file explorer.</span></span> <span data-ttu-id="ebf53-240">여기서 Notebook의 GitHub(원시 콘텐츠) URL을 붙여넣고 **열기**를 클릭할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-240">Here you can paste the GitHub (raw content) URL of the Notebook and click **Open**.</span></span> 

<span data-ttu-id="ebf53-241">**업로드** 버튼을 다시 사용하면 Jupyter 파일 목록에서 파일 이름을 확인할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-241">You see the file name on your Jupyter file list with an **Upload** button again.</span></span> <span data-ttu-id="ebf53-242">이 **업로드** 버튼을 클릭합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-242">Click this **Upload** button.</span></span> <span data-ttu-id="ebf53-243">이제 Notebook을 가져왔습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-243">Now you have imported the notebook.</span></span> <span data-ttu-id="ebf53-244">이 단계를 반복하여 이 연습에서 다른 Notebook을 업로드합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-244">Repeat these steps to upload the other notebooks from this walkthrough.</span></span>

> [!TIP]
> <span data-ttu-id="ebf53-245">브라우저의 링크를 마우스 오른쪽 단추로 클릭하고 **링크 복사**를 선택하여 github 원시 콘텐츠 URL을 가져올 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-245">You can right-click the links on your browser and select **Copy Link** to get the github raw content URL.</span></span> <span data-ttu-id="ebf53-246">Jupyter 업로드 파일 탐색기 대화 상자에 이 URL을 붙여넣을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-246">You can paste this URL into the Jupyter Upload file explorer dialog box.</span></span>
> 
> 

<span data-ttu-id="ebf53-247">이제 다음을 수행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-247">Now you can:</span></span>

* <span data-ttu-id="ebf53-248">Notebook을 클릭하여 코드를 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-248">See the code by clicking the notebook.</span></span>
* <span data-ttu-id="ebf53-249">**Shift+Enter**를 눌러 각 셀을 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-249">Execute each cell by pressing **SHIFT-ENTER**.</span></span>
* <span data-ttu-id="ebf53-250">**셀** -> **실행**을 클릭하여 전체 Notebook을 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-250">Run the entire notebook by clicking on **Cell** -> **Run**.</span></span>
* <span data-ttu-id="ebf53-251">쿼리의 자동 시각화를 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-251">Use the automatic visualization of queries.</span></span>

> [!TIP]
> <span data-ttu-id="ebf53-252">PySpark 커널은 SQL(HiveQL) 쿼리의 출력을 자동으로 시각화합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-252">The PySpark kernel automatically visualizes the output of SQL (HiveQL) queries.</span></span> <span data-ttu-id="ebf53-253">Notebook의 **형식** 메뉴 버튼을 사용하여 다양한 시각화 형식(테이블, 원형, 꺾은선형, 영역 또는 막대) 중에서 선택할 수 있는 옵션이 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-253">You are given the option to select among several different types of visualizations (Table, Pie, Line, Area, or Bar) by using the **Type** menu buttons in the notebook:</span></span>
> 
> 

![일반적인 접근 방식에 대한 로지스틱 회귀 분석 ROC 곡선](./media/machine-learning-data-science-spark-overview/pyspark-jupyter-autovisualization.png)

## <a name="whats-next"></a><span data-ttu-id="ebf53-255">다음 작업</span><span class="sxs-lookup"><span data-stu-id="ebf53-255">What's next?</span></span>
<span data-ttu-id="ebf53-256">이제 HDInsight Spark 클러스터를 설치하고 Jupyter Notebook을 업로드했으므로 이 세 PySpark Notebook에 해당하는 항목을 진행할 준비가 되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-256">Now that you are set up with an HDInsight Spark cluster and have uploaded the Jupyter notebooks, you are ready to work through the topics that correspond to the three PySpark notebooks.</span></span> <span data-ttu-id="ebf53-257">이들 항목은 데이터 탐색 방법을 보여 준 후 모델을 만들고 사용하는 방법을 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-257">They show how to explore your data and then how to create and consume models.</span></span> <span data-ttu-id="ebf53-258">고급 데이터 탐색 및 모델링 Notebook은 교차 유효성 검사, 하이퍼 매개 변수 비우기 및 모델 평가를 포함하는 방법을 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-258">The advanced data exploration and modeling notebook shows how to include cross-validation, hyper-parameter sweeping, and model evaluation.</span></span> 

<span data-ttu-id="ebf53-259">**Spark로 데이터 탐색 및 모델링:** [Spark MLlib 도구 키트를 사용하여 데이터에 대한 이진 분류 및 회귀 모델 만들기](machine-learning-data-science-spark-data-exploration-modeling.md) 항목을 수행하여 데이터 집합을 탐색하고 기계 학습 모델 만들기, 점수 매기기 및 평가를 수행합니다.</span><span class="sxs-lookup"><span data-stu-id="ebf53-259">**Data Exploration and modeling with Spark:** Explore the dataset and create, score, and evaluate the machine learning models by working through the [Create binary classification and regression models for data with the Spark MLlib toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) topic.</span></span>

<span data-ttu-id="ebf53-260">**모델 사용:** 이 항목에서 만든 분류 및 회귀 모델의 점수를 매기는 방법을 알아보려면 [Spark로 빌드된 기계 학습 모델 점수 매기기 및 평가](machine-learning-data-science-spark-model-consumption.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-260">**Model consumption:** To learn how to score the classification and regression models created in this topic, see [Score and evaluate Spark-built machine learning models](machine-learning-data-science-spark-model-consumption.md).</span></span>

<span data-ttu-id="ebf53-261">**교차 유효성 검사 및 하이퍼 매개 변수 비우기**: 교차 유효성 검사 및 하이퍼 매개 변수 비우기를 사용하여 모델을 학습하는 방법은 [Spark로 고급 데이터 탐색 및 모델링](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) 을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="ebf53-261">**Cross-validation and hyperparameter sweeping**: See [Advanced data exploration and modeling with Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) on how models can be trained using cross-validation and hyper-parameter sweeping</span></span>

