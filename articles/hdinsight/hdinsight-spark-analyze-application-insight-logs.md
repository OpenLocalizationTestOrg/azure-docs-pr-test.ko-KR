---
title: "Spark를 사용하여 Application Insights 로그 분석 - Azure HDInsight | Microsoft Docs"
description: "Application Insight 로그를 Blob 저장소에 내보낸 다음 HDInsight에서 Spark를 사용하여 로그를 분석하는 방법을 알아봅니다."
services: hdinsight
documentationcenter: 
author: Blackmist
manager: jhubbard
editor: cgronlun
ms.assetid: 883beae6-9839-45b5-94f7-7eb0f4534ad5
ms.service: hdinsight
ms.custom: hdinsightactive
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 08/15/2017
ms.author: larryfr
ms.openlocfilehash: d98e403683618ef6115372f99e4949af87af4490
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/18/2017
---
# <a name="analyze-application-insights-telemetry-logs-with-spark-on-hdinsight"></a><span data-ttu-id="b40d2-103">HDInsight에서 Spark를 사용하여 Application Insights 원격 분석 로그 분석</span><span class="sxs-lookup"><span data-stu-id="b40d2-103">Analyze Application Insights telemetry logs with Spark on HDInsight</span></span>

<span data-ttu-id="b40d2-104">HDInsight에서 Spark를 사용하여 Application Insights 원격 분석 데이터를 분석하는 방법에 대해 알아봅니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-104">Learn how to use Spark on HDInsight to analyze Application Insight telemetry data.</span></span>

<span data-ttu-id="b40d2-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) 는 웹 응용 프로그램을 모니터링하는 분석 서비스입니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) is an analytics service that monitors your web applications.</span></span> <span data-ttu-id="b40d2-106">Application Insights에 의해 생성된 원격 분석 데이터를 Azure Storage로 내보낼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-106">Telemetry data generated by Application Insights can be exported to Azure Storage.</span></span> <span data-ttu-id="b40d2-107">데이터가 Azure Storage에 있으면 HDInsight를 사용하여 분석할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-107">Once the data is in Azure Storage, HDInsight can be used to analyze it.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="b40d2-108">필수 조건</span><span class="sxs-lookup"><span data-stu-id="b40d2-108">Prerequisites</span></span>

* <span data-ttu-id="b40d2-109">응용 프로그램에서 Application Insights를 사용하도록 구성합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-109">An application that is configured to use Application Insights.</span></span>

* <span data-ttu-id="b40d2-110">Linux 기반 HDInsight 클러스터를 만드는 데 익숙해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-110">Familiarity with creating a Linux-based HDInsight cluster.</span></span> <span data-ttu-id="b40d2-111">자세한 내용은 [HDInsight에서 Spark 만들기](hdinsight-apache-spark-jupyter-spark-sql.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="b40d2-111">For more information, see [Create Spark on HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

  > [!IMPORTANT]
  > <span data-ttu-id="b40d2-112">이 문서의 단계에는 Linux를 사용하는 HDInsight 클러스터가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-112">The steps in this document require an HDInsight cluster that uses Linux.</span></span> <span data-ttu-id="b40d2-113">Linux는 HDInsight 버전 3.4 이상에서 사용되는 유일한 운영 체제입니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-113">Linux is the only operating system used on HDInsight version 3.4 or greater.</span></span> <span data-ttu-id="b40d2-114">자세한 내용은 [Windows에서 HDInsight 사용 중지](hdinsight-component-versioning.md#hdinsight-windows-retirement)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="b40d2-114">For more information, see [HDInsight retirement on Windows](hdinsight-component-versioning.md#hdinsight-windows-retirement).</span></span>

* <span data-ttu-id="b40d2-115">웹 브라우저.</span><span class="sxs-lookup"><span data-stu-id="b40d2-115">A web browser.</span></span>

<span data-ttu-id="b40d2-116">이 문서를 개발하고 테스트하는 데 다음 리소스를 사용했습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-116">The following resources were used in developing and testing this document:</span></span>

* <span data-ttu-id="b40d2-117">[Application Insights를 사용하도록 구성된 Node.js 웹앱](../application-insights/app-insights-nodejs.md)를 사용하여 Application Insights 원격 분석 데이터를 생성했습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-117">Application Insights telemetry data was generated using a [Node.js web app configured to use Application Insights](../application-insights/app-insights-nodejs.md).</span></span>

* <span data-ttu-id="b40d2-118">HDInsight 클러스터 버전 3.5의 Linux 기반 Spark는 데이터를 분석하는 데 사용되었습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-118">A Linux-based Spark on HDInsight cluster version 3.5 was used to analyze the data.</span></span>

## <a name="architecture-and-planning"></a><span data-ttu-id="b40d2-119">아키텍처 및 계획</span><span class="sxs-lookup"><span data-stu-id="b40d2-119">Architecture and planning</span></span>

<span data-ttu-id="b40d2-120">다음 다이어그램은 이 예제의 서비스 아키텍처를 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-120">The following diagram illustrates the service architecture of this example:</span></span>

![다이어그램에서는 데이터를 Application Insights에서 Blob 저장소로 전달하고 HDInsight의 Spark에서 처리하는 방법을 보여 줍니다.](./media/hdinsight-spark-analyze-application-insight-logs/appinsightshdinsight.png)

### <a name="azure-storage"></a><span data-ttu-id="b40d2-122">Azure 저장소</span><span class="sxs-lookup"><span data-stu-id="b40d2-122">Azure storage</span></span>

<span data-ttu-id="b40d2-123">Application Insights가 Blob에 원격 분석 정보를 지속적으로 내보내도록 구성될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-123">Application Insights can be configured to continuously export telemetry information to blobs.</span></span> <span data-ttu-id="b40d2-124">그러면 HDInsight는 Blob에 저장된 데이터를 읽을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-124">HDInsight can then read data stored in the blobs.</span></span> <span data-ttu-id="b40d2-125">그러나 따라야 할 몇 가지 요구 사항이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-125">However, there are some requirements that you must follow:</span></span>

* <span data-ttu-id="b40d2-126">**위치**: 저장소 계정 및 HDInsight가 다른 위치에 있는 경우 대기 시간이 증가할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-126">**Location**: If the Storage Account and HDInsight are in different locations, it may increase latency.</span></span> <span data-ttu-id="b40d2-127">또한 지역 간에 이동하는 데이터에 송신 요금이 적용되면 비용이 증가합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-127">It also increases cost, as egress charges are applied to data moving between regions.</span></span>

    > [!WARNING]
    > <span data-ttu-id="b40d2-128">HDInsight와 다른 위치에서는 저장소 계정을 사용할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-128">Using a Storage Account in a different location than HDInsight is not supported.</span></span>

* <span data-ttu-id="b40d2-129">**Blob 유형**: HDInsight는 블록 Blob만을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-129">**Blob type**: HDInsight only supports block blobs.</span></span> <span data-ttu-id="b40d2-130">Application Insights의 기본값은 블록 Blob을 사용하므로 기본적으로 HDInsight와 함께 사용해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-130">Application Insights defaults to using block blobs, so should work by default with HDInsight.</span></span>

<span data-ttu-id="b40d2-131">기존 HDInsight 클러스터에 추가 저장소를 추가하는 방법에 대한 내용은 [추가 저장소 계정 추가](hdinsight-hadoop-add-storage.md) 문서를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="b40d2-131">For information on adding additional storage to an existing HDInsight cluster, see the [Add additional storage accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

### <a name="data-schema"></a><span data-ttu-id="b40d2-132">데이터 스키마</span><span class="sxs-lookup"><span data-stu-id="b40d2-132">Data schema</span></span>

<span data-ttu-id="b40d2-133">Application Insights는 Blob으로 내보낸 원격 분석 데이터 형식에 대한 [데이터 모델 내보내기](../application-insights/app-insights-export-data-model.md) 정보를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-133">Application Insights provides [export data model](../application-insights/app-insights-export-data-model.md) information for the telemetry data format exported to blobs.</span></span> <span data-ttu-id="b40d2-134">이 문서의 단계에서는 Spark SQL을 데이터와 함께 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-134">The steps in this document use Spark SQL to work with the data.</span></span> <span data-ttu-id="b40d2-135">Spark SQL은 Application Insights에 의해 기록된 JSON 데이터 구조체에 대한 스키마를 자동으로 생성할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-135">Spark SQL can automatically generate a schema for the JSON data structure logged by Application Insights.</span></span>

## <a name="export-telemetry-data"></a><span data-ttu-id="b40d2-136">원격 분석 데이터 내보내기</span><span class="sxs-lookup"><span data-stu-id="b40d2-136">Export telemetry data</span></span>

<span data-ttu-id="b40d2-137">[연속 내보내기 구성](../application-insights/app-insights-export-telemetry.md) 의 단계에 따라 Azure Storage Blob으로 원격 분석 정보를 내보내도록 Application Insights를 구성할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-137">Follow the steps in [Configure Continuous Export](../application-insights/app-insights-export-telemetry.md) to configure your Application Insights to export telemetry information to an Azure storage blob.</span></span>

## <a name="configure-hdinsight-to-access-the-data"></a><span data-ttu-id="b40d2-138">HDInsight를 구성하여 데이터에 액세스</span><span class="sxs-lookup"><span data-stu-id="b40d2-138">Configure HDInsight to access the data</span></span>

<span data-ttu-id="b40d2-139">HDInsight 클러스터를 만드는 경우에 클러스터를 만드는 동안 저장소 계정을 추가합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-139">If you are creating an HDInsight cluster, add the storage account during cluster creation.</span></span>

<span data-ttu-id="b40d2-140">Azure Storage 계정을 기존 클러스터에 추가하려면 [추가 저장소 계정 추가](hdinsight-hadoop-add-storage.md) 문서의 내용을 사용하세요.</span><span class="sxs-lookup"><span data-stu-id="b40d2-140">To add the Azure Storage Account to an existing cluster, use the information in the [Add additional Storage Accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

## <a name="analyze-the-data-pyspark"></a><span data-ttu-id="b40d2-141">데이터 분석: PySpark</span><span class="sxs-lookup"><span data-stu-id="b40d2-141">Analyze the data: PySpark</span></span>

1. <span data-ttu-id="b40d2-142">[Azure 포털](https://portal.azure.com)에서 HDInsight 클러스터의 Spark를 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-142">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="b40d2-143">**빠른 링크** 섹션에서 **클러스터 대시보드**를 선택한 다음 클러스터 대시보드__ 블레이드에서 **Jupyter Notebook**을 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-143">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![클러스터 대시보드](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)

2. <span data-ttu-id="b40d2-145">Jupyter 페이지의 오른쪽 위 모퉁이에서 **새로 만들기**, **PySpark**를 차례로 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-145">In the upper right corner of the Jupyter page, select **New**, and then **PySpark**.</span></span> <span data-ttu-id="b40d2-146">Python 기반 Jupyter Notebook을 포함하는 새 브라우저 탭이 열립니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-146">A new browser tab containing a Python-based Jupyter Notebook opens.</span></span>

3. <span data-ttu-id="b40d2-147">페이지의 첫 번째 필드(**셀**이라고 함)에 다음 텍스트를 입력합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-147">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```python
   sc._jsc.hadoopConfiguration().set('mapreduce.input.fileinputformat.input.dir.recursive', 'true')
   ```

    <span data-ttu-id="b40d2-148">이 코드는 Spark가 입력 데이터에 대한 디렉터리 구조에 재귀적으로 액세스하도록 구성합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-148">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="b40d2-149">Application Insights 원격 분석은 `/{telemetry type}/YYYY-MM-DD/{##}/`과 유사한 디렉터리 구조에 기록됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-149">Application Insights telemetry is logged to a directory structure similar to the `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="b40d2-150">**SHIFT+ENTER** 를 사용하여 코드를 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-150">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="b40d2-151">'\*'가 셀의 왼쪽에 대괄호 사이에 표시되면 이 셀의 코드가 실행되고 있음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-151">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="b40d2-152">완료되면 '\*'는 번호로 변경되고 셀 아래에 다음 텍스트와 유사한 출력이 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-152">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    pyspark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="b40d2-153">새 셀은 첫 번째 셀의 아래에 생성됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-153">A new cell is created below the first one.</span></span> <span data-ttu-id="b40d2-154">새 셀에서 다음 텍스트를 입력합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-154">Enter the following text in the new cell.</span></span> <span data-ttu-id="b40d2-155">`CONTAINER` 및 `STORAGEACCOUNT`를 Application Insights 데이터를 포함하는 Azure Storage 계정 이름 및 BLOB 컨테이너 이름으로 바꿉니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-155">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights data.</span></span>

   ```python
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="b40d2-156">**SHIFT+ENTER**를 사용하여 이 셀을 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-156">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="b40d2-157">다음 텍스트와 유사한 결과가 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-157">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="b40d2-158">반환된 wasb 경로는 Application Insights 원격 분석 데이터의 위치입니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-158">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="b40d2-159">셀에서 `hdfs dfs -ls` 줄을 변경하여 반환된 wasb 경로를 사용한 다음 **SHIFT+ENTER**를 사용하여 셀을 다시 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-159">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="b40d2-160">이번 결과는 원격 분석 데이터를 포함하는 디렉터리를 표시해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-160">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="b40d2-161">이 섹션의 나머지 단계에서는 `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` 디렉터리를 사용했습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-161">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="b40d2-162">사용자의 디렉터리 구조는 다를 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-162">Your directory structure may be different.</span></span>

6. <span data-ttu-id="b40d2-163">다음 셀에서 다음 코드를 입력합니다. `WASB_PATH`를 이전 단계의 경로로 바꿉니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-163">In the next cell, enter the following code: Replace `WASB_PATH` with the path from the previous step.</span></span>

   ```python
   jsonFiles = sc.textFile('WASB_PATH')
   jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="b40d2-164">이 코드는 연속 내보내기 프로세스에서 내보낸 JSON 파일에서 데이터 프레임을 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-164">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="b40d2-165">**SHIFT+ENTER** 를 사용하여 이 셀을 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-165">Use **SHIFT+ENTER** to run this cell.</span></span>
7. <span data-ttu-id="b40d2-166">다음 셀에서 다음을 입력하고 실행하여 Spark가 JSON 파일에 대해 만든 스키마를 봅니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-166">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```python
   jsonData.printSchema()
   ```

    <span data-ttu-id="b40d2-167">각 유형의 원격 분석에 대한 스키마는 달라질 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-167">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="b40d2-168">다음 예제는 웹 요청(`Requests` 하위 디렉터리에 저장된 데이터)에 대해 생성되는 스키마입니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-168">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)
8. <span data-ttu-id="b40d2-169">다음을 사용하여 데이터 프레임을 임시 테이블로 등록하고 데이터에 대해 쿼리를 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-169">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```python
   jsonData.registerTempTable("requests")
   df = sqlContext.sql("select context.location.city from requests where context.location.city is not null")
   df.show()
   ```

    <span data-ttu-id="b40d2-170">이 쿼리는 context.location.city가 null이 아닌 상위 20개 레코드에 대한 도시 정보를 반환합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-170">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="b40d2-171">컨텍스트 구조는 Application Insights에 의해 기록된 모든 원격 분석에 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-171">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="b40d2-172">도시 요소는 로그에서 채워지지 않을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-172">The city element may not be populated in your logs.</span></span> <span data-ttu-id="b40d2-173">스키마를 사용하여 로그에 대한 데이터를 포함하는 쿼리할 수 있는 다른 요소를 식별합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-173">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>

    <span data-ttu-id="b40d2-174">이 쿼리는 다음 텍스트와 비슷한 정보를 반환합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-174">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="analyze-the-data-scala"></a><span data-ttu-id="b40d2-175">데이터 분석: Scala</span><span class="sxs-lookup"><span data-stu-id="b40d2-175">Analyze the data: Scala</span></span>

1. <span data-ttu-id="b40d2-176">[Azure 포털](https://portal.azure.com)에서 HDInsight 클러스터의 Spark를 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-176">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="b40d2-177">**빠른 링크** 섹션에서 **클러스터 대시보드**를 선택한 다음 클러스터 대시보드__ 블레이드에서 **Jupyter Notebook**을 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-177">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![클러스터 대시보드](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)
2. <span data-ttu-id="b40d2-179">Jupyter 페이지의 오른쪽 위 모퉁이에서 **새로 만들기**, **Scala**를 차례로 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-179">In the upper right corner of the Jupyter page, select **New**, and then **Scala**.</span></span> <span data-ttu-id="b40d2-180">Scala 기반 Jupyter Notebook을 포함하는 새 브라우저 탭이 나타납니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-180">A new browser tab containing a Scala-based Jupyter Notebook appears.</span></span>
3. <span data-ttu-id="b40d2-181">페이지의 첫 번째 필드(**셀**이라고 함)에 다음 텍스트를 입력합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-181">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```scala
   sc.hadoopConfiguration.set("mapreduce.input.fileinputformat.input.dir.recursive", "true")
   ```

    <span data-ttu-id="b40d2-182">이 코드는 Spark가 입력 데이터에 대한 디렉터리 구조에 재귀적으로 액세스하도록 구성합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-182">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="b40d2-183">Application Insights 원격 분석은 `/{telemetry type}/YYYY-MM-DD/{##}/`과 유사한 디렉터리 구조에 기록됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-183">Application Insights telemetry is logged to a directory structure similar to `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="b40d2-184">**SHIFT+ENTER** 를 사용하여 코드를 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-184">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="b40d2-185">'\*'가 셀의 왼쪽에 대괄호 사이에 표시되면 이 셀의 코드가 실행되고 있음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-185">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="b40d2-186">완료되면 '\*'는 번호로 변경되고 셀 아래에 다음 텍스트와 유사한 출력이 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-186">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    spark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="b40d2-187">새 셀은 첫 번째 셀의 아래에 생성됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-187">A new cell is created below the first one.</span></span> <span data-ttu-id="b40d2-188">새 셀에서 다음 텍스트를 입력합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-188">Enter the following text in the new cell.</span></span> <span data-ttu-id="b40d2-189">`CONTAINER` 및 `STORAGEACCOUNT`를 Application Insights 로그를 포함하는 Azure Storage 계정 이름 및 BLOB 컨테이너 이름으로 바꿉니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-189">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights logs.</span></span>

   ```scala
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="b40d2-190">**SHIFT+ENTER**를 사용하여 이 셀을 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-190">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="b40d2-191">다음 텍스트와 유사한 결과가 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-191">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="b40d2-192">반환된 wasb 경로는 Application Insights 원격 분석 데이터의 위치입니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-192">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="b40d2-193">셀에서 `hdfs dfs -ls` 줄을 변경하여 반환된 wasb 경로를 사용한 다음 **SHIFT+ENTER**를 사용하여 셀을 다시 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-193">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="b40d2-194">이번 결과는 원격 분석 데이터를 포함하는 디렉터리를 표시해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-194">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="b40d2-195">이 섹션의 나머지 단계에서는 `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` 디렉터리를 사용했습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-195">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="b40d2-196">원격 분석 데이터가 웹앱에 대한 것이 아니면 이 디렉터리는 없을 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-196">This directory may not exist unless your telemetry data is for a web app.</span></span>

6. <span data-ttu-id="b40d2-197">다음 셀에서 다음 코드를 입력합니다. `WASB\_PATH`를 이전 단계의 경로로 바꿉니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-197">In the next cell, enter the following code: Replace `WASB\_PATH` with the path from the previous step.</span></span>

   ```scala
   var jsonFiles = sc.textFile('WASB_PATH')
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   var jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="b40d2-198">이 코드는 연속 내보내기 프로세스에서 내보낸 JSON 파일에서 데이터 프레임을 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-198">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="b40d2-199">**SHIFT+ENTER** 를 사용하여 이 셀을 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-199">Use **SHIFT+ENTER** to run this cell.</span></span>

7. <span data-ttu-id="b40d2-200">다음 셀에서 다음을 입력하고 실행하여 Spark가 JSON 파일에 대해 만든 스키마를 봅니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-200">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```scala
   jsonData.printSchema
   ```

    <span data-ttu-id="b40d2-201">각 유형의 원격 분석에 대한 스키마는 달라질 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-201">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="b40d2-202">다음 예제는 웹 요청(`Requests` 하위 디렉터리에 저장된 데이터)에 대해 생성되는 스키마입니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-202">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)

8. <span data-ttu-id="b40d2-203">다음을 사용하여 데이터 프레임을 임시 테이블로 등록하고 데이터에 대해 쿼리를 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-203">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```scala
   jsonData.registerTempTable("requests")
   var city = sqlContext.sql("select context.location.city from requests where context.location.city is not null limit 10").show()
   ```

    <span data-ttu-id="b40d2-204">이 쿼리는 context.location.city가 null이 아닌 상위 20개 레코드에 대한 도시 정보를 반환합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-204">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="b40d2-205">컨텍스트 구조는 Application Insights에 의해 기록된 모든 원격 분석에 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-205">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="b40d2-206">도시 요소는 로그에서 채워지지 않을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-206">The city element may not be populated in your logs.</span></span> <span data-ttu-id="b40d2-207">스키마를 사용하여 로그에 대한 데이터를 포함하는 쿼리할 수 있는 다른 요소를 식별합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-207">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>
   >
   >

    <span data-ttu-id="b40d2-208">이 쿼리는 다음 텍스트와 비슷한 정보를 반환합니다.</span><span class="sxs-lookup"><span data-stu-id="b40d2-208">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="next-steps"></a><span data-ttu-id="b40d2-209">다음 단계</span><span class="sxs-lookup"><span data-stu-id="b40d2-209">Next steps</span></span>

<span data-ttu-id="b40d2-210">Azure의 데이터와 서비스로 작업하기 위해 Spark를 사용하는 방법의 자세한 예제는 다음 문서를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="b40d2-210">For more examples of using Spark to work with data and services in Azure, see the following documents:</span></span>

* [<span data-ttu-id="b40d2-211">BI와 Spark: BI 도구와 함께 HDInsight에서 Spark를 사용하여 대화형 데이터 분석 수행</span><span class="sxs-lookup"><span data-stu-id="b40d2-211">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="b40d2-212">기계 학습과 Spark: HVAC 데이터를 사용하여 건물 온도를 분석하는 데 HDInsight의 Spark 사용</span><span class="sxs-lookup"><span data-stu-id="b40d2-212">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="b40d2-213">기계 학습과 Spark: 음식 검사 결과를 예측하는 데 HDInsight의 Spark 사용</span><span class="sxs-lookup"><span data-stu-id="b40d2-213">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="b40d2-214">Spark 스트리밍: HDInsight에서 Spark를 사용하여 스트리밍 응용 프로그램 빌드</span><span class="sxs-lookup"><span data-stu-id="b40d2-214">Spark Streaming: Use Spark in HDInsight for building streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="b40d2-215">HDInsight의 Spark를 사용하여 웹 사이트 로그 분석</span><span class="sxs-lookup"><span data-stu-id="b40d2-215">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

<span data-ttu-id="b40d2-216">Spark 응용 프로그램을 만들고 실행하는 자세한 내용은 다음 문서를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="b40d2-216">For information on creating and running Spark applications, see the following documents:</span></span>

* [<span data-ttu-id="b40d2-217">Scala를 사용하여 독립 실행형 응용 프로그램 만들기</span><span class="sxs-lookup"><span data-stu-id="b40d2-217">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="b40d2-218">Livy를 사용하여 Spark 클러스터에서 원격으로 작업 실행</span><span class="sxs-lookup"><span data-stu-id="b40d2-218">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)
