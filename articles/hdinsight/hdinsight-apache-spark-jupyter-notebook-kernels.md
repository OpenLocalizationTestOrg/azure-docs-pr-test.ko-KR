---
title: "Azure HDInsight에서 Spark 클러스터의 Jupyter 노트북에 대한 커널 | Microsoft Docs"
description: "Azure HDInsight에서 Spark 클러스터와 함께 Jupyter 노트북에 사용할 수 있는 PySpark, PySpark3 및 Spark 커널에 대해 알아봅니다."
keywords: "spark의 jupyter 노트북, jupyter spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="4cbda-104">Azure HDInsight에서 Spark 클러스터의 Jupyter 노트북에 대한 커널</span><span class="sxs-lookup"><span data-stu-id="4cbda-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="4cbda-105">HDInsight Spark 클러스터는 응용 프로그램 테스트를 위해 Spark에서 Jupyter 노트북과 함께 사용할 수 있는 커널을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="4cbda-106">커널은 코드를 실행하고 해석하는 프로그램입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="4cbda-107">세 개의 커널은 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-107">The three kernels are:</span></span>

- <span data-ttu-id="4cbda-108">**PySpark** - Python2에서 작성한 응용 프로그램용</span><span class="sxs-lookup"><span data-stu-id="4cbda-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="4cbda-109">**PySpark3** - Python3에서 작성한 응용 프로그램용</span><span class="sxs-lookup"><span data-stu-id="4cbda-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="4cbda-110">**Spark** - Scala에서 작성한 응용 프로그램용</span><span class="sxs-lookup"><span data-stu-id="4cbda-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="4cbda-111">이 문서에서는 이러한 커널의 사용 방법과 사용 시의 이점에 대해 알아봅니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="4cbda-112">필수 조건</span><span class="sxs-lookup"><span data-stu-id="4cbda-112">Prerequisites</span></span>

* <span data-ttu-id="4cbda-113">HDInsight의 Apache Spark 클러스터.</span><span class="sxs-lookup"><span data-stu-id="4cbda-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="4cbda-114">자세한 내용은 [Azure HDInsight에서 Apache Spark 클러스터 만들기](hdinsight-apache-spark-jupyter-spark-sql.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="4cbda-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="4cbda-115">Spark HDInsight에서 Jupyter 노트북 만들기</span><span class="sxs-lookup"><span data-stu-id="4cbda-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="4cbda-116">[Azure Portal](https://portal.azure.com/)에서 클러스터를 엽니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="4cbda-117">지침에 대해서는 [클러스터 나열 및 표시](hdinsight-administer-use-portal-linux.md#list-and-show-clusters)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="4cbda-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="4cbda-118">클러스터는 새 포털 블레이드에서 열립니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="4cbda-119">**빠른 링크** 섹션에서 **클러스터 대시보드**를 클릭하여 **클러스터 대시보드** 블레이드를 엽니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="4cbda-120">**빠른 링크**가 표시되지 않으면 블레이드의 왼쪽 메뉴에서 **개요**를 클릭합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="4cbda-121">![Spark의 Jupyter 노트북](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Spark의 Jupyter 노트북")</span><span class="sxs-lookup"><span data-stu-id="4cbda-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="4cbda-122">**Jupyter Notebook**을 클릭합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="4cbda-123">메시지가 표시되면 클러스터에 대한 관리자 자격 증명을 입력합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="4cbda-124">또한 브라우저에서 다음 URL을 열어 Spark 클러스터의 Jupyter 노트북에 접근할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="4cbda-125">**CLUSTERNAME** 을 클러스터의 이름으로 바꿉니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="4cbda-126">**새로 만들기**를 클릭하고 **Pyspark**, **PySpark3** 또는 **Spark** 중 하나를 클릭하여 Notebook을 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="4cbda-127">Scala 응용 프로그램에 대해 Spark 커널을, Python2 응용 프로그램에 대해 PySpark 커널을, Python3 응용 프로그램에 대해 PySpark3 커널을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="4cbda-128">![Spark의 Jupyter 노트북에 대한 커널](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Spark의 Jupyter 노트북에 대한 커널")</span><span class="sxs-lookup"><span data-stu-id="4cbda-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="4cbda-129">선택한 커널로 Notebook이 열립니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="4cbda-130">커널을 사용할 경우의 이점</span><span class="sxs-lookup"><span data-stu-id="4cbda-130">Benefits of using the kernels</span></span>

<span data-ttu-id="4cbda-131">다음은 Spark HDInsight 클러스터에서 Jupyter 노트북과 함께 새 커널을 사용할 경우의 몇 가지 이점입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="4cbda-132">**컨텍스트를 미리 설정합니다**.</span><span class="sxs-lookup"><span data-stu-id="4cbda-132">**Preset contexts**.</span></span> <span data-ttu-id="4cbda-133">**PySpark**, **PySpark3** 또는 **Spark** 커널을 사용하면 응용 프로그램으로 작업을 시작하기 전에 Spark 또는 Hive 컨텍스트를 명시적으로 설정할 필요가 없습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="4cbda-134">이러한 컨텍스트는 기본적으로 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-134">These are available by default.</span></span> <span data-ttu-id="4cbda-135">이러한 컨텍스트는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-135">These contexts are:</span></span>
   
   * <span data-ttu-id="4cbda-136">**sc** - Spark 컨텍스트용</span><span class="sxs-lookup"><span data-stu-id="4cbda-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="4cbda-137">**sqlContext** - Hive 컨텍스트용</span><span class="sxs-lookup"><span data-stu-id="4cbda-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="4cbda-138">따라서 컨텍스트를 설정하기 위해 다음과 같은 문을 실행할 필요가 없습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="4cbda-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="4cbda-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="4cbda-140">대신 응용 프로그램에서 직접 미리 설정된 컨텍스트를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="4cbda-141">**매직 셀**입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-141">**Cell magics**.</span></span> <span data-ttu-id="4cbda-142">PySpark 커널은 특수 명령인 일부 미리 정의된 "매직"을 제공하며 이러한 매직은 `%%`(예: `%%MAGIC` <args>)를 사용하여 호출할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="4cbda-143">매직 명령은 코드 셀의 첫 번째 단어여야 하고 여러 콘텐츠 줄에 허용됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="4cbda-144">매직 단어는 셀의 첫 번째 단어여야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="4cbda-145">매직 앞에 다른 단어(주석 포함)가 있으면 오류가 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="4cbda-146">매직에 대한 자세한 내용은 [여기](http://ipython.readthedocs.org/en/stable/interactive/magics.html)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="4cbda-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="4cbda-147">다음 표에는 커널을 통해 사용할 수 있는 다양한 매직이 나열되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="4cbda-148">매직</span><span class="sxs-lookup"><span data-stu-id="4cbda-148">Magic</span></span> | <span data-ttu-id="4cbda-149">예</span><span class="sxs-lookup"><span data-stu-id="4cbda-149">Example</span></span> | <span data-ttu-id="4cbda-150">설명</span><span class="sxs-lookup"><span data-stu-id="4cbda-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="4cbda-151">help</span><span class="sxs-lookup"><span data-stu-id="4cbda-151">help</span></span> |`%%help` |<span data-ttu-id="4cbda-152">예제 및 설명과 함께 사용할 수 있는 모든 매직이 포함된 테이블을 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="4cbda-153">info</span><span class="sxs-lookup"><span data-stu-id="4cbda-153">info</span></span> |`%%info` |<span data-ttu-id="4cbda-154">현재 Livy 끝점에 대한 출력 세션 정보</span><span class="sxs-lookup"><span data-stu-id="4cbda-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="4cbda-155">구성</span><span class="sxs-lookup"><span data-stu-id="4cbda-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="4cbda-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="4cbda-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="4cbda-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="4cbda-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="4cbda-158">세션 만들기에 대한 매개 변수를 구성합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="4cbda-159">이미 세션이 만들어진 경우 강제 플래그(-f)가 필수이며 이렇게 하면 세션을 삭제 후 다시 만들 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="4cbda-160">유효한 매개 변수 목록은 [Livy의 POST /sessions Request Body](https://github.com/cloudera/livy#request-body) 를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="4cbda-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="4cbda-161">매개 변수는 JSON 문자열로 전달되어야 하며, 아래 예제 열과 같이 매직 뒤의 다음 줄에 있어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="4cbda-162">sql</span><span class="sxs-lookup"><span data-stu-id="4cbda-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="4cbda-163">sqlContext에 대해 Hive 쿼리를 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="4cbda-164">`-o` 매개 변수가 전달된 경우 쿼리 결과가 %%local Python 컨텍스트에서 [Pandas](http://pandas.pydata.org/) 데이터 프레임으로 유지됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="4cbda-165">local</span><span class="sxs-lookup"><span data-stu-id="4cbda-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="4cbda-166">다음 줄의 모든 코드는 로컬로 실행됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="4cbda-167">코드는 사용 중인 커널에 관계없이 유효한 Python2 코드여야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="4cbda-168">따라서 Notebook을 만드는 동안 **PySpark3** 또는 **Spark** 커널을 선택하더라도 셀에서 `%%local` 매직을 사용하면 해당 셀은 유효한 Python2 코드만 포함해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="4cbda-169">로그</span><span class="sxs-lookup"><span data-stu-id="4cbda-169">logs</span></span> |`%%logs` |<span data-ttu-id="4cbda-170">현재 Livy 세션에 대한 로그를 출력합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="4cbda-171">delete</span><span class="sxs-lookup"><span data-stu-id="4cbda-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="4cbda-172">현재 Livy 끝점의 특정 세션을 삭제합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="4cbda-173">커널 자체에 대해 시작된 세션은 삭제할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="4cbda-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="4cbda-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="4cbda-175">이 노트북의 세션을 포함하여 현재 Livy 끝점에 대한 모든 세션을 삭제합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="4cbda-176">강제 플래그 -f는 필수입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="4cbda-177">PySpark 커널에서 추가한 매직 외에도 `%%sh`를 포함하여 [기본 제공 IPython 매직](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics)도 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="4cbda-178">`%%sh` 매직을 사용하면 클러스터 헤드 노드에서 스크립트 및 코드 블록을 실행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="4cbda-179">**자동 시각화**.</span><span class="sxs-lookup"><span data-stu-id="4cbda-179">**Auto visualization**.</span></span> <span data-ttu-id="4cbda-180">**Pyspark** 커널은 Hive 및 SQL 쿼리 출력을 자동으로 시각화합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="4cbda-181">테이블, 원형, 선, 영역, 막대를 포함하여 다양한 시각화 형식 중에서 선택할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="4cbda-182">%%sql 매직에서 지원되는 매개 변수</span><span class="sxs-lookup"><span data-stu-id="4cbda-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="4cbda-183">`%%sql` 매직은 쿼리를 실행할 때 검색하는 출력 종류를 제어하는 데 사용할 수 있는 여러 매개 변수를 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="4cbda-184">다음 표에는 출력이 나와 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-184">The following table lists the output.</span></span>

| <span data-ttu-id="4cbda-185">매개 변수</span><span class="sxs-lookup"><span data-stu-id="4cbda-185">Parameter</span></span> | <span data-ttu-id="4cbda-186">예</span><span class="sxs-lookup"><span data-stu-id="4cbda-186">Example</span></span> | <span data-ttu-id="4cbda-187">설명</span><span class="sxs-lookup"><span data-stu-id="4cbda-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="4cbda-188">-o</span><span class="sxs-lookup"><span data-stu-id="4cbda-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="4cbda-189">이 매개 변수를 사용하여 쿼리 결과를 %%local Python 컨텍스트에서 [Pandas](http://pandas.pydata.org/) 데이터 프레임으로 유지할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="4cbda-190">데이터 프레임 변수 이름은 사용자가 지정한 변수 이름입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="4cbda-191">-q</span><span class="sxs-lookup"><span data-stu-id="4cbda-191">-q</span></span> |`-q` |<span data-ttu-id="4cbda-192">이 매개 변수를 사용하여 셀에 대한 시각화를 해제할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="4cbda-193">셀 내용을 자동으로 시각화하지 않고 데이터 프레임으로 캡처하기만 하려면 `-q -o <VARIABLE>`을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="4cbda-194">`CREATE TABLE` 문과 같은 SQL 쿼리를 실행하려는 등의 경우 결과를 캡처하지 않고 시각화를 해제하려면 `-o` 인수를 지정하지 않고 `-q`만 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="4cbda-195">-m</span><span class="sxs-lookup"><span data-stu-id="4cbda-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="4cbda-196">여기서 **METHOD**는 **take** 또는 **sample**(기본값: **take**)입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="4cbda-197">METHOD가 **take**인 경우 커널은 MAXROWS(이 표의 뒷부분에서 설명함)로 지정된 결과 데이터 집합의 맨 위에서부터 요소를 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="4cbda-198">METHOD가 **sample**인 경우 커널은 `-r` 매개 변수(이 표의 다음 행에서 설명함)에 따라 데이터 집합의 요소를 무작위로 샘플링합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="4cbda-199">-r</span><span class="sxs-lookup"><span data-stu-id="4cbda-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="4cbda-200">여기서 **FRACTION**은 0.0과 1.0 사이의 부동 소수점 숫자입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="4cbda-201">SQL 쿼리의 샘플 메서드가 `sample`인 경우 커널은 결과 집합 요소의 지정된 부분을 무작위로 샘플링합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="4cbda-202">예를 들어 `-m sample -r 0.01` 인수를 포함하여 SQL 쿼리를 실행할 경우 결과 행의 1%가 무작위로 샘플링됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="4cbda-203">**MAXROWS** 는 정수 값입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="4cbda-204">커널은 출력 행 수를 **MAXROWS**로 제한합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="4cbda-205">**MAXROWS**가 **-1**과 같은 음수인 경우에는 결과 집합의 행 수가 제한되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="4cbda-206">**예제:**</span><span class="sxs-lookup"><span data-stu-id="4cbda-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="4cbda-207">위의 문은 다음 작업을 수행합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-207">The statement above does the following:</span></span>

* <span data-ttu-id="4cbda-208">**hivesampletable**에서 모든 레코드를 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="4cbda-209">-q를 사용하기 때문에 자동 시각화를 해제합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="4cbda-210">`-m sample -r 0.1 -n 500` 을 사용하기 때문에 hivesampletable에서 행의 10%를 무작위로 샘플링하고 결과 집합의 크기를 500개 행으로 제한합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="4cbda-211">마지막으로, `-o query2` 를 사용하기 때문에 출력을 **query2**라는 데이터 프레임에도 저장합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="4cbda-212">새 커널을 사용하는 동안 고려 사항</span><span class="sxs-lookup"><span data-stu-id="4cbda-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="4cbda-213">사용하는 커널에 따라 Notebook을 실행 중인 상태로 두면 클러스터 리소스가 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="4cbda-214">이러한 커널의 경우 컨텍스트가 미리 설정되어 있기 때문에 Notebook이 컨텍스트를 종료하지 않으므로 클러스터 리소스는 계속 사용 중인 것으로 됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="4cbda-215">Notebook을 다 사용했으면 Notebook의 **파일** 메뉴에서 **닫기 및 중지** 옵션을 사용하는 것이 좋습니다. 이 옵션은 컨텍스트를 종료한 후 Notebook을 종료합니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="4cbda-216">몇 가지 예제 보기</span><span class="sxs-lookup"><span data-stu-id="4cbda-216">Show me some examples</span></span>

<span data-ttu-id="4cbda-217">Jupyter Notebook을 여는 경우 루트 수준에서 사용할 수 있는 두 개의 폴더가 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="4cbda-218">**PySpark** 폴더에는 새 **Python** 커널을 사용하는 샘플 Notebook이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="4cbda-219">**Scala** 폴더에는 새 **Spark** 커널을 사용하는 샘플 Notebook이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="4cbda-220">**PySpark** 또는 **Spark** 폴더에서 **00 - [READ ME FIRST] Spark 매직 커널 기능** Notebook을 열어 사용 가능한 다양한 매직에 대해 살펴볼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="4cbda-221">두 폴더에서 사용할 수 있는 다른 샘플 노트북을 사용하여 Jupyter 노트북을 HDInsight Spark 클러스터와 함께 사용하는 다양한 시나리오에 대해 알아볼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="4cbda-222">Notebook이 저장되는 위치</span><span class="sxs-lookup"><span data-stu-id="4cbda-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="4cbda-223">Jupyter Notebook은 클러스터와 연결된 저장소 계정의 **/HdiNotebooks** 폴더 아래에 저장됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="4cbda-224">Jupyter 내에서 만든 Notebook, 텍스트 파일 및 폴더는 저장소 계정에서 액세스할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="4cbda-225">예를 들어 Jupyter를 사용하여 **myfolder** 폴더와 **myfolder/mynotebook.ipynb** Notebook을 만든 경우 저장소 계정 내, `/HdiNotebooks/myfolder/mynotebook.ipynb`에서 이 Notebook에 액세스할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="4cbda-226">반대의 경우도 마찬가지입니다. 즉, `/HdiNotebooks/mynotebook1.ipynb`에서 저장소 계정에 직접 Notebook을 업로드한 경우 Jupyter에서도 이 Notebook을 볼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="4cbda-227">Notebook은 클러스터를 삭제한 후에도 저장소 계정에 유지됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="4cbda-228">Notebook이 저장소 계정에 저장되는 방식은 HDFS와 호환됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="4cbda-229">따라서 클러스터에 SSH 연결을 설정한 경우 다음 코드 조각에 표시된 것처럼 파일 관리 명령을 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="4cbda-230">클러스터의 저장소 계정에 액세스하는 데 문제가 있는 경우 헤드 노드 `/var/lib/jupyter`에도 노트북이 저장됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="4cbda-231">지원되는 브라우저</span><span class="sxs-lookup"><span data-stu-id="4cbda-231">Supported browser</span></span>

<span data-ttu-id="4cbda-232">Spark HDInsight 클러스터의 Jupyter 노트북은 Google Chrome에서만 지원됩니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="4cbda-233">사용자 의견</span><span class="sxs-lookup"><span data-stu-id="4cbda-233">Feedback</span></span>
<span data-ttu-id="4cbda-234">새로운 커널도 현재 개발 중이며 곧 완성될 예정입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="4cbda-235">이는 API가 이러한 커널의 성숙에 따라 변경될 수 있음을 의미할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="4cbda-236">이러한 새로운 커널을 사용하는 동안 가진 의견을 보내주시면 감사하겠습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="4cbda-237">이러한 커널의 최종 릴리스 형성에 유용할 것입니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="4cbda-238">이 문서의 맨 아래 **의견** 섹션 아래에 의견/피드백을 남길 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="4cbda-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="4cbda-239"><a name="seealso"></a>참고 항목</span><span class="sxs-lookup"><span data-stu-id="4cbda-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="4cbda-240">개요: Azure HDInsight에서 Apache Spark</span><span class="sxs-lookup"><span data-stu-id="4cbda-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="4cbda-241">시나리오</span><span class="sxs-lookup"><span data-stu-id="4cbda-241">Scenarios</span></span>
* [<span data-ttu-id="4cbda-242">BI와 Spark: BI 도구와 함께 HDInsight에서 Spark를 사용하여 대화형 데이터 분석 수행</span><span class="sxs-lookup"><span data-stu-id="4cbda-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="4cbda-243">기계 학습과 Spark: HVAC 데이터를 사용하여 건물 온도를 분석하는 데 HDInsight의 Spark 사용</span><span class="sxs-lookup"><span data-stu-id="4cbda-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="4cbda-244">기계 학습과 Spark: 음식 검사 결과를 예측하는 데 HDInsight의 Spark 사용</span><span class="sxs-lookup"><span data-stu-id="4cbda-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="4cbda-245">Spark 스트리밍: HDInsight에서 Spark를 사용하여 실시간 스트리밍 응용 프로그램 빌드</span><span class="sxs-lookup"><span data-stu-id="4cbda-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="4cbda-246">HDInsight의 Spark를 사용하여 웹 사이트 로그 분석</span><span class="sxs-lookup"><span data-stu-id="4cbda-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="4cbda-247">응용 프로그램 만들기 및 실행</span><span class="sxs-lookup"><span data-stu-id="4cbda-247">Create and run applications</span></span>
* [<span data-ttu-id="4cbda-248">Scala를 사용하여 독립 실행형 응용 프로그램 만들기</span><span class="sxs-lookup"><span data-stu-id="4cbda-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="4cbda-249">Livy를 사용하여 Spark 클러스터에서 원격으로 작업 실행</span><span class="sxs-lookup"><span data-stu-id="4cbda-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="4cbda-250">도구 및 확장</span><span class="sxs-lookup"><span data-stu-id="4cbda-250">Tools and extensions</span></span>
* [<span data-ttu-id="4cbda-251">IntelliJ IDEA용 HDInsight 도구 플러그 인을 사용하여 Spark Scala 응용 프로그램 만들기 및 제출</span><span class="sxs-lookup"><span data-stu-id="4cbda-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="4cbda-252">IntelliJ IDEA용 HDInsight 도구 플러그 인을 사용하여 Spark 응용 프로그램을 원격으로 디버그</span><span class="sxs-lookup"><span data-stu-id="4cbda-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="4cbda-253">HDInsight에서 Spark 클러스터와 함께 Zeppelin Notebook 사용</span><span class="sxs-lookup"><span data-stu-id="4cbda-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="4cbda-254">Jupyter 노트북에서 외부 패키지 사용</span><span class="sxs-lookup"><span data-stu-id="4cbda-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="4cbda-255">컴퓨터에 Jupyter를 설치하고 HDInsight Spark 클러스터에 연결</span><span class="sxs-lookup"><span data-stu-id="4cbda-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="4cbda-256">리소스 관리</span><span class="sxs-lookup"><span data-stu-id="4cbda-256">Manage resources</span></span>
* [<span data-ttu-id="4cbda-257">Azure HDInsight에서 Apache Spark 클러스터에 대한 리소스 관리</span><span class="sxs-lookup"><span data-stu-id="4cbda-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="4cbda-258">HDInsight의 Apache Spark 클러스터에서 실행되는 작업 추적 및 디버그</span><span class="sxs-lookup"><span data-stu-id="4cbda-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
