---
title: "Azure HDInsight의 Spark에 Jupyter 노트북에 대 한 aaaKernels 클러스터 | Microsoft Docs"
description: "Azure HDInsight의 Spark 클러스터와 함께 사용할 수 있는 Jupyter 노트북에 대 한 hello PySpark, PySpark3, 및 Spark 커널에 알아봅니다."
keywords: "spark의 jupyter 노트북, jupyter spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 560c944fe850c5753ac9fa90550b804f0c47d14c
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="09cee-104">Azure HDInsight에서 Spark 클러스터의 Jupyter 노트북에 대한 커널</span><span class="sxs-lookup"><span data-stu-id="09cee-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="09cee-105">HDInsight Spark 클러스터 응용 프로그램을 테스트 하기 위한 Spark에 hello Jupyter 노트북으로 사용할 수 있는 커널을 제공 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-105">HDInsight Spark clusters provide kernels that you can use with hello Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="09cee-106">커널은 코드를 실행하고 해석하는 프로그램입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="09cee-107">hello 세 커널 됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-107">hello three kernels are:</span></span>

- <span data-ttu-id="09cee-108">**PySpark** - Python2에서 작성한 응용 프로그램용</span><span class="sxs-lookup"><span data-stu-id="09cee-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="09cee-109">**PySpark3** - Python3에서 작성한 응용 프로그램용</span><span class="sxs-lookup"><span data-stu-id="09cee-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="09cee-110">**Spark** - Scala에서 작성한 응용 프로그램용</span><span class="sxs-lookup"><span data-stu-id="09cee-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="09cee-111">이 문서에서는 설명 어떻게 toouse 이러한 커널 및 어설션을 사용 하 여 hello 이점입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-111">In this article, you learn how toouse these kernels and hello benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="09cee-112">필수 조건</span><span class="sxs-lookup"><span data-stu-id="09cee-112">Prerequisites</span></span>

* <span data-ttu-id="09cee-113">HDInsight의 Apache Spark 클러스터.</span><span class="sxs-lookup"><span data-stu-id="09cee-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="09cee-114">자세한 내용은 [Azure HDInsight에서 Apache Spark 클러스터 만들기](hdinsight-apache-spark-jupyter-spark-sql.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="09cee-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="09cee-115">Spark HDInsight에서 Jupyter 노트북 만들기</span><span class="sxs-lookup"><span data-stu-id="09cee-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="09cee-116">Hello에서 [Azure 포털](https://portal.azure.com/), 클러스터를 엽니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-116">From hello [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="09cee-117">참조 [목록 및 표시 클러스터](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) hello 지침에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for hello instructions.</span></span> <span data-ttu-id="09cee-118">hello 클러스터에서 새 포털 블레이드가 열립니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-118">hello cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="09cee-119">Hello에서 **빠른 링크** 섹션에서 클릭 **클러스터 대시보드** tooopen hello **클러스터 대시보드** 블레이드입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-119">From hello **Quick links** section, click **Cluster dashboards** tooopen hello **Cluster dashboards** blade.</span></span>  <span data-ttu-id="09cee-120">표시 되지 않으면 **빠른 링크**, 클릭 **개요** hello 블레이드에서 hello 왼쪽된 메뉴에서 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-120">If you don't see **Quick Links**, click **Overview** from hello left menu on hello blade.</span></span>

    <span data-ttu-id="09cee-121">![Spark의 Jupyter 노트북](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Spark의 Jupyter 노트북")</span><span class="sxs-lookup"><span data-stu-id="09cee-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="09cee-122">**Jupyter Notebook**을 클릭합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="09cee-123">메시지가 표시 되 면 hello 클러스터에 대 한 hello 관리자 자격 증명을 입력 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-123">If prompted, enter hello admin credentials for hello cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="09cee-124">또한 hello Jupyter 노트북 열어 hello 브라우저의 URL을 따라 하 여 Spark 클러스터에 도달할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-124">You may also reach hello Jupyter notebook on Spark cluster by opening hello following URL in your browser.</span></span> <span data-ttu-id="09cee-125">대체 **CLUSTERNAME** 클러스터의 hello 이름의:</span><span class="sxs-lookup"><span data-stu-id="09cee-125">Replace **CLUSTERNAME** with hello name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="09cee-126">클릭 **새로**, 다음 중 하나를 클릭 하 고 **Pyspark**, **PySpark3**, 또는 **Spark** toocreate 노트북 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** toocreate a notebook.</span></span> <span data-ttu-id="09cee-127">Scala 응용 프로그램에 대 한 hello Spark 커널, Python2 응용 프로그램에 대 한 PySpark 커널 및 PySpark3 커널 Python3에 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-127">Use hello Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="09cee-128">![Spark의 Jupyter 노트북에 대한 커널](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Spark의 Jupyter 노트북에 대한 커널")</span><span class="sxs-lookup"><span data-stu-id="09cee-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="09cee-129">선택한 hello 커널 노트북이 열립니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-129">A notebook opens with hello kernel you selected.</span></span>

## <a name="benefits-of-using-hello-kernels"></a><span data-ttu-id="09cee-130">Hello 커널에서 사용의 이점</span><span class="sxs-lookup"><span data-stu-id="09cee-130">Benefits of using hello kernels</span></span>

<span data-ttu-id="09cee-131">다음은 새 커널 hello를 사용 하 여 HDInsight Spark 클러스터에서 Jupyter 노트북으로의 몇 가지 이점입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-131">Here are a few benefits of using hello new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="09cee-132">**컨텍스트를 미리 설정합니다**.</span><span class="sxs-lookup"><span data-stu-id="09cee-132">**Preset contexts**.</span></span> <span data-ttu-id="09cee-133">와 **PySpark**, **PySpark3**, 또는 hello **Spark** 커널, 않아도 tooset hello Spark 또는 Hive 컨텍스트 명시적으로 응용 프로그램 사용을 시작 하기 전에.</span><span class="sxs-lookup"><span data-stu-id="09cee-133">With  **PySpark**, **PySpark3**, or hello **Spark** kernels, you do not need tooset hello Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="09cee-134">이러한 컨텍스트는 기본적으로 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-134">These are available by default.</span></span> <span data-ttu-id="09cee-135">이러한 컨텍스트는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-135">These contexts are:</span></span>
   
   * <span data-ttu-id="09cee-136">**sc** - Spark 컨텍스트용</span><span class="sxs-lookup"><span data-stu-id="09cee-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="09cee-137">**sqlContext** - Hive 컨텍스트용</span><span class="sxs-lookup"><span data-stu-id="09cee-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="09cee-138">따라서 tooset hello 컨텍스트에 따라 hello 같은 toorun 문이 필요가 없습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-138">So, you don't have toorun statements like hello following tooset hello contexts:</span></span>

        <span data-ttu-id="09cee-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="09cee-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="09cee-140">대신, 직접 사용할 수 있습니다 hello 사전 응용 프로그램에서 컨텍스트를 설정 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-140">Instead, you can directly use hello preset contexts in your application.</span></span>

- <span data-ttu-id="09cee-141">**매직 셀**입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-141">**Cell magics**.</span></span> <span data-ttu-id="09cee-142">hello PySpark 커널 제공 일부 미리 정의 된 "마법"으로 호출할 수 있는 특수 명령을 `%%` (예를 들어 `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="09cee-142">hello PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="09cee-143">여러 줄의 콘텐츠를 허용 하는 코드 셀에 첫 번째 단어 hello를 hello 매직 명령 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-143">hello magic command must be hello first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="09cee-144">hello 매직 단어 hello 셀에 첫 번째 단어 hello 이어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-144">hello magic word should be hello first word in hello cell.</span></span> <span data-ttu-id="09cee-145">Hello 매직, 짝수 메모 하기 전에 아무 것도 추가 하면 오류가 발생 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-145">Adding anything before hello magic, even comments, causes an error.</span></span>     <span data-ttu-id="09cee-146">매직에 대한 자세한 내용은 [여기](http://ipython.readthedocs.org/en/stable/interactive/magics.html)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="09cee-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="09cee-147">hello 다음 표에 hello 다른 마법 hello 커널을 통해 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-147">hello following table lists hello different magics available through hello kernels.</span></span>

   | <span data-ttu-id="09cee-148">매직</span><span class="sxs-lookup"><span data-stu-id="09cee-148">Magic</span></span> | <span data-ttu-id="09cee-149">예</span><span class="sxs-lookup"><span data-stu-id="09cee-149">Example</span></span> | <span data-ttu-id="09cee-150">설명</span><span class="sxs-lookup"><span data-stu-id="09cee-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="09cee-151">help</span><span class="sxs-lookup"><span data-stu-id="09cee-151">help</span></span> |`%%help` |<span data-ttu-id="09cee-152">예제 및 설명과 함께 모든 hello 사용 가능한 마법 목차를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-152">Generates a table of all hello available magics with example and description</span></span> |
   | <span data-ttu-id="09cee-153">info</span><span class="sxs-lookup"><span data-stu-id="09cee-153">info</span></span> |`%%info` |<span data-ttu-id="09cee-154">현재 리비 끝점 hello에 대 한 출력 세션 정보</span><span class="sxs-lookup"><span data-stu-id="09cee-154">Outputs session information for hello current Livy endpoint</span></span> |
   | <span data-ttu-id="09cee-155">구성</span><span class="sxs-lookup"><span data-stu-id="09cee-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="09cee-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="09cee-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="09cee-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="09cee-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="09cee-158">세션을 만들기 위한 hello 매개 변수를 구성 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-158">Configures hello parameters for creating a session.</span></span> <span data-ttu-id="09cee-159">force 플래그가 hello (-f)는 필수 세션을 이미 만든 경우를 통해 해당 hello 세션을 삭제 하 고 다시 만들어집니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-159">hello force flag (-f) is mandatory if a session has already been created, which ensures that hello session is dropped and recreated.</span></span> <span data-ttu-id="09cee-160">유효한 매개 변수 목록은 [Livy의 POST /sessions Request Body](https://github.com/cloudera/livy#request-body) 를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="09cee-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="09cee-161">매개 변수를 JSON 문자열로 전달 해야 하며 hello 예에서는 열에 표시 된 것 처럼 hello 매직 후 hello 다음 줄에서 이어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-161">Parameters must be passed in as a JSON string and must be on hello next line after hello magic, as shown in hello example column.</span></span> |
   | <span data-ttu-id="09cee-162">sql</span><span class="sxs-lookup"><span data-stu-id="09cee-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="09cee-163">SqlContext hello에 대 한 하이브 쿼리를 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-163">Executes a Hive query against hello sqlContext.</span></span> <span data-ttu-id="09cee-164">경우 hello `-o` 매개 변수를 전달, hello 쿼리의 hello 결과 hello에서 유지 되 % %로 로컬 Python 컨텍스트는 [팬더](http://pandas.pydata.org/) 데이터 프레임입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-164">If hello `-o` parameter is passed, hello result of hello query is persisted in hello %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="09cee-165">local</span><span class="sxs-lookup"><span data-stu-id="09cee-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="09cee-166">다음 줄의 모든 hello 코드가 로컬로 실행 됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-166">All hello code in subsequent lines is executed locally.</span></span> <span data-ttu-id="09cee-167">코드를 사용 하는 hello 커널에 관계 없이 올바른 Python2 코드 여야 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-167">Code must be valid Python2 code even irrespective of hello kernel you are using.</span></span> <span data-ttu-id="09cee-168">따라서 선택한 경우에 **PySpark3** 또는 **Spark** hello를 사용 하는 경우 hello 노트북을 만드는 동안 커널 `%%local` 셀에 매직, 해당 셀만 유효한 Python2 코드가 있어야...</span><span class="sxs-lookup"><span data-stu-id="09cee-168">So, even if you selected **PySpark3** or **Spark** kernels while creating hello notebook, if you use hello `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="09cee-169">로그</span><span class="sxs-lookup"><span data-stu-id="09cee-169">logs</span></span> |`%%logs` |<span data-ttu-id="09cee-170">출력은 hello 현재 리비 세션에 대 한 로그를 hello 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-170">Outputs hello logs for hello current Livy session.</span></span> |
   | <span data-ttu-id="09cee-171">delete</span><span class="sxs-lookup"><span data-stu-id="09cee-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="09cee-172">Hello 현재 리비 끝점의 특정 세션을 삭제합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-172">Deletes a specific session of hello current Livy endpoint.</span></span> <span data-ttu-id="09cee-173">참고 hello 커널 자체에 대 한 시작 된 hello 세션을 삭제할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-173">Note that you cannot delete hello session that is initiated for hello kernel itself.</span></span> |
   | <span data-ttu-id="09cee-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="09cee-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="09cee-175">Hello 현재 리비 끝점의 경우이 전자 필기장의이 세션을 포함 하 여 모든 hello 세션을 삭제 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-175">Deletes all hello sessions for hello current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="09cee-176">hello force 플래그-f는 필수입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-176">hello force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="09cee-177">또한 toohello 마법 추가 hello PySpark 커널에서 hello를 사용할 수도 있습니다 [기본 제공 IPython 마법](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics)를 포함 하 여 `%%sh`합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-177">In addition toohello magics added by hello PySpark kernel, you can also use hello [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="09cee-178">Hello를 사용할 수 있습니다 `%%sh` 매직 toorun 스크립트와의 hello 클러스터 헤드 노드에 코드 블록입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-178">You can use hello `%%sh` magic toorun scripts and block of code on hello cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="09cee-179">**자동 시각화**.</span><span class="sxs-lookup"><span data-stu-id="09cee-179">**Auto visualization**.</span></span> <span data-ttu-id="09cee-180">hello **Pyspark** 커널 hello 출력의 Hive 및 SQL 쿼리를 자동으로 시각화 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-180">hello **Pyspark** kernel automatically visualizes hello output of Hive and SQL queries.</span></span> <span data-ttu-id="09cee-181">테이블, 원형, 선, 영역, 막대를 포함하여 다양한 시각화 형식 중에서 선택할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-hello-sql-magic"></a><span data-ttu-id="09cee-182">Hello로 지원 되는 매개 변수 %%sql 매직</span><span class="sxs-lookup"><span data-stu-id="09cee-182">Parameters supported with hello %%sql magic</span></span>
<span data-ttu-id="09cee-183">hello `%%sql` 매직 toocontrol hello 종류의 쿼리를 실행할 때 나타나는 출력을 사용할 수 있는 다른 매개 변수를 지원 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-183">hello `%%sql` magic supports different parameters that you can use toocontrol hello kind of output that you receive when you run queries.</span></span> <span data-ttu-id="09cee-184">다음 표에서 hello hello 출력을 나열 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-184">hello following table lists hello output.</span></span>

| <span data-ttu-id="09cee-185">매개 변수</span><span class="sxs-lookup"><span data-stu-id="09cee-185">Parameter</span></span> | <span data-ttu-id="09cee-186">예</span><span class="sxs-lookup"><span data-stu-id="09cee-186">Example</span></span> | <span data-ttu-id="09cee-187">설명</span><span class="sxs-lookup"><span data-stu-id="09cee-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="09cee-188">-o</span><span class="sxs-lookup"><span data-stu-id="09cee-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="09cee-189">이 매개 변수 toopersist hello hello 쿼리의 결과 사용 하 여 hello에 % % 로컬 Python 컨텍스트로는 [팬더](http://pandas.pydata.org/) 데이터 프레임입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-189">Use this parameter toopersist hello result of hello query, in hello %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="09cee-190">hello hello 데이터 프레임 변수 이름이 hello 변수 이름을 지정 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-190">hello name of hello dataframe variable is hello variable name you specify.</span></span> |
| <span data-ttu-id="09cee-191">-q</span><span class="sxs-lookup"><span data-stu-id="09cee-191">-q</span></span> |`-q` |<span data-ttu-id="09cee-192">Hello 셀에 대 한이 tooturn 시각화 off를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-192">Use this tooturn off visualizations for hello cell.</span></span> <span data-ttu-id="09cee-193">Tooauto 하지 않으려면-셀의 hello 콘텐츠를 시각화 하 고 원하는 toocapture을 데이터 프레임으로 사용 하 여 `-q -o <VARIABLE>`합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-193">If you don't want tooauto-visualize hello content of a cell and just want toocapture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="09cee-194">Hello 결과 캡처하지 않고 tooturn 시각화 해제 하려는 경우 (같은 SQL 쿼리 실행에 대 한 예를 들어는 `CREATE TABLE` 문)를 사용 하 여 `-q` 지정 하지 않고는 `-o` 인수입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-194">If you want tooturn off visualizations without capturing hello results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="09cee-195">-m</span><span class="sxs-lookup"><span data-stu-id="09cee-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="09cee-196">여기서 **METHOD**는 **take** 또는 **sample**(기본값: **take**)입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="09cee-197">Hello 메서드가 **걸릴**, hello 커널 MAXROWS (이 표의 뒷부분에서 설명)로 지정 된 hello 결과 데이터 집합의 hello 위에서 요소를 선택 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-197">If hello method is **take**, hello kernel picks elements from hello top of hello result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="09cee-198">Hello 메서드가 **샘플**, hello 커널 너무에 따라 hello 데이터 집합의 요소를 무작위로 샘플링`-r` 매개 변수를이 테이블에 다음에 설명 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-198">If hello method is **sample**, hello kernel randomly samples elements of hello data set according too`-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="09cee-199">-r</span><span class="sxs-lookup"><span data-stu-id="09cee-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="09cee-200">여기서 **FRACTION**은 0.0과 1.0 사이의 부동 소수점 숫자입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="09cee-201">Hello SQL 쿼리를 위한 샘플 메서드가 hello 이면 `sample`, hello 커널 hello 요소 자동으로 설정 하는 hello 결과의 지정 된 비율 hello를 무작위로 샘플링 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-201">If hello sample method for hello SQL query is `sample`, then hello kernel randomly samples hello specified fraction of hello elements of hello result set for you.</span></span> <span data-ttu-id="09cee-202">예를 들어 hello 인수를 갖는 SQL 쿼리를 실행 하는 경우 `-m sample -r 0.01`, hello 결과 행의 1% 임의로 샘플링 됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-202">For example, if you run a SQL query with hello arguments `-m sample -r 0.01`, then 1% of hello result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="09cee-203">**MAXROWS** 는 정수 값입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="09cee-204">hello 커널 hello 출력 행 수를 너무 제한**MAXROWS**합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-204">hello kernel limits hello number of output rows too**MAXROWS**.</span></span> <span data-ttu-id="09cee-205">경우 **MAXROWS** 은 음수와 같은 **-1**, hello hello 결과 집합의 행 수가 제한 되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-205">If **MAXROWS** is a negative number such as **-1**, then hello number of rows in hello result set is not limited.</span></span> |

<span data-ttu-id="09cee-206">**예제:**</span><span class="sxs-lookup"><span data-stu-id="09cee-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="09cee-207">위의 hello 문은 다음 hello지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-207">hello statement above does hello following:</span></span>

* <span data-ttu-id="09cee-208">**hivesampletable**에서 모든 레코드를 선택합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="09cee-209">-q를 사용하기 때문에 자동 시각화를 해제합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="09cee-210">사용 하므로 `-m sample -r 0.1 -n 500` hello hivesampletable의 hello 행 중 10%를 무작위로 샘플링 및 제한을 hello hello 결과 집합 too500 행의 크기입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of hello rows in hello hivesampletable and limits hello size of hello result set too500 rows.</span></span>
* <span data-ttu-id="09cee-211">마지막으로 사용 하기 때문에 `-o query2` 라는 데이터 프레임이에 한 hello 출력을 저장할지 **query2**합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-211">Finally, because we used `-o query2` it also saves hello output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-hello-new-kernels"></a><span data-ttu-id="09cee-212">새 커널 hello를 사용 하는 동안 고려 사항</span><span class="sxs-lookup"><span data-stu-id="09cee-212">Considerations while using hello new kernels</span></span>

<span data-ttu-id="09cee-213">를 사용 하면 어떤 커널 hello 클러스터 리소스를 소비 hello 전자 필기장 실행을 종료 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-213">Whichever kernel you use, leaving hello notebooks running consumes hello cluster resources.</span></span>  <span data-ttu-id="09cee-214">이러한 커널 hello 컨텍스트는 미리 설정 때문에 단순히 hello 전자 필기장 종료 hello 컨텍스트를 중단 하지 않습니다 하 고 따라서 hello 클러스터 리소스 계속 toobe 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-214">With these kernels, because hello contexts are preset, simply exiting hello notebooks does not kill hello context and hence hello cluster resources continue toobe in use.</span></span> <span data-ttu-id="09cee-215">일반적으로는 toouse hello **닫고 중단** hello 노트북에서 옵션 **파일** hello 컨텍스트 해당 프로세스를 중지 하는 hello 노트북을 사용 하 여 작업이 완료 되 고 다음 종료 되거나 hello 전자 필기장 메뉴.</span><span class="sxs-lookup"><span data-stu-id="09cee-215">A good practice is toouse hello **Close and Halt** option from hello notebook's **File** menu when you are finished using hello notebook, which kills hello context and then exits hello notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="09cee-216">몇 가지 예제 보기</span><span class="sxs-lookup"><span data-stu-id="09cee-216">Show me some examples</span></span>

<span data-ttu-id="09cee-217">Jupyter 노트북을 열 때 hello 루트 수준에서 사용할 수 있는 두 개의 폴더를 참조 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-217">When you open a Jupyter notebook, you see two folders available at hello root level.</span></span>

* <span data-ttu-id="09cee-218">hello **PySpark** 폴더에 샘플 전자 필기장을 사용 하 여 hello 새 **Python** 커널 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-218">hello **PySpark** folder has sample notebooks that use hello new **Python** kernel.</span></span>
* <span data-ttu-id="09cee-219">hello **Scala** 폴더에 샘플 전자 필기장을 사용 하 여 hello 새 **Spark** 커널 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-219">hello **Scala** folder has sample notebooks that use hello new **Spark** kernel.</span></span>

<span data-ttu-id="09cee-220">Hello를 열 수 **00-[읽기 나 처음] Spark 매직 커널 기능** hello에서 노트북 **PySpark** 또는 **Spark** 사용할 수 있는 다른 마법 hello에 대 한 폴더 toolearn 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-220">You can open hello **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from hello **PySpark** or **Spark** folder toolearn about hello different magics available.</span></span> <span data-ttu-id="09cee-221">사용할 수 있습니다 어떻게 hello 두 폴더 toolearn에서 사용할 수 있는 다른 샘플 전자 필기장 hello tooachieve 다양 한 시나리오를 Jupyter 노트북을 사용 하 여 HDInsight Spark 클러스터로 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-221">You can also use hello other sample notebooks available under hello two folders toolearn how tooachieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-hello-notebooks-stored"></a><span data-ttu-id="09cee-222">Hello 전자 필기장의 저장 위치</span><span class="sxs-lookup"><span data-stu-id="09cee-222">Where are hello notebooks stored?</span></span>

<span data-ttu-id="09cee-223">Jupyter 노트북 hello hello 클러스터와 연결 된 toohello 저장소 계정을 저장 **/HdiNotebooks** 폴더입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-223">Jupyter notebooks are saved toohello storage account associated with hello cluster under hello **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="09cee-224">노트북, 텍스트 파일 및 Jupyter 내에서 만드는 폴더는 hello 저장소 계정에서 액세스할 수입니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from hello storage account.</span></span>  <span data-ttu-id="09cee-225">예를 들어, Jupyter toocreate 폴더를 사용 하는 경우 **myfolder** 와 노트북 **myfolder/mynotebook.ipynb**에서 해당 노트북에 액세스할 수 있습니다 `/HdiNotebooks/myfolder/mynotebook.ipynb` hello 저장소 계정 내에서.</span><span class="sxs-lookup"><span data-stu-id="09cee-225">For example, if you use Jupyter toocreate a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within hello storage account.</span></span>  <span data-ttu-id="09cee-226">hello 역방향는 true 이면 즉, tooyour 저장소 계정에서 직접 노트북을 업로드 하는 경우 `/HdiNotebooks/mynotebook1.ipynb`, hello 노트북도 Jupyter에서 표시 됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-226">hello reverse is also true, that is, if you upload a notebook directly tooyour storage account at `/HdiNotebooks/mynotebook1.ipynb`, hello notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="09cee-227">Hello 클러스터를 삭제 한 후에 전자 필기장 hello 저장소 계정에 남아 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-227">Notebooks remain in hello storage account even after hello cluster is deleted.</span></span>

<span data-ttu-id="09cee-228">hello 방식으로 전자 필기장 toohello 저장소 계정에 저장 되는 HDFS 호환 됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-228">hello way notebooks are saved toohello storage account is compatible with HDFS.</span></span> <span data-ttu-id="09cee-229">따라서 경우 hello 다음 코드 조각에에서 나와 있는 것 처럼 관리 명령을 파일 SSH hello 클러스터를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-229">So, if you SSH into hello cluster you can use file management commands as shown in hello following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at hello root directory – everything in this directory is visible tooJupyter from hello home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download hello contents of hello HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb toohello root folder so it’s visible from Jupyter


<span data-ttu-id="09cee-230">Hello 전자 필기장 hello 헤드 노드에도 저장 됩니다 hello 클러스터에 대 한 hello 저장소 계정에 액세스 하는 문제가 있는 경우 `/var/lib/jupyter`합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-230">In case there are issues accessing hello storage account for hello cluster, hello notebooks are also saved on hello headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="09cee-231">지원되는 브라우저</span><span class="sxs-lookup"><span data-stu-id="09cee-231">Supported browser</span></span>

<span data-ttu-id="09cee-232">Spark HDInsight 클러스터의 Jupyter 노트북은 Google Chrome에서만 지원됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="09cee-233">사용자 의견</span><span class="sxs-lookup"><span data-stu-id="09cee-233">Feedback</span></span>
<span data-ttu-id="09cee-234">새 커널 hello 진화 단계에 있으며 시간이 지남에 따라 완성 됩니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-234">hello new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="09cee-235">이는 API가 이러한 커널의 성숙에 따라 변경될 수 있음을 의미할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="09cee-236">이러한 새로운 커널을 사용하는 동안 가진 의견을 보내주시면 감사하겠습니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="09cee-237">이러한 커널의 최종 릴리스 hello 모양 지정에서 유용 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-237">This is useful in shaping hello final release of these kernels.</span></span> <span data-ttu-id="09cee-238">Hello에서 주석을/피드백을 그대로 둘 수 **주석** 이 문서의 hello 아래쪽 섹션.</span><span class="sxs-lookup"><span data-stu-id="09cee-238">You can leave your comments/feedback under hello **Comments** section at hello bottom of this article.</span></span>

## <span data-ttu-id="09cee-239"><a name="seealso"></a>참고 항목</span><span class="sxs-lookup"><span data-stu-id="09cee-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="09cee-240">개요: Azure HDInsight에서 Apache Spark</span><span class="sxs-lookup"><span data-stu-id="09cee-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="09cee-241">시나리오</span><span class="sxs-lookup"><span data-stu-id="09cee-241">Scenarios</span></span>
* [<span data-ttu-id="09cee-242">BI와 Spark: BI 도구와 함께 HDInsight에서 Spark를 사용하여 대화형 데이터 분석 수행</span><span class="sxs-lookup"><span data-stu-id="09cee-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="09cee-243">기계 학습과 Spark: HVAC 데이터를 사용하여 건물 온도를 분석하는 데 HDInsight의 Spark 사용</span><span class="sxs-lookup"><span data-stu-id="09cee-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="09cee-244">Spark와 기계 학습: HDInsight toopredict 음식 검사 결과에 사용 하 여 Spark</span><span class="sxs-lookup"><span data-stu-id="09cee-244">Spark with Machine Learning: Use Spark in HDInsight toopredict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="09cee-245">Spark 스트리밍: HDInsight에서 Spark를 사용하여 실시간 스트리밍 응용 프로그램 빌드</span><span class="sxs-lookup"><span data-stu-id="09cee-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="09cee-246">HDInsight의 Spark를 사용하여 웹 사이트 로그 분석</span><span class="sxs-lookup"><span data-stu-id="09cee-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="09cee-247">응용 프로그램 만들기 및 실행</span><span class="sxs-lookup"><span data-stu-id="09cee-247">Create and run applications</span></span>
* [<span data-ttu-id="09cee-248">Scala를 사용하여 독립 실행형 응용 프로그램 만들기</span><span class="sxs-lookup"><span data-stu-id="09cee-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="09cee-249">Livy를 사용하여 Spark 클러스터에서 원격으로 작업 실행</span><span class="sxs-lookup"><span data-stu-id="09cee-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="09cee-250">도구 및 확장</span><span class="sxs-lookup"><span data-stu-id="09cee-250">Tools and extensions</span></span>
* [<span data-ttu-id="09cee-251">IntelliJ 아이디어 toocreate에 대 한 HDInsight 도구 플러그 인을 사용 하 고 스파크 Scala 응용 프로그램 제출</span><span class="sxs-lookup"><span data-stu-id="09cee-251">Use HDInsight Tools Plugin for IntelliJ IDEA toocreate and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="09cee-252">IntelliJ 아이디어 toodebug Spark 응용 프로그램에 대 한 HDInsight 도구 플러그 인을 원격으로 사용</span><span class="sxs-lookup"><span data-stu-id="09cee-252">Use HDInsight Tools Plugin for IntelliJ IDEA toodebug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="09cee-253">HDInsight에서 Spark 클러스터와 함께 Zeppelin Notebook 사용</span><span class="sxs-lookup"><span data-stu-id="09cee-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="09cee-254">Jupyter 노트북에서 외부 패키지 사용</span><span class="sxs-lookup"><span data-stu-id="09cee-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="09cee-255">Jupyter 사용자 컴퓨터에 설치 하 고 tooan HDInsight Spark 클러스터를 연결 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-255">Install Jupyter on your computer and connect tooan HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="09cee-256">리소스 관리</span><span class="sxs-lookup"><span data-stu-id="09cee-256">Manage resources</span></span>
* [<span data-ttu-id="09cee-257">Azure HDInsight의 Apache Spark 클러스터 hello에 대 한 리소스를 관리 합니다.</span><span class="sxs-lookup"><span data-stu-id="09cee-257">Manage resources for hello Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="09cee-258">HDInsight의 Apache Spark 클러스터에서 실행되는 작업 추적 및 디버그</span><span class="sxs-lookup"><span data-stu-id="09cee-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
