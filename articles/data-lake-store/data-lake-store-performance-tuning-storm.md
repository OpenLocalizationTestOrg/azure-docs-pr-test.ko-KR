---
title: "aaaAzure 데이터 레이크 저장소 스톰 성능 튜닝 방법 지침 | Microsoft Docs"
description: "Azure Data Lake Store Storm 성능 조정 지침"
services: data-lake-store
documentationcenter: 
author: stewu
manager: amitkul
editor: stewu
ms.assetid: ebde7b9f-2e51-4d43-b7ab-566417221335
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 12/19/2016
ms.author: stewu
ms.openlocfilehash: 5412fd46cf2373f5877030913df4fe1fc6f5473a
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="performance-tuning-guidance-for-storm-on-hdinsight-and-azure-data-lake-store"></a><span data-ttu-id="a728c-103">HDInsight의 Storm 및 Azure Data Lake Store에 대한 성능 조정 지침</span><span class="sxs-lookup"><span data-stu-id="a728c-103">Performance tuning guidance for Storm on HDInsight and Azure Data Lake Store</span></span>

<span data-ttu-id="a728c-104">Azure 스톰 토폴로지의 hello 성능을 조정할 때 고려해 야 하는 hello 요소를 이해 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-104">Understand hello factors that should be considered when you tune hello performance of an Azure Storm topology.</span></span> <span data-ttu-id="a728c-105">예를 들어 hello spouts 및 hello 볼트 (hello 작업 인지 I/O 또는 메모리를 많이)에서 수행 하는 hello 작업의 중요 한 toounderstand hello 특성 것 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-105">For example, it's important toounderstand hello characteristics of hello work done by hello spouts and hello bolts (whether hello work is I/O or memory intensive).</span></span> <span data-ttu-id="a728c-106">이 문서에서는 다양한 성능 조정 지침, 일반적인 문제 해결 등을 다룹니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-106">This article covers a range of performance tuning guidelines, including troubleshooting common issues.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="a728c-107">필수 조건</span><span class="sxs-lookup"><span data-stu-id="a728c-107">Prerequisites</span></span>

* <span data-ttu-id="a728c-108">**Azure 구독**.</span><span class="sxs-lookup"><span data-stu-id="a728c-108">**An Azure subscription**.</span></span> <span data-ttu-id="a728c-109">[Azure 무료 평가판](https://azure.microsoft.com/pricing/free-trial/)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="a728c-109">See [Get Azure free trial](https://azure.microsoft.com/pricing/free-trial/).</span></span>
* <span data-ttu-id="a728c-110">**Azure 데이터 레이크 저장소 계정**.</span><span class="sxs-lookup"><span data-stu-id="a728c-110">**An Azure Data Lake Store account**.</span></span> <span data-ttu-id="a728c-111">방법에 대 한 지침은 toocreate 하나, 참조 [Azure 데이터 레이크 저장소 시작](data-lake-store-get-started-portal.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-111">For instructions on how toocreate one, see [Get started with Azure Data Lake Store](data-lake-store-get-started-portal.md).</span></span>
* <span data-ttu-id="a728c-112">**Azure HDInsight 클러스터** 액세스 tooa 데이터 레이크 저장소 계정 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-112">**An Azure HDInsight cluster** with access tooa Data Lake Store account.</span></span> <span data-ttu-id="a728c-113">[Data Lake Store가 있는 HDInsight 클러스터 만들기](data-lake-store-hdinsight-hadoop-use-portal.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="a728c-113">See [Create an HDInsight cluster with Data Lake Store](data-lake-store-hdinsight-hadoop-use-portal.md).</span></span> <span data-ttu-id="a728c-114">Hello 클러스터에 대 한 원격 데스크톱을 사용 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-114">Make sure you enable Remote Desktop for hello cluster.</span></span>
* <span data-ttu-id="a728c-115">**Data Lake Store에서 실행 중인 Storm 클러스터**</span><span class="sxs-lookup"><span data-stu-id="a728c-115">**Running a Storm cluster on Data Lake Store**.</span></span> <span data-ttu-id="a728c-116">자세한 내용은 [HDInsight의 Storm](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="a728c-116">For more information, see [Storm on HDInsight](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview).</span></span>
* <span data-ttu-id="a728c-117">**Data Lake Store 성능 조정 지침**</span><span class="sxs-lookup"><span data-stu-id="a728c-117">**Performance tuning guidelines on Data Lake Store**.</span></span>  <span data-ttu-id="a728c-118">일반적인 성능 개념은 [Data Lake Store 성능 조정 지침](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="a728c-118">For general performance concepts, see [Data Lake Store Performance Tuning Guidance](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance).</span></span>  

## <a name="tune-hello-parallelism-of-hello-topology"></a><span data-ttu-id="a728c-119">Hello 토폴로지의 hello 병렬 처리를 조정 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-119">Tune hello parallelism of hello topology</span></span>

<span data-ttu-id="a728c-120">데이터 레이크 저장소에서 hello I/O tooand의 증가 hello 동시성 수 tooimprove 성능 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-120">You might be able tooimprove performance by increasing hello concurrency of hello I/O tooand from Data Lake Store.</span></span> <span data-ttu-id="a728c-121">Storm 토폴로지는 hello 병렬 처리를 결정 하는 구성 집합에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-121">A Storm topology has a set of configurations that determine hello parallelism:</span></span>
* <span data-ttu-id="a728c-122">작업자 프로세스 (hello 작업자는 전체에 고르게 분산 hello Vm)의 수입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-122">Number of worker processes (hello workers are evenly distributed across hello VMs).</span></span>
* <span data-ttu-id="a728c-123">Spout 실행기 인스턴스 수</span><span class="sxs-lookup"><span data-stu-id="a728c-123">Number of spout executor instances.</span></span>
* <span data-ttu-id="a728c-124">Bolt 실행기 인스턴스 수</span><span class="sxs-lookup"><span data-stu-id="a728c-124">Number of bolt executor instances.</span></span>
* <span data-ttu-id="a728c-125">Spout 태스크 수</span><span class="sxs-lookup"><span data-stu-id="a728c-125">Number of spout tasks.</span></span>
* <span data-ttu-id="a728c-126">Bolt 태스크 수</span><span class="sxs-lookup"><span data-stu-id="a728c-126">Number of bolt tasks.</span></span>

<span data-ttu-id="a728c-127">예를 들어 vm 4 개 및 작업자 프로세스가 4, 32 배출구 executor 및 32와 배출구 작업 및 256 볼트 executor 512 볼트 작업 클러스터에서 hello 다음 사항을 고려 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-127">For example, on a cluster with 4 VMs and 4 worker processes, 32 spout executors and 32 spout tasks, and 256 bolt executors and 512 bolt tasks, consider hello following:</span></span>

<span data-ttu-id="a728c-128">작업자 노드인 각 감독자에는 단일 작업자 JVM(Java 가상 컴퓨터) 프로세스가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-128">Each supervisor, which is a worker node, has a single worker Java virtual machine (JVM) process.</span></span> <span data-ttu-id="a728c-129">이 JVM 프로세스는 4개의 Spout 스레드 및 64개의 Bolt 스레드를 관리합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-129">This JVM process manages 4 spout threads and 64 bolt threads.</span></span> <span data-ttu-id="a728c-130">각 스레드 내에서 태스크를 순차적으로 실행합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-130">Within each thread, tasks are run sequentially.</span></span> <span data-ttu-id="a728c-131">Hello 구성 앞로 각 배출구 스레드는 1 작업 하며 각 볼트 스레드가 2 작업입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-131">With hello preceding configuration, each spout thread has 1 task, and each bolt thread has 2 tasks.</span></span>

<span data-ttu-id="a728c-132">Storm에서 여기 hello, 관련 된 다양 한 구성 요소 이며 hello 수준의 있는 병렬 처리에 미치는 영향:</span><span class="sxs-lookup"><span data-stu-id="a728c-132">In Storm, here are hello various components involved, and how they affect hello level of parallelism you have:</span></span>
* <span data-ttu-id="a728c-133">hello 헤드 노드 (스톰에 호출된 Nimbus)가 사용 되는 toosubmit 하 고 작업을 관리 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-133">hello head node (called Nimbus in Storm) is used toosubmit and manage jobs.</span></span> <span data-ttu-id="a728c-134">이러한 노드는 hello 병렬 처리 수준에 영향을 미치지 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-134">These nodes have no impact on hello degree of parallelism.</span></span>
* <span data-ttu-id="a728c-135">hello 감독자 노드입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-135">hello supervisor nodes.</span></span> <span data-ttu-id="a728c-136">HDInsight에 tooa 작업자 노드 Azure VM이 해당합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-136">In HDInsight, this corresponds tooa worker node Azure VM.</span></span>
* <span data-ttu-id="a728c-137">hello 작업자 태스크는 hello Vm에서에서 실행 중인 스톰 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-137">hello worker tasks are Storm processes running in hello VMs.</span></span> <span data-ttu-id="a728c-138">각 작업자 태스크 tooa JVM 인스턴스에 해당합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-138">Each worker task corresponds tooa JVM instance.</span></span> <span data-ttu-id="a728c-139">Storm hello 번호 분산 작업자 프로세스의 지정할수록 toohello 작업자 노드 최대한 일정 하 게 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-139">Storm distributes hello number of worker processes you specify toohello worker nodes as evenly as possible.</span></span>
* <span data-ttu-id="a728c-140">Spout 및 Bolt 실행기 인스턴스</span><span class="sxs-lookup"><span data-stu-id="a728c-140">Spout and bolt executor instances.</span></span> <span data-ttu-id="a728c-141">Executor 인스턴스마다 hello 작업자 (Jvm) 내에서 실행 되는 tooa 스레드를 해당 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-141">Each executor instance corresponds tooa thread running within hello workers (JVMs).</span></span>
* <span data-ttu-id="a728c-142">Storm 태스크</span><span class="sxs-lookup"><span data-stu-id="a728c-142">Storm tasks.</span></span> <span data-ttu-id="a728c-143">이러한 각 스레드가 실행되는 논리적 태스크입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-143">These are logical tasks that each of these threads run.</span></span> <span data-ttu-id="a728c-144">이 변경 되지 hello 수준의 병렬 처리 수준, 않으므로 여부 실행자 당 여러 작업을 해야 할 경우 평가 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-144">This does not change hello level of parallelism, so you should evaluate if you need multiple tasks per executor or not.</span></span>

### <a name="get-hello-best-performance-from-data-lake-store"></a><span data-ttu-id="a728c-145">데이터 레이크 저장소에서 hello 최상의 성능을 얻으려면</span><span class="sxs-lookup"><span data-stu-id="a728c-145">Get hello best performance from Data Lake Store</span></span>

<span data-ttu-id="a728c-146">데이터 레이크 저장소를 사용할 때는 다음 hello 수행 하는 경우 hello 최적의 성능을 얻습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-146">When working with Data Lake Store, you get hello best performance if you do hello following:</span></span>
* <span data-ttu-id="a728c-147">작은 크기의 추가 항목을 보다 큰 크기로 병합합니다(이상적으로 4MB).</span><span class="sxs-lookup"><span data-stu-id="a728c-147">Coalesce your small appends into larger sizes (ideally 4 MB).</span></span>
* <span data-ttu-id="a728c-148">가능한 동시 요청을 많이 수행.</span><span class="sxs-lookup"><span data-stu-id="a728c-148">Do as many concurrent requests as you can.</span></span> <span data-ttu-id="a728c-149">각 볼트 스레드의 차단 읽기 작업을 하기 때문에 원하는 hello 범위의 8-12 스레드 2에서 toohave 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-149">Because each bolt thread is doing blocking reads, you want toohave somewhere in hello range of 8-12 threads per core.</span></span> <span data-ttu-id="a728c-150">그러면 hello NIC 및 hello CPU 사용률도 유지 됩니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-150">This keeps hello NIC and hello CPU well utilized.</span></span> <span data-ttu-id="a728c-151">VM 크기가 크면 더 많은 동시 요청이 가능합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-151">A larger VM enables more concurrent requests.</span></span>  

### <a name="example-topology"></a><span data-ttu-id="a728c-152">예제 토폴로지</span><span class="sxs-lookup"><span data-stu-id="a728c-152">Example topology</span></span>

<span data-ttu-id="a728c-153">D13v2 Azure VM과 함께 8개의 작업자 노드 클러스터가 있다고 가정해 보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-153">Let’s assume you have an 8 worker node cluster with a D13v2 Azure VM.</span></span> <span data-ttu-id="a728c-154">이 VM에는 코어 8 개, 따라서 간에 8 작업자 노드를 hello, 64 코어, 총 있는 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-154">This VM has 8 cores, so among hello 8 worker nodes, you have 64 total cores.</span></span>

<span data-ttu-id="a728c-155">코어당 8개의 Bolt 스레드를 수행한다고 가정해 보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-155">Let’s say we do 8 bolt threads per core.</span></span> <span data-ttu-id="a728c-156">64개의 코어가 제공되므로 총 512개의 Bolt 실행기 인스턴스(즉, 스레드)가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-156">Given 64 cores, that means we want 512 total bolt executor instances (that is, threads).</span></span> <span data-ttu-id="a728c-157">이 경우 VM, 당 하나의 JVM을 시작 하 고 주로 hello JVM tooachieve 동시성 내 hello 스레드 동시성을 사용 하 여 가정해 봅니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-157">In this case, let’s say we start with one JVM per VM, and mainly use hello thread concurrency within hello JVM tooachieve concurrency.</span></span> <span data-ttu-id="a728c-158">즉, 8개의 작업자 태스크(Azure VM당 하나)와 512개의 Bolt 실행기가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-158">That means we need 8 worker tasks (one per Azure VM), and 512 bolt executors.</span></span> <span data-ttu-id="a728c-159">구성이 이와 스톰 시도 1 각 작업자 노드 수 있도록 작업자 노드 (감독자 노드), 간에 균등 하 게 toodistribute hello 작업자 JVM 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-159">Given this configuration, Storm tries toodistribute hello workers evenly across worker nodes (also known as supervisor nodes), giving each worker node 1 JVM.</span></span> <span data-ttu-id="a728c-160">이제 hello 감독자 내 스톰 감독자 간에 균등 하 게 toodistribute hello executor 오류 값을 제공 (즉, JVM) 각 감독자 8 스레드 각각입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-160">Now within hello supervisors, Storm tries toodistribute hello executors evenly between supervisors, giving each supervisor (that is, JVM) 8 threads each.</span></span>

## <a name="tune-additional-parameters"></a><span data-ttu-id="a728c-161">추가 매개 변수 조정</span><span class="sxs-lookup"><span data-stu-id="a728c-161">Tune additional parameters</span></span>
<span data-ttu-id="a728c-162">Hello 기본 토폴로지를 사용 하는 다음 사용할지 tootweak hello 매개 변수 중 하나를 고려할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-162">After you have hello basic topology, you can consider whether you want tootweak any of hello parameters:</span></span>
* <span data-ttu-id="a728c-163">**작업자 노드당 JVM 수**</span><span class="sxs-lookup"><span data-stu-id="a728c-163">**Number of JVMs per worker node.**</span></span> <span data-ttu-id="a728c-164">메모리 내에 호스트하는 큰 데이터 구조(예: 조회 테이블)가 있는 경우 각 JVM에는 별도 복사본이 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-164">If you have a large data structure (for example, a lookup table) that you host in memory, each JVM requires a separate copy.</span></span> <span data-ttu-id="a728c-165">또는 더 적은 Jvm을 사용 하는 경우 여러 스레드 간에 hello 데이터 구조를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-165">Alternatively, you can use hello data structure across many threads if you have fewer JVMs.</span></span> <span data-ttu-id="a728c-166">Hello 볼트의 I/O에 대 한 hello 수가 Jvm 미치지 않으며 차이 hello 해당 Jvm 전체에 걸쳐 추가 하는 스레드 수를 많이 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-166">For hello bolt’s I/O, hello number of JVMs does not make as much of a difference as hello number of threads added across those JVMs.</span></span> <span data-ttu-id="a728c-167">하나는 것이 좋습니다 toohave 것은 간단 작업자 당 JVM 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-167">For simplicity, it's a good idea toohave one JVM per worker.</span></span> <span data-ttu-id="a728c-168">프로그램 볼트 수행 하는 또는 처리 할 응용 프로그램에 따라 필요한 있지만이 숫자 toochange를 할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-168">Depending on what your bolt is doing or what application processing you require, though, you may need toochange this number.</span></span>
* <span data-ttu-id="a728c-169">**Spout 실행자 수**</span><span class="sxs-lookup"><span data-stu-id="a728c-169">**Number of spout executors.**</span></span> <span data-ttu-id="a728c-170">Hello 앞의 예제를 사용 하므로 볼트 tooData Lake 저장소를 작성 하기 위한 hello 수가 spouts 직접 관련 toohello 볼트 성능 않습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-170">Because hello preceding example uses bolts for writing tooData Lake Store, hello number of spouts is not directly relevant toohello bolt performance.</span></span> <span data-ttu-id="a728c-171">그러나 hello 양을 처리 또는 I/O에서에서 발생 하는 hello 배출구는 것이 좋습니다에 따라 최상의 성능을 위해 tootune hello spouts 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-171">However, depending on hello amount of processing or I/O happening in hello spout, it's a good idea tootune hello spouts for best performance.</span></span> <span data-ttu-id="a728c-172">충분 한 spouts toobe 수 tookeep hello 볼트 사용 중 가졌는지 확인 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-172">Ensure that you have enough spouts toobe able tookeep hello bolts busy.</span></span> <span data-ttu-id="a728c-173">hello spouts의 hello 출력 비율 hello 볼트의 hello 처리량을 일치 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-173">hello output rates of hello spouts should match hello throughput of hello bolts.</span></span> <span data-ttu-id="a728c-174">실제 구성이 hello hello 배출구에 따라 달라 집니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-174">hello actual configuration depends on hello spout.</span></span>
* <span data-ttu-id="a728c-175">**태스크 수**</span><span class="sxs-lookup"><span data-stu-id="a728c-175">**Number of tasks.**</span></span> <span data-ttu-id="a728c-176">각 Bolt는 단일 스레드로 실행됩니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-176">Each bolt runs as a single thread.</span></span> <span data-ttu-id="a728c-177">Bolt당 추가 태스크는 어떠한 동시성도 제공하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-177">Additional tasks per bolt don't provide any additional concurrency.</span></span> <span data-ttu-id="a728c-178">hello 유일한 시간 혜택는 hello 튜플 승인의 프로세스 볼트 실행 시간의 상당 경우 것입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-178">hello only time they are of benefit is if your process of acknowledging hello tuple takes a large proportion of your bolt execution time.</span></span> <span data-ttu-id="a728c-179">Hello 볼트에서 승인을 보내기 전에 집합에 더 큰 추가 하는 것이 좋습니다 toogroup 이며</span><span class="sxs-lookup"><span data-stu-id="a728c-179">It's a good idea toogroup many tuples into a larger append before you send an acknowledgement from hello bolt.</span></span> <span data-ttu-id="a728c-180">즉, 대부분의 경우에 여러 태스크에는 추가적인 이점이 없습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-180">So, in most cases, multiple tasks provide no additional benefit.</span></span>
* <span data-ttu-id="a728c-181">**로컬 또는 순서 섞기 그룹화**</span><span class="sxs-lookup"><span data-stu-id="a728c-181">**Local or shuffle grouping.**</span></span> <span data-ttu-id="a728c-182">이 설정을 사용 하는 튜플을 보내집니다 toobolts hello 내에서 동일한 작업자 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-182">When this setting is enabled, tuples are sent toobolts within hello same worker process.</span></span> <span data-ttu-id="a728c-183">그러면 프로세스 간 통신과 네트워크 호출이 줄어듭니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-183">This reduces inter-process communication and network calls.</span></span> <span data-ttu-id="a728c-184">대부분의 토폴로지에 사용하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-184">This is recommended for most topologies.</span></span>

<span data-ttu-id="a728c-185">이 기본 시나리오는 시작 지점으로 사용하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-185">This basic scenario is a good starting point.</span></span> <span data-ttu-id="a728c-186">사용자 고유의 데이터 tootweak hello 앞에 매개 변수 tooachieve 최적의 성능을 테스트 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-186">Test with your own data tootweak hello preceding parameters tooachieve optimal performance.</span></span>

## <a name="tune-hello-spout"></a><span data-ttu-id="a728c-187">Hello 배출구 조정</span><span class="sxs-lookup"><span data-stu-id="a728c-187">Tune hello spout</span></span>

<span data-ttu-id="a728c-188">다음 설정을 tootune hello 배출구 hello를 수정할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-188">You can modify hello following settings tootune hello spout.</span></span>

- <span data-ttu-id="a728c-189">**튜플 시간 초과: topology.message.timeout.secs**</span><span class="sxs-lookup"><span data-stu-id="a728c-189">**Tuple timeout: topology.message.timeout.secs**.</span></span> <span data-ttu-id="a728c-190">이 설정은 hello 메시지 toocomplete에 걸리는 시간을 결정 하 고 간주 될 때까지 승인을 수신 하지 못했습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-190">This setting determines hello amount of time a message takes toocomplete, and receive acknowledgement, before it is considered failed.</span></span>

- <span data-ttu-id="a728c-191">**작업자 프로세스당 최대 메모리: worker.childopts**</span><span class="sxs-lookup"><span data-stu-id="a728c-191">**Max memory per worker process: worker.childopts**.</span></span> <span data-ttu-id="a728c-192">이 설정을 사용 하면 toohello Java 작업 자가 추가 명령줄 매개 변수를 지정할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-192">This setting lets you specify additional command-line parameters toohello Java workers.</span></span> <span data-ttu-id="a728c-193">이 설정은 가장 일반적으로 사용 되는 hello XmX hello 할당 된 최대 메모리 tooa JVM의 힙 결정 하는입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-193">hello most commonly used setting here is XmX, which determines hello maximum memory allocated tooa JVM’s heap.</span></span>

- <span data-ttu-id="a728c-194">**보류 중인 최대 Spout: topology.max.spout.pending**</span><span class="sxs-lookup"><span data-stu-id="a728c-194">**Max spout pending: topology.max.spout.pending**.</span></span> <span data-ttu-id="a728c-195">이 설정은 언제 든 지 배출구 스레드당 비행 (hello 토폴로지의 모든 노드에서 승인 하지 않은) 수에 있는 튜플 hello 수를 결정 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-195">This setting determines hello number of tuples that can in be flight (not yet acknowledged at all nodes in hello topology) per spout thread at any time.</span></span>

 <span data-ttu-id="a728c-196">좋은 계산 toodo는 프로그램 튜플의 각 tooestimate hello 크기입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-196">A good calculation toodo is tooestimate hello size of each of your tuples.</span></span> <span data-ttu-id="a728c-197">그런 다음 한 개의 Spout 스레드에 지정된 메모리 양을 파악합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-197">Then figure out how much memory one spout thread has.</span></span> <span data-ttu-id="a728c-198">hello이 값을 나눈 tooa 스레드에 할당 된 총 메모리를 제공 해야 hello 매개 변수 보류 중인 최대 배출구 hello에 대 한 상한입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-198">hello total memory allocated tooa thread, divided by this value, should give you hello upper bound for hello max spout pending parameter.</span></span>

## <a name="tune-hello-bolt"></a><span data-ttu-id="a728c-199">Hello 볼트 조정</span><span class="sxs-lookup"><span data-stu-id="a728c-199">Tune hello bolt</span></span>
<span data-ttu-id="a728c-200">TooData Lake 저장소를 작성 하는 경우 설정 크기 동기화 정책 (hello 클라이언트 쪽에서 버퍼) too4 MB입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-200">When you're writing tooData Lake Store, set a size sync policy (buffer on hello client side) too4 MB.</span></span> <span data-ttu-id="a728c-201">플러시 또는 hsync() hello 버퍼 크기는이 값에는 hello 하는 경우에 수행 됩니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-201">A flushing or hsync() is then performed only when hello buffer size is hello at this value.</span></span> <span data-ttu-id="a728c-202">VM 작업자의 hello hello 데이터 레이크 저장소 드라이버를 자동으로 수행이 버퍼링는 hsync()를 명시적으로 수행 하지 않는 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-202">hello Data Lake Store driver on hello worker VM automatically does this buffering, unless you explicitly perform an hsync().</span></span>

<span data-ttu-id="a728c-203">기본 데이터 레이크 저장소 스톰 볼트 hello 크기 동기화 정책 매개를 변수가 (fileBufferSize) tootune 사용된 될 수 있는이 매개 변수입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-203">hello default Data Lake Store Storm bolt has a size sync policy parameter (fileBufferSize) that can be used tootune this parameter.</span></span>

<span data-ttu-id="a728c-204">I/o 많은 토폴로지는 편이 좋습니다 toohave 각 볼트 스레드 쓰기 tooits 직접 파일 및 tooset 파일 회전 정책 (fileRotationSize).</span><span class="sxs-lookup"><span data-stu-id="a728c-204">In I/O-intensive topologies, it's a good idea toohave each bolt thread write tooits own file, and tooset a file rotation policy (fileRotationSize).</span></span> <span data-ttu-id="a728c-205">Hello 파일이 특정 크기에 도달 하면 hello 스트림을 자동으로 플러시할지 하 고 새 파일에 기록 됩니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-205">When hello file reaches a certain size, hello stream is automatically flushed and a new file is written to.</span></span> <span data-ttu-id="a728c-206">hello 권장 회전에 대 한 파일 크기는 1GB입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-206">hello recommended file size for rotation is 1 GB.</span></span>

### <a name="handle-tuple-data"></a><span data-ttu-id="a728c-207">튜플 데이터 처리</span><span class="sxs-lookup"><span data-stu-id="a728c-207">Handle tuple data</span></span>

<span data-ttu-id="a728c-208">명시적으로 인식에 hello 볼트 될 때까지 한 배출구 tooa 튜플에 보유 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-208">In Storm, a spout holds on tooa tuple until it is explicitly acknowledged by hello bolt.</span></span> <span data-ttu-id="a728c-209">튜플을 hello 볼트 읽은 아직 승인 되지 않은 경우, hello 배출구 백 엔드 데이터 레이크 저장소에 유지 되지 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-209">If a tuple has been read by hello bolt but has not been acknowledged yet, hello spout might not have persisted into Data Lake Store back end.</span></span> <span data-ttu-id="a728c-210">튜플을 승인 되었는지 hello 배출구 hello 볼트 하 여 지 속성을 보장할 수 하 고 모든 소스에서 읽고에서 hello 원본 데이터를 삭제할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-210">After a tuple is acknowledged, hello spout can be guaranteed persistence by hello bolt, and can then delete hello source data from whatever source it is reading from.</span></span>  

<span data-ttu-id="a728c-211">데이터 레이크 저장소의 최상의 성능을 위해 hello 볼트가 4MB의 튜플 데이터를 버퍼링 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-211">For best performance on Data Lake Store, have hello bolt buffer 4 MB of tuple data.</span></span> <span data-ttu-id="a728c-212">데이터 레이크 저장소를 다시 시작할 최 하나의 4MB 쓰기도 toohello를 작성 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-212">Then write toohello Data Lake Store back end as one 4-MB write.</span></span> <span data-ttu-id="a728c-213">Hello 데이터 저장소 toohello 성공적으로 작성 된 후에 (호출 hflush()) 하 여 hello 볼트 승인할 수 있습니다 hello 데이터 뒤로 toohello 배출구 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-213">After hello data has been successfully written toohello store (by calling hflush()), hello bolt can acknowledge hello data back toohello spout.</span></span> <span data-ttu-id="a728c-214">이 어떤 hello 예 볼트 여기있지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-214">This is what hello example bolt supplied here does.</span></span> <span data-ttu-id="a728c-215">허용 가능한 toohold 많은 hello hflush() 호출 전에 튜플 및 hello 이기도 튜플 승인 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-215">It is also acceptable toohold a larger number of tuples before hello hflush() call is made and hello tuples acknowledged.</span></span> <span data-ttu-id="a728c-216">하지만이 hello hello 배출구 toohold, 필요 하며 따라서 증가 hello jvm에 필요한 메모리 양을 한다고 항공편의 튜플 수를 늘립니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-216">However, this increases hello number of tuples in flight that hello spout needs toohold, and therefore increases hello amount of memory required per JVM.</span></span>

> [!NOTE]
<span data-ttu-id="a728c-217">응용 프로그램에는 다른 비 성능상의 이유로 더 자주 (데이터 크기 보다 작으면에서 4MB) 요구 사항 tooacknowledge 튜플이 있을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-217">Applications might have a requirement tooacknowledge tuples more frequently (at data sizes less than 4 MB) for other non-performance reasons.</span></span> <span data-ttu-id="a728c-218">그러나 hello I/O 처리량 toohello 저장소 백 엔드 영향을 줄 수입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-218">However, that might affect hello I/O throughput toohello storage back end.</span></span> <span data-ttu-id="a728c-219">Hello 볼트의 I/O 성능에 대 한 이러한 상충 관계를 신중 하 게 평가 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-219">Carefully weigh this tradeoff against hello bolt’s I/O performance.</span></span>

<span data-ttu-id="a728c-220">Hello 4MB 버퍼는 데 오랜 시간이 toofill 걸릴 hello 튜플의 수신 속도, 고 있으면이 완화 하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-220">If hello incoming rate of tuples is not high, so hello 4-MB buffer takes a long time toofill, consider mitigating this by:</span></span>
* <span data-ttu-id="a728c-221">볼트 hello 수를 줄이면 있습니다 버퍼 toofill 적습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-221">Reducing hello number of bolts, so there are fewer buffers toofill.</span></span>
* <span data-ttu-id="a728c-222">여기서는 hflush()은 시간 기반 또는 개수 기반 정책 있으면 매 flushes x 또는 y 밀리초 트리거되고 지금까지 축적 된 hello 튜플 백 승인이 전달 됩니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-222">Having a time-based or count-based policy, where an hflush() is triggered every x flushes or every y milliseconds, and hello tuples accumulated so far are acknowledged back.</span></span>

<span data-ttu-id="a728c-223">Hello 처리량이 경우 더 낮으면 하지만 이벤트 속도가 느린, 최대 처리량은 하지 hello 가장 큰 목표 그래도 note 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-223">Note that hello throughput in this case is lower, but with a slow rate of events, maximum throughput is not hello biggest objective anyway.</span></span> <span data-ttu-id="a728c-224">이러한 완화 소요 되는 튜플 tooflow toohello 스토어를 통해 hello 총 시간을 단축 하는 데 도움이 됩니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-224">These mitigations help you reduce hello total time that it takes for a tuple tooflow through toohello store.</span></span> <span data-ttu-id="a728c-225">이벤트 비율이 낮은 실시간 파이프라인을 사용하려는 경우 문제가 될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-225">This might matter if you want a real-time pipeline even with a low event rate.</span></span> <span data-ttu-id="a728c-226">튜플 수신율 프로그램 낮은 경우 조정 해야 hello topology.message.timeout_secs 매개 변수 hello 튜플 발생 하는 동안 시간 초과 하지 않는 하므로 참고 또한 버퍼링 또는 처리 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-226">Also note that if your incoming tuple rate is low, you should adjust hello topology.message.timeout_secs parameter, so hello tuples don’t time out while they are getting buffered or processed.</span></span>

## <a name="monitor-your-topology-in-storm"></a><span data-ttu-id="a728c-227">Storm에서 토폴로지 모니터링</span><span class="sxs-lookup"><span data-stu-id="a728c-227">Monitor your topology in Storm</span></span>  
<span data-ttu-id="a728c-228">토폴로지에 실행 되는 동안에 hello 스톰 사용자 인터페이스에서 모니터링할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-228">While your topology is running, you can monitor it in hello Storm user interface.</span></span> <span data-ttu-id="a728c-229">Hello 주요 매개 변수 toolook에는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-229">Here are hello main parameters toolook at:</span></span>

* <span data-ttu-id="a728c-230">**총 프로세스 실행 대기 시간**</span><span class="sxs-lookup"><span data-stu-id="a728c-230">**Total process execution latency.**</span></span> <span data-ttu-id="a728c-231">하나의 튜플은 hello 배출구에서 내보내는 hello 볼트에서 처리 하 고 승인 toobe hello 평균 시간입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-231">This is hello average time one tuple takes toobe emitted by hello spout, processed by hello bolt, and acknowledged.</span></span>

* <span data-ttu-id="a728c-232">**총 Bolt 프로세스 대기 시간**</span><span class="sxs-lookup"><span data-stu-id="a728c-232">**Total bolt process latency.**</span></span> <span data-ttu-id="a728c-233">Hello 승인의 받을 때까지 hello 볼트 hello 튜플은에서 소요 되는 평균 시간입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-233">This is hello average time spent by hello tuple at hello bolt until it receives an acknowledgement.</span></span>

* <span data-ttu-id="a728c-234">**총 Bolt 실행 대기 시간**</span><span class="sxs-lookup"><span data-stu-id="a728c-234">**Total bolt execute latency.**</span></span> <span data-ttu-id="a728c-235">이 hello 평균 메서드를 실행 하는 hello에 hello 볼트에서 소요 된 시간입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-235">This is hello average time spent by hello bolt in hello execute method.</span></span>

* <span data-ttu-id="a728c-236">**오류 수**</span><span class="sxs-lookup"><span data-stu-id="a728c-236">**Number of failures.**</span></span> <span data-ttu-id="a728c-237">이 toohello toobe 시간이 초과 되었습니다 전에 완전히 처리 하지 못한 튜플 수를 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-237">This refers toohello number of tuples that failed toobe fully processed before they timed out.</span></span>

* <span data-ttu-id="a728c-238">**용량**</span><span class="sxs-lookup"><span data-stu-id="a728c-238">**Capacity.**</span></span> <span data-ttu-id="a728c-239">시스템이 얼마나 사용 중인지를 측정한 값입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-239">This is a measure of how busy your system is.</span></span> <span data-ttu-id="a728c-240">1이면 Bolt가 가장 빠르게 작동 중입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-240">If this number is 1, your bolts are working as fast as they can.</span></span> <span data-ttu-id="a728c-241">1 보다 작은 경우 hello 병렬 처리를 늘립니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-241">If it is less than 1, increase hello parallelism.</span></span> <span data-ttu-id="a728c-242">1 보다 큰 경우 hello 병렬 처리를 줄입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-242">If it is greater than 1, reduce hello parallelism.</span></span>

## <a name="troubleshoot-common-problems"></a><span data-ttu-id="a728c-243">일반적인 문제 해결</span><span class="sxs-lookup"><span data-stu-id="a728c-243">Troubleshoot common problems</span></span>
<span data-ttu-id="a728c-244">일반적인 문제 해결 시나리오는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-244">Here are a few common troubleshooting scenarios.</span></span>
* <span data-ttu-id="a728c-245">**많은 튜플의 시간이 초과되었습니다.** 여기서는 hello 병목 hello 토폴로지 toodetermine 노드마다 살펴봅니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-245">**Many tuples are timing out.** Look at each node in hello topology toodetermine where hello bottleneck is.</span></span> <span data-ttu-id="a728c-246">이 대 한 가장 일반적인 이유 hello은 hello 볼트 되지 않는다는 수 tookeep를 hello spouts와 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-246">hello most common reason for this is that hello bolts are not able tookeep up with hello spouts.</span></span> <span data-ttu-id="a728c-247">이 대기 toobe 처리 하는 동안 내부 버퍼 hello 방해 tootuples를 이어집니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-247">This leads tootuples clogging hello internal buffers while waiting toobe processed.</span></span> <span data-ttu-id="a728c-248">Hello 제한 시간 값을 늘리거나 보류 중인 최대 배출구 hello 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-248">Consider increasing hello timeout value or decreasing hello max spout pending.</span></span>

* <span data-ttu-id="a728c-249">**총 프로세스 실행 대기 시간이 길지만 Bolt 프로세스 대기 시간이 짧습니다.**</span><span class="sxs-lookup"><span data-stu-id="a728c-249">**There is a high total process execution latency, but a low bolt process latency.**</span></span> <span data-ttu-id="a728c-250">이 경우는 hello 튜플 확인 되지 않은 되 고 충분히 빠르게 가능한입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-250">In this case, it is possible that hello tuples are not being acknowledged fast enough.</span></span> <span data-ttu-id="a728c-251">acknowledger 수가 충분한지 확인하세요.</span><span class="sxs-lookup"><span data-stu-id="a728c-251">Check that there are a sufficient number of acknowledgers.</span></span> <span data-ttu-id="a728c-252">또 다른 원인은 hello 쐐기 시작을 처리 하기 전에 hello 큐에 너무 오래 대기 중인 하 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-252">Another possibility is that they are waiting in hello queue for too long before hello bolts start processing them.</span></span> <span data-ttu-id="a728c-253">보류 중인 최대 배출구를 hello를 줄입니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-253">Decrease hello max spout pending.</span></span>

* <span data-ttu-id="a728c-254">**Bolt 실행 대기 시간이 깁니다.**</span><span class="sxs-lookup"><span data-stu-id="a728c-254">**There is a high bolt execute latency.**</span></span> <span data-ttu-id="a728c-255">이 프로그램 볼트의 hello execute () 메서드를 너무 오래 걸리는 것을 의미 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-255">This means that hello execute() method of your bolt is taking too long.</span></span> <span data-ttu-id="a728c-256">Hello 코드 최적화 또는 쓰기 크기와 동작을 플러시 하십시오.</span><span class="sxs-lookup"><span data-stu-id="a728c-256">Optimize hello code, or look at write sizes and flush behavior.</span></span>

### <a name="data-lake-store-throttling"></a><span data-ttu-id="a728c-257">Data Lake Store 제한</span><span class="sxs-lookup"><span data-stu-id="a728c-257">Data Lake Store throttling</span></span>
<span data-ttu-id="a728c-258">데이터 레이크 저장소에서 제공 하는 대역폭의 hello 한계에 도달 하는 경우 작업 실패가 표시 될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-258">If you hit hello limits of bandwidth provided by Data Lake Store, you might see task failures.</span></span> <span data-ttu-id="a728c-259">제한 오류에 대한 태스크 로그를 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-259">Check task logs for throttling errors.</span></span> <span data-ttu-id="a728c-260">컨테이너 크기를 늘려 hello 병렬 처리를 줄일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-260">You can decrease hello parallelism by increasing container size.</span></span>    

<span data-ttu-id="a728c-261">hello 디버그 hello 클라이언트 쪽에서 로깅을 설정 하는 경우 사용자는 제한에 이르기, toocheck:</span><span class="sxs-lookup"><span data-stu-id="a728c-261">toocheck if you are getting throttled, enable hello debug logging on hello client side:</span></span>

1. <span data-ttu-id="a728c-262">**Ambari** > **스톰** > **Config** > **스톰-작업자-log4j 고급**, 변경  **&lt;루트 수준 = "info"&gt;**  너무**&lt;루트 수준 = "debug"&gt;**합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-262">In **Ambari** > **Storm** > **Config** > **Advanced storm-worker-log4j**, change **&lt;root level="info"&gt;** too**&lt;root level=”debug”&gt;**.</span></span> <span data-ttu-id="a728c-263">모든 hello 노드/서비스 hello 구성 tootake 효과 대 한 다시 시작 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-263">Restart all hello nodes/service for hello configuration tootake effect.</span></span>
2. <span data-ttu-id="a728c-264">작업자 노드가 로그온 모니터 hello 스톰 토폴로지 (/var/log/storm/worker-artifacts 아래 /&lt;TopologyName&gt;/&lt;포트&gt;/worker.log) 예외를 제한 하는 데이터 레이크 저장소에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-264">Monitor hello Storm topology logs on worker nodes (under /var/log/storm/worker-artifacts/&lt;TopologyName&gt;/&lt;port&gt;/worker.log) for Data Lake Store throttling exceptions.</span></span>

## <a name="next-steps"></a><span data-ttu-id="a728c-265">다음 단계</span><span class="sxs-lookup"><span data-stu-id="a728c-265">Next steps</span></span>
<span data-ttu-id="a728c-266">Storm의 추가 성능 조정은 이 [블로그](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="a728c-266">Additional performance tuning for Storm can be referenced in [this blog](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/).</span></span>

<span data-ttu-id="a728c-267">참조는 추가 예제 toorun [GitHub에서이 이와](https://github.com/hdinsight/storm-performance-automation)합니다.</span><span class="sxs-lookup"><span data-stu-id="a728c-267">For an additional example toorun, see [this one on GitHub](https://github.com/hdinsight/storm-performance-automation).</span></span>
