---
title: "Service Fabric 클러스터 리소스 관리자 - 관리 통합 | Microsoft Docs"
description: "클러스터 리소스 관리자과 서비스 패브릭 관리 간에 통합 지점의 개요입니다."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: 956cd0b8-b6e3-4436-a224-8766320e8cd7
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 9601e758e1033b4e2f86c2c230d4f49479fe6f45
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/18/2017
---
# <a name="cluster-resource-manager-integration-with-service-fabric-cluster-management"></a><span data-ttu-id="ed1ee-103">클러스터 리소스 관리자와 서비스 패브릭 클러스터 관리 통합</span><span class="sxs-lookup"><span data-stu-id="ed1ee-103">Cluster resource manager integration with Service Fabric cluster management</span></span>
<span data-ttu-id="ed1ee-104">Service Fabric 클러스터 리소스 관리자는 Service Fabric에서 업그레이드를 수행하지는 않지만 관련되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-104">The Service Fabric Cluster Resource Manager doesn't drive upgrades in Service Fabric, but it is involved.</span></span> <span data-ttu-id="ed1ee-105">Cluster Resource Manager에서 관리에 유용하게 사용할 수 있는 첫 번째 방법은 내부에 있는 서비스와 클러스터의 필요한 상태를 추적하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-105">The first way that the Cluster Resource Manager helps with management is by tracking the desired state of the cluster and the services inside it.</span></span> <span data-ttu-id="ed1ee-106">Cluster Resource Manager는 원하는 구성에 클러스터를 배치할 수 없는 경우 상태 보고서를 보냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-106">The Cluster Resource Manager sends out health reports when it cannot put the cluster into the desired configuration.</span></span> <span data-ttu-id="ed1ee-107">예를 들어 용량이 부족하면 클러스터 리소스 관리자에서 문제를 나타내는 상태 경고 및 오류를 보냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-107">For example, if there is insufficient capacity the Cluster Resource Manager sends out health warnings and errors indicating the problem.</span></span> <span data-ttu-id="ed1ee-108">통합의 다른 부분은 업그레이드 작동 방법와 관련이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-108">Another piece of integration has to do with how upgrades work.</span></span> <span data-ttu-id="ed1ee-109">클러스터 리소스 관리자는 업그레이드 중에 해당 동작을 약간 변경합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-109">The Cluster Resource Manager alters its behavior slightly during upgrades.</span></span>  

## <a name="health-integration"></a><span data-ttu-id="ed1ee-110">상태 통합</span><span class="sxs-lookup"><span data-stu-id="ed1ee-110">Health integration</span></span>
<span data-ttu-id="ed1ee-111">클러스터 리소스 관리자는 서비스를 배치하기 위해 정의한 규칙을 지속적으로 추적합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-111">The Cluster Resource Manager constantly tracks the rules you have defined for placing your services.</span></span> <span data-ttu-id="ed1ee-112">또한 노드와 클러스터 및 클러스터 전체에서 각 메트릭에 남은 용량을 추적합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-112">It also tracks the remaining capacity for each metric on the nodes and in the cluster and in the cluster as a whole.</span></span> <span data-ttu-id="ed1ee-113">해당 규칙을 충족할 수 없거나 용량이 부족한 경우 상태 경고 및 오류를 내보냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-113">If it can't satisfy those rules or if there is insufficient capacity, health warnings and errors are emitted.</span></span> <span data-ttu-id="ed1ee-114">예를 들어, 노드가 용량을 초과하면 Cluster Resource Manager는 서비스를 이동하여 상황을 수정하려고 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-114">For example, if a node is over capacity and the Cluster Resource Manager will try to fix the situation by moving services.</span></span> <span data-ttu-id="ed1ee-115">상황을 수정할 수 없는 경우 용량을 초과한 노드와 해당 메트릭을 나타내는 상태 경고를 내보냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-115">If it can't correct the situation it emits a health warning indicating which node is over capacity, and for which metrics.</span></span>

<span data-ttu-id="ed1ee-116">Resource Manager 상태 경고의 또 다른 예는 배치 제약 조건 위반에 대한 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-116">Another example of the Resource Manager's health warnings is violations of placement constraints.</span></span> <span data-ttu-id="ed1ee-117">예를 들어 배치 제약 조건(예: `“NodeColor == Blue”`)을 정의하고 리소스 관리자에서 해당 제약 조건 위반을 검색한 경우 상태 경고를 내보냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-117">For example, if you have defined a placement constraint (such as `“NodeColor == Blue”`) and the Resource Manager detects a violation of that constraint, it emits a health warning.</span></span> <span data-ttu-id="ed1ee-118">이는 사용자 지정 제약 조건 및 기본 제약 조건(예: 장애 도메인 및 업그레이드 도메인 제약 조건)에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-118">This is true for custom constraints and the default constraints (like the Fault Domain and Upgrade Domain constraints).</span></span>

<span data-ttu-id="ed1ee-119">이러한 상태 보고서의 예를 들면 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-119">Here’s an example of one such health report.</span></span> <span data-ttu-id="ed1ee-120">이 경우에 상태 보고서는 시스템 서비스의 파티션 중 하나에 대한 내용입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-120">In this case, the health report is for one of the system service’s partitions.</span></span> <span data-ttu-id="ed1ee-121">상태 메시지는 해당 파티션의 복제본이 너무 적은 업그레이드 도메인에 일시적으로 압축되었음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-121">The health message indicates the replicas of that partition are temporarily packed into too few Upgrade Domains.</span></span>

```posh
PS C:\Users\User > Get-WindowsFabricPartitionHealth -PartitionId '00000000-0000-0000-0000-000000000001'


PartitionId           : 00000000-0000-0000-0000-000000000001
AggregatedHealthState : Warning
UnhealthyEvaluations  :
                        Unhealthy event: SourceId='System.PLB', Property='ReplicaConstraintViolation_UpgradeDomain', HealthState='Warning', ConsiderWarningAsError=false.

ReplicaHealthStates   :
                        ReplicaId             : 130766528804733380
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577821
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528854889931
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577822
                        AggregatedHealthState : Ok

                        ReplicaId             : 130837073190680024
                        AggregatedHealthState : Ok

HealthEvents          :
                        SourceId              : System.PLB
                        Property              : ReplicaConstraintViolation_UpgradeDomain
                        HealthState           : Warning
                        SequenceNumber        : 130837100116930204
                        SentAt                : 8/10/2015 7:53:31 PM
                        ReceivedAt            : 8/10/2015 7:53:33 PM
                        TTL                   : 00:01:05
                        Description           : The Load Balancer has detected a Constraint Violation for this Replica: fabric:/System/FailoverManagerService Secondary Partition 00000000-0000-0000-0000-000000000001 is
                        violating the Constraint: UpgradeDomain Details: UpgradeDomain ID -- 4, Replica on NodeName -- Node.8 Currently Upgrading -- false Distribution Policy -- Packing
                        RemoveWhenExpired     : True
                        IsExpired             : False
                        Transitions           : Ok->Warning = 8/10/2015 7:13:02 PM, LastError = 1/1/0001 12:00:00 AM
```

<span data-ttu-id="ed1ee-122">상태 메시지는 다음과 같은 사항을 알려줍니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-122">Here's what this health message is telling us is:</span></span>

1. <span data-ttu-id="ed1ee-123">모든 복제본 자체는 정상(각 복제본에 AggregatedHealthState가 있습니다: 확인)입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-123">All the replicas themselves are healthy: Each has AggregatedHealthState : Ok</span></span>
2. <span data-ttu-id="ed1ee-124">현재 업그레이드 도메인 분산 제약 조건이 위반되었습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-124">The Upgrade Domain distribution constraint is currently being violated.</span></span> <span data-ttu-id="ed1ee-125">이는 특정 업그레이드 도메인에 이 파티션보다 더 많은 복제본이 있음을 의미합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-125">This means a particular Upgrade Domain has more replicas from this partition than it should.</span></span>
3. <span data-ttu-id="ed1ee-126">위반이 발생하는 복제본을 포함한 노드 -</span><span class="sxs-lookup"><span data-stu-id="ed1ee-126">Which node contains the replica causing the violation.</span></span> <span data-ttu-id="ed1ee-127">이 경우 이름이 "Node.8"인 노드입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-127">In this case it's the node with the name "Node.8"</span></span>
4. <span data-ttu-id="ed1ee-128">이 파티션에서 현재 업그레이드가 진행 중인지 여부("현재 업그레이드 중 - false")</span><span class="sxs-lookup"><span data-stu-id="ed1ee-128">Whether an upgrade is currently happening for this partition ("Currently Upgrading -- false")</span></span>
5. <span data-ttu-id="ed1ee-129">이 서비스에 대한 배포 정책("배포 정책 - 압축 중") -</span><span class="sxs-lookup"><span data-stu-id="ed1ee-129">The distribution policy for this service: "Distribution Policy -- Packing".</span></span> <span data-ttu-id="ed1ee-130">`RequireDomainDistribution` [배치 정책](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing)으로 관리됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-130">This is governed by the `RequireDomainDistribution` [placement policy](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing).</span></span> <span data-ttu-id="ed1ee-131">"압축 중"은 이 경우 DomainDistribution이 필요하지 _않음_을 나타냅니다. 따라서 이 서비스에 대해 배치 정책이 지정되지 않았습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-131">"Packing" indicates that in this case DomainDistribution was _not_ required, so we know that placement policy was not specified for this service.</span></span> 
6. <span data-ttu-id="ed1ee-132">보고서가 발생한 시기 - 2015년 8월 10일 오후 7:13:02</span><span class="sxs-lookup"><span data-stu-id="ed1ee-132">When the report happened - 8/10/2015 7:13:02 PM</span></span>

<span data-ttu-id="ed1ee-133">이와 같은 정보는 무언가 문제가 발생했음을 알려주기 위해 프로덕션에서 실행되는 경고를 제공하며, 잘못된 업그레이드를 검색하고 중단하는 데에도 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-133">Information like this powers alerts that fire in production to let you know something has gone wrong and is also used to detect and halt bad upgrades.</span></span> <span data-ttu-id="ed1ee-134">이 경우에 Resource Manager가 업그레이드 도메인에 복제본을 압축해야 했던 이유를 알아낼 수 있는지 확인하려고 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-134">In this case, we’d want to see if we can figure out why the Resource Manager had to pack the replicas into the Upgrade Domain.</span></span> <span data-ttu-id="ed1ee-135">예를 들어 다른 업그레이드 도메인의 노드가 중단되었기 때문에 압축이 일반적으로 일시적일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-135">Usually packing is transient because the nodes in the other Upgrade Domains were down, for example.</span></span>

<span data-ttu-id="ed1ee-136">클러스터 리소스 관리자에서 일부 서비스를 배치하려고 시도하지만 작동하는 솔루션이 없다고 가정해 보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-136">Let’s say the Cluster Resource Manager is trying to place some services, but there aren't any solutions that work.</span></span> <span data-ttu-id="ed1ee-137">서비스를 배치할 수 없는 경우 일반적으로 다음과 같은 이유 중 하나에 해당합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-137">When services can't be placed, it is usually for one of the following reasons:</span></span>

1. <span data-ttu-id="ed1ee-138">일부 일시적 조건 때문에 서비스 인스턴스 또는 복제본을 올바르게 배치할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-138">Some transient condition has made it impossible to place this service instance or replica correctly</span></span>
2. <span data-ttu-id="ed1ee-139">서비스의 배치 요구 사항이 만족스럽지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-139">The service’s placement requirements are unsatisfiable.</span></span>

<span data-ttu-id="ed1ee-140">이러한 경우 클러스터 리소스 관리자의 상태 보고서를 통해 서비스를 배치할 수 없는 이유를 확인할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-140">In these cases, health reports from the Cluster Resource Manager help you determine why the service can’t be placed.</span></span> <span data-ttu-id="ed1ee-141">이 프로세스를 제약 조건 제거 시퀀스라고 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-141">We call this process the constraint elimination sequence.</span></span> <span data-ttu-id="ed1ee-142">이 프로세스를 실행하는 동안 서비스에 영향을 주는 구성된 제약 조건이 설명되고 제거한 항목이 기록됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-142">During it, the system walks through the configured constraints affecting the service and records what they eliminate.</span></span> <span data-ttu-id="ed1ee-143">이와 같이 서비스를 배치할 수 없는 경우 제거된 노드 및 원인을 확인할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-143">This way when services aren’t able to be placed, you can see which nodes were eliminated and why.</span></span>

## <a name="constraint-types"></a><span data-ttu-id="ed1ee-144">제약 조건 형식</span><span class="sxs-lookup"><span data-stu-id="ed1ee-144">Constraint types</span></span>
<span data-ttu-id="ed1ee-145">이러한 상태 보고서에서 서로 다른 제약 조건 각각에 대해 살펴보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-145">Let’s talk about each of the different constraints in these health reports.</span></span> <span data-ttu-id="ed1ee-146">복제본을 배치할 수 없는 경우 이러한 제약 조건과 관련된 상태 메시지가 표시됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-146">You will see health messages related to these constraints when replicas can't be placed.</span></span>

* <span data-ttu-id="ed1ee-147">**ReplicaExclusionStatic** 및 **ReplicaExclusionDynamic**: 이러한 제약 조건은 동일한 파티션의 두 서비스 개체가 동일한 노드에 배치되어야 하므로 솔루션이 거부되었음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-147">**ReplicaExclusionStatic** and **ReplicaExclusionDynamic**: These constraints indicates that a solution was rejected because two service objects from the same partition would have to be placed on the same node.</span></span> <span data-ttu-id="ed1ee-148">이는 해당 노드의 실패가 해당 파티션에 지나치게 영향을 줄 수 있으므로 허용되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-148">This isn’t allowed because then failure of that node would overly impact that partition.</span></span> <span data-ttu-id="ed1ee-149">ReplicaExclusionStatic 및 ReplicaExclusionDynamic은 거의 동일한 규칙이며, 차이점은 실제로 문제가 되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-149">ReplicaExclusionStatic and ReplicaExclusionDynamic are almost the same rule and the differences don't really matter.</span></span> <span data-ttu-id="ed1ee-150">ReplicaExclusionStatic 또는 ReplicaExclusionDynamic 제약 조건이 포함된 제약 조건 제거 시퀀스가 표시되는 경우 클러스터 리소스 관리자에서는 노드가 충분하지 않다고 인식합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-150">If you are seeing a constraint elimination sequence containing either the ReplicaExclusionStatic or ReplicaExclusionDynamic constraint, the Cluster Resource Manager thinks that there aren’t enough nodes.</span></span> <span data-ttu-id="ed1ee-151">이 경우 나머지 솔루션에서 허용되지 않는 이러한 잘못된 배치를 사용해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-151">This requires remaining solutions to use these invalid placements which are disallowed.</span></span> <span data-ttu-id="ed1ee-152">시퀀스의 다른 제약 조건은 일반적으로 노드가 처음부터 제거되는 이유를 알려줍니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-152">The other constraints in the sequence will usually tell us why nodes are being eliminated in the first place.</span></span>
* <span data-ttu-id="ed1ee-153">**PlacementConstraint**: 이 메시지가 표시되는 경우 서비스의 배치 제약 조건에 일치하지 않는 일부 노드를 제거했음을 의미합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-153">**PlacementConstraint**: If you see this message, it means that we eliminated some nodes because they didn’t match the service’s placement constraints.</span></span> <span data-ttu-id="ed1ee-154">현재 구성된 배치 제약 조건을 이 메시지의 일부분으로 추적합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-154">We trace out the currently configured placement constraints as a part of this message.</span></span> <span data-ttu-id="ed1ee-155">배치 제약 조건이 정의되어 있는 경우 이는 정상입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-155">This is normal if you have a placement constraint defined.</span></span> <span data-ttu-id="ed1ee-156">그러나 배치 제약 조건이 잘못되어 너무 많은 노드를 제거하면 이를 인식하게 됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-156">However, if placement constraint is incorrectly causing too many nodes to be eliminated this is how you would notice.</span></span>
* <span data-ttu-id="ed1ee-157">**NodeCapacity**: 이 제약 조건은 노드의 용량을 초과할 수 있기 때문에 클러스터 리소스 관리자에서 지정된 노드에 복제본을 배치할 수 없었다는 의미입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-157">**NodeCapacity**: This constraint means that the Cluster Resource Manager couldn’t place the replicas on the indicated nodes because that would put them over capacity.</span></span>
* <span data-ttu-id="ed1ee-158">**선호도**: 이 제약 조건은 선호도 제약 조건을 위반하기 때문에 영향을 받는 노드에 복제본을 배치할 수 없었다는 것을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-158">**Affinity**: This constraint indicates that we couldn’t place the replica on the affected nodes since it would cause a violation of the affinity constraint.</span></span> <span data-ttu-id="ed1ee-159">선호도에 대한 자세한 내용은 [이 문서](service-fabric-cluster-resource-manager-advanced-placement-rules-affinity.md)에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-159">More information on affinity is in [this article](service-fabric-cluster-resource-manager-advanced-placement-rules-affinity.md)</span></span>
* <span data-ttu-id="ed1ee-160">**FaultDomain** 및 **UpgradeDomain**: 이 제약 조건은 표시된 노드에 복제본을 배치하여 특정 장애 또는 업그레이드 도메인에서 압축이 일어나는 경우 노드를 제거합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-160">**FaultDomain** and **UpgradeDomain**: This constraint eliminates nodes if placing the replica on the indicated nodes would cause packing in a particular fault or upgrade domain.</span></span> <span data-ttu-id="ed1ee-161">이 제약 조건을 설명하는 몇 가지 예는 [장애 및 업그레이드 도메인 제약 조건 및 결과 동작](service-fabric-cluster-resource-manager-cluster-description.md)</span><span class="sxs-lookup"><span data-stu-id="ed1ee-161">Several examples discussing this constraint are presented in the topic on [fault and upgrade domain constraints and resulting behavior](service-fabric-cluster-resource-manager-cluster-description.md)</span></span>
* <span data-ttu-id="ed1ee-162">**PreferredLocation**: 기본적으로 최적화로 실행되므로 솔루션에서 노드를 제거하면 일반적으로 이 제약 조건이 표시되지 않아야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-162">**PreferredLocation**: You shouldn’t normally see this constraint removing nodes from the solution since it runs as an optimization by default.</span></span> <span data-ttu-id="ed1ee-163">또한 기본 설정 위치 제약 조건은 업그레이드 중에도 존재합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-163">The preferred location constraint is also present during upgrades.</span></span> <span data-ttu-id="ed1ee-164">이 제약 조건은 업그레이드하는 동안 업그레이드를 시작했을 때의 위치로 서비스를 다시 이동하는 데 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-164">During upgrade it is used to move services back to where they were when the upgrade started.</span></span>

## <a name="blocklisting-nodes"></a><span data-ttu-id="ed1ee-165">노드 차단 목록 작성</span><span class="sxs-lookup"><span data-stu-id="ed1ee-165">Blocklisting Nodes</span></span>
<span data-ttu-id="ed1ee-166">클러스터 리소스 관리자에서 보고하는 또 다른 상태 메시지는 노드가 차단 목록에 있는 경우입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-166">Another health message the Cluster Resource Manager reports is when nodes are blocklisted.</span></span> <span data-ttu-id="ed1ee-167">차단 목록은 자동으로 적용되는 임시 제약 조건으로 간주할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-167">You can think of blocklisting as a temporary constraint that is automatically applied for you.</span></span> <span data-ttu-id="ed1ee-168">해당 서비스 유형의 인스턴스를 시작할 때 반복적인 오류가 발생하면 노드가 차단 목록에 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-168">Nodes get blocklisted when they experience repeated failures when launching instances of that service type.</span></span> <span data-ttu-id="ed1ee-169">노드는 서비스 유형별로 차단 목록에 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-169">Nodes are blocklisted on a per-service-type basis.</span></span> <span data-ttu-id="ed1ee-170">하나의 서비스 유형에 대한 노드가 차단 목록에 포함되지만 다른 유형에 대해서는 차단 목록에 포함될 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-170">A node may be blocklisted for one service type but not another.</span></span> 

<span data-ttu-id="ed1ee-171">개발하는 중에 차단 목록이 자주 표시됩니다. 일부 버그로 인해 서비스 호스트가 시작될 때 크래시가 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-171">You'll see blocklisting kick in often during development: some bug causes your service host to crash on startup.</span></span> <span data-ttu-id="ed1ee-172">Service Fabric에서 서비스 호스트를 만들려고 몇 번을 시도하지만 오류가 계속 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-172">Service Fabric tries to create the service host a few times, and the failure keeps occurring.</span></span> <span data-ttu-id="ed1ee-173">몇 번 시도한 후에 해당 노드가 차단 목록에 포함되고 클러스터 리소스 관리자에서는 다른 위치에 해당 서비스를 만들려고 시도합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-173">After a few attempts, the node gets blocklisted, and the Cluster Resource Manager will try to create the service elsewhere.</span></span> <span data-ttu-id="ed1ee-174">이 오류가 여러 노드에서 계속 발생하면 클러스터에서 유효한 모든 노드가 차단될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-174">If that failure keeps happening on multiple nodes, it's possible that all of the valid nodes in the cluster end up blocked.</span></span> <span data-ttu-id="ed1ee-175">또한 차단 목록 작성은 원하는 규모에 맞게 서비스를 성공적으로 시작할 수 없을 만큼 너무 많은 노드를 제거할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-175">Blocklisting cna also remove so many nodes that not enough can successfully launch the service to meet the desired scale.</span></span> <span data-ttu-id="ed1ee-176">일반적으로 서비스가 원하는 복제본 또는 인스턴스 수 이하에 있음을 나타내는 추가 오류 또는 경고뿐만 아니라 처음에 차단 목록으로 연결되는 오류가 무엇인지 알려주는 상태 메시지도 클러스터 리소스 관리자에서 표시합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-176">You'll typically see additional errors or warnings from the Cluster Resource Manager indicating that the service is below the desired replica or instance count, as well as health messages indicating what the failure is that's leading to the blocklisting in the first place.</span></span>

<span data-ttu-id="ed1ee-177">차단 목록은 영구적인 조건이 아닙니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-177">Blocklisting is not a permanent condition.</span></span> <span data-ttu-id="ed1ee-178">몇 분 후에 노드가 차단 목록에서 제거되고 Service Fabric에서 해당 노드의 서비스를 다시 활성화할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-178">After a few minutes, the node is removed from the blocklist and Service Fabric may activate the services on that node again.</span></span> <span data-ttu-id="ed1ee-179">서비스가 계속 실패하면 해당 서비스 유형에 대한 노드가 차단 목록에 다시 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-179">If services continue to fail, the node is blocklisted for that service type again.</span></span> 

### <a name="constraint-priorities"></a><span data-ttu-id="ed1ee-180">제약 조건 우선 순위</span><span class="sxs-lookup"><span data-stu-id="ed1ee-180">Constraint priorities</span></span>

> [!WARNING]
> <span data-ttu-id="ed1ee-181">제약 조건 우선 순위를 변경하는 것은 권장되지 않으며 클러스터에 심각하게 부정적인 영향을 미칠 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-181">Changing constraint priorities is not recommended and may have significant adverse effects on your cluster.</span></span> <span data-ttu-id="ed1ee-182">아래의 정보는 기본 제약 조건 우선 순위 및 그 동작을 참조하기 위해 제공됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-182">The below information is provided for reference of the default constraint priorities and their behavior.</span></span> 
>

<span data-ttu-id="ed1ee-183">이러한 모든 제약 조건을 고려할 때 "내 시스템에서 장애 도메인 제약 조건이 가장 중요하다고 생각합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-183">With all of these constraints, you may have been thinking “Hey – I think that fault domain constraints are the most important thing in my system.</span></span> <span data-ttu-id="ed1ee-184">장애 도메인 제약 조건을 위반하지 않도록 보장하기 위해 다른 제약 조건을 위반하려고 합니다."라고 생각했을 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-184">In order to ensure the fault domain constraint isn't violated, I’m willing to violate other constraints.”</span></span>

<span data-ttu-id="ed1ee-185">제약 조건은 서로 다른 우선 순위 수준으로 구성할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-185">Constraints can be configured with different priority levels.</span></span> <span data-ttu-id="ed1ee-186">다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-186">These are:</span></span>

   - <span data-ttu-id="ed1ee-187">"하드" (0)</span><span class="sxs-lookup"><span data-stu-id="ed1ee-187">“hard” (0)</span></span>
   - <span data-ttu-id="ed1ee-188">"소프트" (1)</span><span class="sxs-lookup"><span data-stu-id="ed1ee-188">“soft” (1)</span></span>
   - <span data-ttu-id="ed1ee-189">"최적화" (2)</span><span class="sxs-lookup"><span data-stu-id="ed1ee-189">“optimization” (2)</span></span>
   - <span data-ttu-id="ed1ee-190">"해제" (-1)</span><span class="sxs-lookup"><span data-stu-id="ed1ee-190">“off” (-1).</span></span> 
   
<span data-ttu-id="ed1ee-191">대부분의 제약 조건은 기본적으로 하드 제약 조건으로 구성됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-191">Most of the constraints are configured as hard constraints by default.</span></span>

<span data-ttu-id="ed1ee-192">제약 조건의 우선 순위 변경은 흔하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-192">Changing the priority of constraints is uncommon.</span></span> <span data-ttu-id="ed1ee-193">대개 환경에 영향을 미친 다른 버그 또는 동작을 해결하기 위해 제약 조건 우선 순위를 변경해야 하는 경우가 있었습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-193">There have been times where constraint priorities needed to change, usually to work around some other bug or behavior that was impacting the environment.</span></span> <span data-ttu-id="ed1ee-194">일반적으로 제약 조건 우선 순위 인프라의 유연성은 훌륭하게 작동하지만 자주 필요하지는 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-194">Generally the flexibility of the constraint priority infrastructure has worked very well, but it isn't needed often.</span></span> <span data-ttu-id="ed1ee-195">모든 항목은 대부분 해당 기본 우선 순위에 위치합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-195">Most of the time everything sits at their default priorities.</span></span> 

<span data-ttu-id="ed1ee-196">우선 순위 수준은 _지정된 제약 조건이 위반되거나 항상 충족된다_는 것을 의미하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-196">The priority levels don't mean that a given constraint _will_ be violated, nor that it will always be met.</span></span> <span data-ttu-id="ed1ee-197">제약 조건 우선 순위는 제약 조건이 적용되는 순서를 정의합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-197">Constraint priorities define an order in which constraints are enforced.</span></span> <span data-ttu-id="ed1ee-198">우선 순위는 모든 제약 조건을 충족할 수 없는 경우의 절충 작업을 정의합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-198">Priorities define the tradeoffs when it is impossible to satisfy all constraints.</span></span> <span data-ttu-id="ed1ee-199">일반적으로 환경에 다른 작업이 없다면 모든 제약 조건이 충족될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-199">Usually all the constraints can be satisfied unless there's something else going on in the environment.</span></span> <span data-ttu-id="ed1ee-200">제약 조건 위반으로 이어질 시나리오의 몇 가지 예로 충돌하는 제약 조건 또는 많은 수의 동시 실패가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-200">Some examples of scenarios that will lead to constraint violations are conflicting constraints, or large numbers of concurrent failures.</span></span>

<span data-ttu-id="ed1ee-201">고급 시나리오에서는 제약 조건 우선 순위를 변경할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-201">In advanced situations, you can change the constraint priorities.</span></span> <span data-ttu-id="ed1ee-202">예를 들어 노드 용량 문제를 해결하는 데 필요한 경우 선호도를 항상 위반할 수 있다고 가정해 보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-202">For example, say you wanted to ensure that affinity would always be violated when necessary to solve node capacity issues.</span></span> <span data-ttu-id="ed1ee-203">이를 위해 선호도 제약 조건의 우선순위를 "soft"(1)로 설정하고 용량 제약 조건을 "hard"(0)로 설정해 둘 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-203">To achieve this, you could set the priority of the affinity constraint to “soft” (1) and leave the capacity constraint set to “hard” (0).</span></span>

<span data-ttu-id="ed1ee-204">다양한 제약 조건에 대한 기본 우선 순위 값이 지정된 구성은 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-204">The default priority values for the different constraints are specified in the following config:</span></span>

<span data-ttu-id="ed1ee-205">ClusterManifest.xml</span><span class="sxs-lookup"><span data-stu-id="ed1ee-205">ClusterManifest.xml</span></span>

```xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PlacementConstraintPriority" Value="0" />
            <Parameter Name="CapacityConstraintPriority" Value="0" />
            <Parameter Name="AffinityConstraintPriority" Value="0" />
            <Parameter Name="FaultDomainConstraintPriority" Value="0" />
            <Parameter Name="UpgradeDomainConstraintPriority" Value="1" />
            <Parameter Name="PreferredLocationConstraintPriority" Value="2" />
        </Section>
```

<span data-ttu-id="ed1ee-206">독립 실행형 배포의 경우 ClusterConfig.json 또는 Azure 호스티드 클러스터의 경우 Template.json를 통해 수행됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-206">via ClusterConfig.json for Standalone deployments or Template.json for Azure hosted clusters:</span></span>

```json
"fabricSettings": [
  {
    "name": "PlacementAndLoadBalancing",
    "parameters": [
      {
          "name": "PlacementConstraintPriority",
          "value": "0"
      },
      {
          "name": "CapacityConstraintPriority",
          "value": "0"
      },
      {
          "name": "AffinityConstraintPriority",
          "value": "0"
      },
      {
          "name": "FaultDomainConstraintPriority",
          "value": "0"
      },
      {
          "name": "UpgradeDomainConstraintPriority",
          "value": "1"
      },
      {
          "name": "PreferredLocationConstraintPriority",
          "value": "2"
      }
    ]
  }
]
```

## <a name="fault-domain-and-upgrade-domain-constraints"></a><span data-ttu-id="ed1ee-207">장애 도메인 및 업그레이드 도메인 제약 조건</span><span class="sxs-lookup"><span data-stu-id="ed1ee-207">Fault domain and upgrade domain constraints</span></span>
<span data-ttu-id="ed1ee-208">클러스터 리소스 관리자에서 장애 도메인과 업그레이드 도메인 간에 서비스가 분산되도록 유지하려고 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-208">The Cluster Resource Manager wants to keep services spread out among fault and upgrade domains.</span></span> <span data-ttu-id="ed1ee-209">이를 위해 클러스터 리소스 관리자 엔진 내부의 제약 조건으로 모델링합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-209">It models this as a constraint inside the Cluster Resource Manager’s engine.</span></span> <span data-ttu-id="ed1ee-210">이들의 사용 방법과 특정 동작에 대한 자세한 내용은 [클러스터 구성](service-fabric-cluster-resource-manager-cluster-description.md#fault-and-upgrade-domain-constraints-and-resulting-behavior) 문서를 확인하세요.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-210">For more information on how they are used and their specific behavior, check out the article on [cluster configuration](service-fabric-cluster-resource-manager-cluster-description.md#fault-and-upgrade-domain-constraints-and-resulting-behavior).</span></span>

<span data-ttu-id="ed1ee-211">클러스터 리소스 관리자는 업그레이드, 오류 또는 다른 제약 조건 위반을 처리하기 위해 업그레이드 도메인에 몇 개의 복제본을 압축해야 할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-211">The Cluster Resource Manager may need to pack a couple replicas into an upgrade domain in order to deal with upgrades, failures, or other constraint violations.</span></span> <span data-ttu-id="ed1ee-212">장애 도메인 또는 업그레이드 도메인에 압축하는 것은 일반적으로 시스템에 여러 번의 실패 또는 다른 변동이 있어 올바르게 배치할 수 없는 경우에만 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-212">Packing into fault or upgrade domains normally happens only when there are several failures or other churn in the system preventing correct placement.</span></span> <span data-ttu-id="ed1ee-213">이러한 상황에서도 압축을 방지하려면 `RequireDomainDistribution` [배치 정책](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing)을 활용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-213">If you wish to prevent packing even during these situations, you can utilize the `RequireDomainDistribution` [placement policy](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing).</span></span> <span data-ttu-id="ed1ee-214">이 경우 서비스 가용성 및 안정성에 부작용을 미칠 수 있으므로 신중하게 고려해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-214">Note that this may affect service availability and reliability as a side effect, so consider it carefully.</span></span>

<span data-ttu-id="ed1ee-215">환경이 올바르게 구성되면 업그레이드 중에도 모든 제약 조건이 완전히 준수됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-215">If the environment is configured correctly, all constraints are fully respected, even during upgrades.</span></span> <span data-ttu-id="ed1ee-216">클러스터 리소스 관리자에서 제약 조건을 감시하고 있다는 것이 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-216">The key thing is that the Cluster Resource Manager is watching out for your constraints.</span></span> <span data-ttu-id="ed1ee-217">위반을 검색하는 즉시 문제를 보고하고 해결하려고 시도합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-217">When it detects a violation it immediately reports it and tries to correct the issue.</span></span>

## <a name="the-preferred-location-constraint"></a><span data-ttu-id="ed1ee-218">기본 설정 위치 제약 조건</span><span class="sxs-lookup"><span data-stu-id="ed1ee-218">The preferred location constraint</span></span>
<span data-ttu-id="ed1ee-219">PreferredLocation 제약 조건은 두 가지 용도로 사용되므로 약간 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-219">The PreferredLocation constraint is a little different, as it has two different uses.</span></span> <span data-ttu-id="ed1ee-220">이 제약 조건의 한 가지 용도는 응용 프로그램 업그레이드 중에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-220">One use of this constraint is during application upgrades.</span></span> <span data-ttu-id="ed1ee-221">클러스터 리소스 관리자는 업그레이드하는 동안 이 제약 조건을 자동으로 관리합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-221">The Cluster Resource Manager automatically manages this constraint during upgrades.</span></span> <span data-ttu-id="ed1ee-222">업그레이드가 완료되면 해당 복제본이 초기 위치로 반환되는지 확인하는 데 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-222">It is used to ensure that when upgrades are complete that replicas return to their initial locations.</span></span> <span data-ttu-id="ed1ee-223">PreferredLocation 제약 조건의 다른 용도는 [ `PreferredPrimaryDomain`배치 정책](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)에 대한 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-223">The other use of the PreferredLocation constraint is for the [`PreferredPrimaryDomain` placement policy](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md).</span></span> <span data-ttu-id="ed1ee-224">이러한 두 가지 용도가 모두 최적화이므로 PreferredLocation 제약 조건은 기본적으로 "Optimization(최적화)"으로 설정되는 유일한 제약 조건입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-224">Both of these are optimizations, and hence the PreferredLocation constraint is the only constraint set to "Optimization" by default.</span></span>

## <a name="upgrades"></a><span data-ttu-id="ed1ee-225">업그레이드</span><span class="sxs-lookup"><span data-stu-id="ed1ee-225">Upgrades</span></span>
<span data-ttu-id="ed1ee-226">응용 프로그램 및 클러스터를 업그레이드하는 동안 Cluster Resource Manager가 유용합니다. 이 때 다음과 같은 두 개의 작업이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-226">The Cluster Resource Manager also helps during application and cluster upgrades, during which it has two jobs:</span></span>

* <span data-ttu-id="ed1ee-227">클러스터 규칙이 손상되지 않았는지 확인하는 작업</span><span class="sxs-lookup"><span data-stu-id="ed1ee-227">ensure that the rules of the cluster are not compromised</span></span>
* <span data-ttu-id="ed1ee-228">원활한 업그레이드를 돕는 작업</span><span class="sxs-lookup"><span data-stu-id="ed1ee-228">try to help the upgrade go smoothly</span></span>

### <a name="keep-enforcing-the-rules"></a><span data-ttu-id="ed1ee-229">규칙 적용 유지</span><span class="sxs-lookup"><span data-stu-id="ed1ee-229">Keep enforcing the rules</span></span>
<span data-ttu-id="ed1ee-230">알고 있어야 하는 중요한 점은 배치 제약 조건 및 용량과 같은 엄격한 제약 조건인 규칙이 업그레이드 중에도 적용된다는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-230">The main thing to be aware of is that the rules – the strict constraints like placement constraints and capacities - are still enforced during upgrades.</span></span> <span data-ttu-id="ed1ee-231">배치 제약 조건을 통해 업그레이드 중에도 허용된 작업을 실행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-231">Placement constraints ensure that your workloads only run where they are allowed to, even during upgrades.</span></span> <span data-ttu-id="ed1ee-232">서비스에 대한 제약이 심하면 업그레이드가 오래 걸릴 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-232">When services are highly constrained, upgrades can take longer.</span></span> <span data-ttu-id="ed1ee-233">실행 중인 서비스 또는 노드가 업데이트를 위해 중단되면 이동할 수 있는 위치에 대한 옵션이 거의 없을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-233">When the service or the node it is running on is brought down for an update there may be few options for where it can go.</span></span>

### <a name="smart-replacements"></a><span data-ttu-id="ed1ee-234">스마트 대체</span><span class="sxs-lookup"><span data-stu-id="ed1ee-234">Smart replacements</span></span>
<span data-ttu-id="ed1ee-235">업그레이드가 시작되면 Resource Manager는 클러스터 현재 정렬의 스냅숏을 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-235">When an upgrade starts, the Resource Manager takes a snapshot of the current arrangement of the cluster.</span></span> <span data-ttu-id="ed1ee-236">각각의 업그레이드 도메인이 완료되면 해당 업그레이드 도메인에 있던 서비스를 원래 정렬로 반환하려고 시도합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-236">As each Upgrade Domain completes, it attempts to return the services that were in that Upgrade Domain to their original arrangement.</span></span> <span data-ttu-id="ed1ee-237">이렇게 하면 업그레이드하는 동안 서비스에 대해 최대 2개의 전환이 수행됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-237">This way there are at most two transitions for a service during the upgrade.</span></span> <span data-ttu-id="ed1ee-238">영향을 받는 노드에서 하나의 이동이 있고, 하나의 다시 이동이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-238">There is one move out of the affected node and one move back in.</span></span> <span data-ttu-id="ed1ee-239">클러스터 또는 서비스를 업그레이드 이전의 상태로 돌려놓으면 업그레이드에서도 클러스터의 레이아웃에 영향을 주지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-239">Returning the cluster or service to how it was before the upgrade also ensures the upgrade doesn’t impact the layout of the cluster.</span></span> 

### <a name="reduced-churn"></a><span data-ttu-id="ed1ee-240">이탈 감소</span><span class="sxs-lookup"><span data-stu-id="ed1ee-240">Reduced churn</span></span>
<span data-ttu-id="ed1ee-241">업그레이드 중에 발생하는 또 다른 작업은 클러스터 리소스 관리자에서 분산을 해제하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-241">Another thing that happens during upgrades is that the Cluster Resource Manager turns off balancing.</span></span> <span data-ttu-id="ed1ee-242">부하 분산은 업그레이드를 위해 서비스를 비운 노드로 이동하는 것과 같이 업그레이드 자체에 대한 불필요한 반응을 방지합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-242">Preventing balancing prevents unnecessary reactions to the upgrade itself, like moving services into nodes that were emptied for the upgrade.</span></span> <span data-ttu-id="ed1ee-243">문제가 발생한 업그레이드가 클러스터 업그레이드인 경우 전체 클러스터는 업그레이드하는 동안 부하가 분산되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-243">If the upgrade in question is a Cluster upgrade, then the entire cluster is not balanced during the upgrade.</span></span> <span data-ttu-id="ed1ee-244">제약 조건 검사는 활성 상태를 유지하며 메트릭의 사전 분산을 기반으로 하는 이동만 비활성화됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-244">Constraint checks stay active, only movement based on the proactive balancing of metrics is disabled.</span></span>

### <a name="buffered-capacity--upgrade"></a><span data-ttu-id="ed1ee-245">버퍼링된 용량 및 업그레이드</span><span class="sxs-lookup"><span data-stu-id="ed1ee-245">Buffered Capacity & Upgrade</span></span>
<span data-ttu-id="ed1ee-246">일반적으로 클러스터가 제한되거나 거의 가득 찬 경우에도 업그레이드를 완료할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-246">Generally you want the upgrade to complete even if the cluster is constrained or close to full.</span></span> <span data-ttu-id="ed1ee-247">업그레이드하는 동안 클러스터의 용량을 관리하는 것이 평소보다 훨씬 더 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-247">Managing the capacity of the cluster is even more important during upgrades than usual.</span></span> <span data-ttu-id="ed1ee-248">업그레이드 도메인의 수에 따라 클러스터를 통해 업그레이드가 롤링함에 따라 용량의 5 ~ 20%가 마이그레이션되어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-248">Depending on the number of upgrade domains, between 5 and 20 percent of capacity must be migrated as the upgrade rolls through the cluster.</span></span> <span data-ttu-id="ed1ee-249">해당 작업은 어딘가로 이동해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-249">That work has to go somewhere.</span></span> <span data-ttu-id="ed1ee-250">이는 [버퍼링된 용량](service-fabric-cluster-resource-manager-cluster-description.md#buffered-capacity)이라는 개념이 유용한 경우입니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-250">This is where the notion of [buffered capacities](service-fabric-cluster-resource-manager-cluster-description.md#buffered-capacity) is useful.</span></span> <span data-ttu-id="ed1ee-251">버퍼링된 용량은 정상 작업 중에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-251">Buffered capacity is respected during normal operation.</span></span> <span data-ttu-id="ed1ee-252">클러스터 리소스 관리자에서 필요한 경우 업그레이드 중에 노드를 총 용량(버퍼 사용)까지 채울 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="ed1ee-252">The Cluster Resource Manager may fill nodes up to their total capacity (consuming the buffer) during upgrades if necessary.</span></span>

## <a name="next-steps"></a><span data-ttu-id="ed1ee-253">다음 단계</span><span class="sxs-lookup"><span data-stu-id="ed1ee-253">Next steps</span></span>
* <span data-ttu-id="ed1ee-254">처음부터 시작 및 [서비스 패브릭 클러스터 Resource Manager 소개](service-fabric-cluster-resource-manager-introduction.md)</span><span class="sxs-lookup"><span data-stu-id="ed1ee-254">Start from the beginning and [get an Introduction to the Service Fabric Cluster Resource Manager](service-fabric-cluster-resource-manager-introduction.md)</span></span>
