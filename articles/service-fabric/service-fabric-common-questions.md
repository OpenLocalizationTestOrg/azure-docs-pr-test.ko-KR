---
title: "Microsoft Azure Service Fabric에 대 한 질문 aaaCommon | Microsoft Docs"
description: "다음은 Service Fabric에 대해 자주 묻는 몇 가지 질문과 그에 대한 답변입니다."
services: service-fabric
documentationcenter: .net
author: chackdan
manager: timlt
editor: 
ms.assetid: 5a179703-ff0c-4b8e-98cd-377253295d12
ms.service: service-fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: na
ms.date: 08/18/2017
ms.author: chackdan
ms.openlocfilehash: 4cbe92d2a03f7a1ea5d077807fdc982288220a7e
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 10/06/2017
---
# <a name="commonly-asked-service-fabric-questions"></a><span data-ttu-id="90cd0-103">Service Fabric에 대해 자주 묻는 질문</span><span class="sxs-lookup"><span data-stu-id="90cd0-103">Commonly asked Service Fabric questions</span></span>

<span data-ttu-id="90cd0-104">Service Fabric으로 수행할 수 있는 작업 및 사용 방법에 대한 여러 가지 자주 묻는 질문이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-104">There are many commonly asked questions about what Service Fabric can do and how it should be used.</span></span> <span data-ttu-id="90cd0-105">이 문서에서는 자주 묻는 질문 및 그에 대한 답변을 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-105">This document covers many of those common questions and their answers.</span></span>

## <a name="cluster-setup-and-management"></a><span data-ttu-id="90cd0-106">클러스터 설정 및 관리</span><span class="sxs-lookup"><span data-stu-id="90cd0-106">Cluster setup and management</span></span>

### <a name="can-i-create-a-cluster-that-spans-multiple-azure-regions-or-my-own-datacenters"></a><span data-ttu-id="90cd0-107">여러 Azure 지역 또는 나만의 데이터 센터에 걸쳐서 클러스터를 만들 수 있나요?</span><span class="sxs-lookup"><span data-stu-id="90cd0-107">Can I create a cluster that spans multiple Azure regions or my own datacenters?</span></span>

<span data-ttu-id="90cd0-108">예.</span><span class="sxs-lookup"><span data-stu-id="90cd0-108">Yes.</span></span> 

<span data-ttu-id="90cd0-109">네트워크 연결 tooeach 다른 것으로 hello core 서비스 패브릭 클러스터링 기술 hello world에서 요소가 실행 되 고 사용 하는 toocombine 컴퓨터를 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-109">hello core Service Fabric clustering technology can be used toocombine machines running anywhere in hello world, so long as they have network connectivity tooeach other.</span></span> <span data-ttu-id="90cd0-110">그러나 이러한 클러스터를 구축하고 실행하는 것은 복잡할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-110">However, building and running such a cluster can be complicated.</span></span>

<span data-ttu-id="90cd0-111">이 시나리오에 관심이 있다면 의견을 교환하실 연락처에서 tooget hello 통해 [서비스 패브릭 Github 문제 목록](https://github.com/azure/service-fabric-issues) 또는 순서 tooobtain 추가 설명서에 지원 담당자를 통해.</span><span class="sxs-lookup"><span data-stu-id="90cd0-111">If you are interested in this scenario, we encourage you tooget in contact either through hello [Service Fabric Github Issues List](https://github.com/azure/service-fabric-issues) or through your support representative in order tooobtain additional guidance.</span></span> <span data-ttu-id="90cd0-112">서비스 패브릭 팀 hello tooprovide 추가 명확성, 지침 및 권장 사항이이 시나리오에 대 한 작동 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-112">hello Service Fabric team is working tooprovide additional clarity, guidance, and recommendations for this scenario.</span></span> 

<span data-ttu-id="90cd0-113">일부 작업 tooconsider:</span><span class="sxs-lookup"><span data-stu-id="90cd0-113">Some things tooconsider:</span></span> 

1. <span data-ttu-id="90cd0-114">hello azure에서 서비스 패브릭 클러스터 리소스는 오늘 지역별, 마찬가지로 해당 hello 클러스터를 설정 하는 hello 가상 컴퓨터 크기는 기반으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-114">hello Service Fabric cluster resource in Azure is regional today, as are hello virtual machine scale sets that hello cluster is built on.</span></span> <span data-ttu-id="90cd0-115">이 있음을 의미 국가별 실패의 hello 이벤트에서 있습니다 수 hello Azure 리소스 관리자를 통해 hello 기능 toomanage hello 클러스터 손실 hello Azure 포털입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-115">This means that in hello event of a regional failure you may lose hello ability toomanage hello cluster via hello Azure Resource Manager or hello Azure Portal.</span></span> <span data-ttu-id="90cd0-116">이 hello 클러스터를 계속 실행 하 고 직접 수 toointeract 함께 있을 수 있습니다 하는 경우에 발생할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-116">This can happen even though hello cluster remains running and you'd be able toointeract with it directly.</span></span> <span data-ttu-id="90cd0-117">또한 Azure 오늘날 제공 하지 않습니다 hello 기능 toohave 지역에 걸쳐 사용할 수 있는 단일 가상 네트워크.</span><span class="sxs-lookup"><span data-stu-id="90cd0-117">In addition, Azure today does not offer hello ability toohave a single virtual network that is usable across regions.</span></span> <span data-ttu-id="90cd0-118">즉, Azure의 다중 지역 클러스터 어느 하나라도 필요한 [hello VM 크기 집합의에서 각 VM에 대 한 공용 IP 주소](../virtual-machine-scale-sets/virtual-machine-scale-sets-networking.md#public-ipv4-per-virtual-machine) 또는 [Azure VPN 게이트웨이](../vpn-gateway/vpn-gateway-about-vpngateways.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-118">This means that a multi-region cluster in Azure requires either [Public IP Addresses for each VM in hello VM Scale Sets](../virtual-machine-scale-sets/virtual-machine-scale-sets-networking.md#public-ipv4-per-virtual-machine) or [Azure VPN Gateways](../vpn-gateway/vpn-gateway-about-vpngateways.md).</span></span> <span data-ttu-id="90cd0-119">이러한 네트워킹 선택을 시킬 다른 비용, 성능 및 toosome도 응용 프로그램 디자인 주의 깊은 분석과 계획 순위 이러한 환경 구성 하기 전에 필요 하므로.</span><span class="sxs-lookup"><span data-stu-id="90cd0-119">These networking choices have different impacts on costs, performance, and toosome degree application design, so careful analysis and planning is required before standing up such an environment.</span></span>
2. <span data-ttu-id="90cd0-120">hello 유지 관리, 관리 및이 시스템의 모니터링 될 수 있습니다, 복잡 한에 확대 하는 경우에 특히 _형식_ 환경 등 다른 클라우드 공급자 또는 온-프레미스 리소스와 Azure 간 .</span><span class="sxs-lookup"><span data-stu-id="90cd0-120">hello maintenance, management, and monitoring of these machines can become complicated, especially when spanned across _types_ of environments, such as between different cloud providers or between on-premises resources and Azure.</span></span> <span data-ttu-id="90cd0-121">업그레이드 tooensure, 모니터링, 관리, 주의 해야 하 고 진단은 이러한 환경에서 프로덕션 작업을 실행 하기 전에 hello 클러스터와 hello 응용 프로그램에 대 한 이해 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-121">Care must be taken tooensure that upgrades, monitoring, management, and diagnostics are understood for both hello cluster and hello applications before running production workloads in such an environment.</span></span> <span data-ttu-id="90cd0-122">Azure에서 또는 자체 데이터 센터 내에서 이러한 문제를 여러 번 해결해본 적이 있으면 Service Fabric 클러스터를 구축하거나 실행할 때도 이러한 동일한 해결 방법을 적용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-122">If you already have lots of experience solving these problems in Azure or within your own datacenters, then it is likely that those same solutions can be applied when building out or running your Service Fabric cluster.</span></span> 

### <a name="do-service-fabric-nodes-automatically-receive-os-updates"></a><span data-ttu-id="90cd0-123">Service Fabric 노드에서 OS 업데이트를 자동으로 수신하나요?</span><span class="sxs-lookup"><span data-stu-id="90cd0-123">Do Service Fabric nodes automatically receive OS updates?</span></span>

<span data-ttu-id="90cd0-124">오늘 하지 있지만이 또한 Azure 노드가 toodeliver는 일반적으로 요청 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-124">Not today, but this is also a common request that Azure intends toodeliver.</span></span>

<span data-ttu-id="90cd0-125">중간 hello에 [응용 프로그램 제공](service-fabric-patch-orchestration-application.md) hello 운영 체제 서비스 패브릭 노드 아래 패치가 적용 되 고 toodate를 유지 하 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-125">In hello interim, we have [provided an application](service-fabric-patch-orchestration-application.md) that hello operating systems underneath your Service Fabric nodes stay patched and up toodate.</span></span>

<span data-ttu-id="90cd0-126">hello 챌린지 OS 업데이트를 임시 가용성 손실이 발생 하는 hello 컴퓨터에 대 한 다시 부팅 일반적으로 필요가 없다고입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-126">hello challenge with OS updates is that they typically require a reboot of hello machine, which results in temporary availability loss.</span></span> <span data-ttu-id="90cd0-127">자체적으로 하지 않으므로 문제가 서비스 패브릭 서비스 tooother 노드에 대 한 트래픽을 자동 리디렉션됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-127">By itself, that is not a problem, since Service Fabric will automatically redirect traffic for those services tooother nodes.</span></span> <span data-ttu-id="90cd0-128">그러나 OS 업데이트는 hello 클러스터 전체에서 조정 하지 않은, 경우 한 번에 많은 노드 아래로 이동 하는 hello 잠재적인은.</span><span class="sxs-lookup"><span data-stu-id="90cd0-128">However, if OS updates are not coordinated across hello cluster, there is hello potential that many nodes go down at once.</span></span> <span data-ttu-id="90cd0-129">이러한 동시 재부팅은 서비스 또는 적어도 특정 파티션(상태 저장 서비스)에 대한 총체적인 가용성 손실을 유도할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-129">Such simultaneous reboots can cause complete availability loss for a service, or at least for a specific partition (for a stateful service).</span></span>

<span data-ttu-id="90cd0-130">Hello 이후에서 계획 toosupport OS 업데이트 정책을 완전히 자동화 하 고 업데이트 도메인에 걸쳐 조정 되는 기타 예기치 않은 실패 및 다시 부팅 가용성 유지 되도록 보장입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-130">In hello future, we plan toosupport an OS update policy that is fully automated and coordinated across update domains, ensuring that availability is maintained despite reboots and other unexpected failures.</span></span>

### <a name="can-i-use-large-virtual-machine-scale-sets-in-my-sf-cluster"></a><span data-ttu-id="90cd0-131">SF 클러스터 내에서 큰 가상 컴퓨터 크기 집합을 사용할 수 있나요?</span><span class="sxs-lookup"><span data-stu-id="90cd0-131">Can I use Large Virtual Machine Scale Sets in my SF cluster?</span></span> 

<span data-ttu-id="90cd0-132">**간단한 대답** - 아니요</span><span class="sxs-lookup"><span data-stu-id="90cd0-132">**Short answer** - No.</span></span> 

<span data-ttu-id="90cd0-133">**긴 응답** hello 큰 가상 컴퓨터 크기 집합 tooscale 허용 하지만-가상 컴퓨터 크기 집합 최대 1000 VM 인스턴스, hello 배치 그룹 (PGs)를 사용 하 여 작업을 수행 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-133">**Long Answer** - Although hello Large Virtual Machine Scale Sets allow you tooscale a virtual machine scale set upto 1000 VM instances, it does so by hello use of Placement Groups (PGs).</span></span> <span data-ttu-id="90cd0-134">오류 도메인 (Fd) 및 업그레이드 도메인 (Ud)은 배치 그룹 서비스 패브릭에서는 Ud와 Fd toomake 배치 결정 서비스 복제본/서비스 인스턴스 내에서 일관 된 수만 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-134">Fault domains (FDs) and upgrade domains (UDs) are only consistent within a placement group Service fabric uses FDs and UDs toomake placement decisions of your service replicas/Service instances.</span></span> <span data-ttu-id="90cd0-135">Hello Fd 및 Ud 비교 가능한 이므로 배치 그룹 내 에서만 SF 사용할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-135">Since hello FDs  and UDs are comparable only within a placement group SF cannot use it.</span></span> <span data-ttu-id="90cd0-136">예를 들어 p g 1에서 v m 1의 FD 토폴로지 = 0 PG2에 VM9 FD의 토폴로지 있으며 = 4, v m 1과 v m 2는 두 개의 서로 다른 하드웨어 랙에, SF가이 case toomake 배치 결정에 hello FD 값을 사용할 수 없습니다는 따라서 의미 하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-136">For example, If VM1 in PG1 has a topology of FD=0 and VM9 in PG2 has a topology of FD=4 , it does not mean that VM1 and VM2 are on two different Hardware Racks, hence SF cannot use hello FD values in this case toomake placement decisions.</span></span>

<span data-ttu-id="90cd0-137">다른 문제가 있습니다 큰 가상 컴퓨터 크기 집합 현재 처럼 수준-4 hello 부족 부하 분산 지원 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-137">There are other issues with Large virtual machine scale sets currently, like hello lack of level-4 Load balancing support.</span></span> <span data-ttu-id="90cd0-138">Toofor 참조 [큼에 대 한 세부 정보 집합의 크기를 조정](../virtual-machine-scale-sets/virtual-machine-scale-sets-placement-groups.md)</span><span class="sxs-lookup"><span data-stu-id="90cd0-138">Refer toofor [details on Large scale sets](../virtual-machine-scale-sets/virtual-machine-scale-sets-placement-groups.md)</span></span>



### <a name="what-is-hello-minimum-size-of-a-service-fabric-cluster-why-cant-it-be-smaller"></a><span data-ttu-id="90cd0-139">서비스 패브릭 클러스터의 최소 크기 hello 이란?</span><span class="sxs-lookup"><span data-stu-id="90cd0-139">What is hello minimum size of a Service Fabric cluster?</span></span> <span data-ttu-id="90cd0-140">작으면 안되는 이유는 무엇인가요?</span><span class="sxs-lookup"><span data-stu-id="90cd0-140">Why can't it be smaller?</span></span>

<span data-ttu-id="90cd0-141">프로덕션 작업을 실행 하는 서비스 패브릭 클러스터에 대 한 최소 지원 되는 크기를 hello는 5 개의 노드가 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-141">hello minimum supported size for a Service Fabric cluster running production workloads is five nodes.</span></span> <span data-ttu-id="90cd0-142">개발/테스트 시나리오에 대해 3개의 노드 클러스터를 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-142">For dev/test scenarios, we support three node clusters.</span></span>

<span data-ttu-id="90cd0-143">이러한 최소값 hello 서비스 패브릭 클러스터 hello 명명 서비스와 hello 장애 조치 관리자를 포함 하 여 시스템 상태 저장 서비스의 집합을 실행 하기 때문에 존재 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-143">These minimums exist because hello Service Fabric cluster runs a set of stateful system services, including hello naming service and hello failover manager.</span></span> <span data-ttu-id="90cd0-144">되었습니다 서비스를 추적 하는 이러한 서비스 toohello 클러스터를 배포 하 고 이러한 서비스는 호스팅되 현재 위치 강력한 일관성에 따라 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-144">These services, which track what services have been deployed toohello cluster and where they're currently hosted, depend on strong consistency.</span></span> <span data-ttu-id="90cd0-145">강력한 일관성 해당 차례로 hello 기능 tooacquire에 따라 달라 집니다는 *쿼럼* 특정된 업데이트에 대 한 이들의 toohello 상태 서비스, 쿼럼 엄격 하 게 다 수의 지정된 된 서비스에 대 한 hello 복제본 (N/2 + 1)을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-145">That strong consistency, in turn, depends on hello ability tooacquire a *quorum* for any given update toohello state of those services, where a quorum represents a strict majority of hello replicas (N/2 +1) for a given service.</span></span>

<span data-ttu-id="90cd0-146">배경 정보로 몇 가지 가능한 클러스터 구성을 살펴보겠습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-146">With that background, let's examine some possible cluster configurations:</span></span>

<span data-ttu-id="90cd0-147">**한 노드에서**: 어떤 이유로 든 hello 단일 노드의 hello 손실을 hello 전체 클러스터 hello 손실 의미 하므로이 옵션 고가용성을 제공 하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-147">**One node**: this option does not provide high availability since hello loss of hello single node for any reason means hello loss of hello entire cluster.</span></span>

<span data-ttu-id="90cd0-148">**두 개 노드**: 두 노드(N = 2) 간에 배포된 서비스의 쿼럼은 2(2/2 + 1 = 2)입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-148">**Two nodes**: a quorum for a service deployed across two nodes (N = 2) is 2 (2/2 + 1 = 2).</span></span> <span data-ttu-id="90cd0-149">단일 복제본을 잃어버린 경우 불가능 한 toocreate 쿼럼 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-149">When a single replica is lost, it is impossible toocreate a quorum.</span></span> <span data-ttu-id="90cd0-150">서비스 업그레이드를 수행하려면 복제본을 일시적으로 종료해야 하므로 유용한 구성이 아닙니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-150">Since performing a service upgrade requires temporarily taking down a replica, this is not a useful configuration.</span></span>

<span data-ttu-id="90cd0-151">**3 개의 노드가**: 3 개의 노드 (N = 3), hello 요구 사항 toocreate 쿼럼은 여전히 두 노드 (3/2 + 1 = 2).</span><span class="sxs-lookup"><span data-stu-id="90cd0-151">**Three nodes**: with three nodes (N=3), hello requirement toocreate a quorum is still two nodes (3/2 + 1 = 2).</span></span> <span data-ttu-id="90cd0-152">따라서 개별 노드를 손실하고 쿼럼을 계속 유지할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-152">This means that you can lose an individual node and still maintain quorum.</span></span>

<span data-ttu-id="90cd0-153">hello 3 개 노드 클러스터 구성 대해서는 개발/테스트 안전 하 게 업그레이드를 수행 하 고 개별 노드 실패 수 있으므로으로 동시에 발생 하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-153">hello three node cluster configuration is supported for dev/test because you can safely perform upgrades and survive individual node failures, as long as they don't happen simultaneously.</span></span> <span data-ttu-id="90cd0-154">프로덕션 작업에 대해 5 개의 노드가 필요 하므로 탄력적인 toosuch 동시 실패 이어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-154">For production workloads, you must be resilient toosuch a simultaneous failure, so five nodes are required.</span></span>

### <a name="can-i-turn-off-my-cluster-at-nightweekends-toosave-costs"></a><span data-ttu-id="90cd0-155">밤/주말 toosave 비용에 클러스터를 해제할 수 있습니까?</span><span class="sxs-lookup"><span data-stu-id="90cd0-155">Can I turn off my cluster at night/weekends toosave costs?</span></span>

<span data-ttu-id="90cd0-156">일반적으로 그렇지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-156">In general, no.</span></span> <span data-ttu-id="90cd0-157">서비스 패브릭 상태를 저장 임시 로컬 디스크에는 hello 가상 컴퓨터가 다른 호스트로 이동된 tooa 이면 hello 데이터 이동 하지 않습니다 것을 의미 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-157">Service Fabric stores state on local, ephemeral disks, meaning that if hello virtual machine is moved tooa different host, hello data does not move with it.</span></span> <span data-ttu-id="90cd0-158">정상 작업에서 하는 문제가 되지 않습니다는 hello 새 노드에 다른 노드가 toodate에 가져오는 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-158">In normal operation, that is not a problem as hello new node is brought up toodate by other nodes.</span></span> <span data-ttu-id="90cd0-159">그러나 모든 노드를 중지 하 고 나중에 다시 시작 하는 경우 대부분의 hello 노드 새 호스트에서 시작한 hello 시스템 없습니다 toorecover 확인 상당한 가능성은.</span><span class="sxs-lookup"><span data-stu-id="90cd0-159">However, if you stop all nodes and restart them later, there is a significant possibility that most of hello nodes start on new hosts and make hello system unable toorecover.</span></span>

<span data-ttu-id="90cd0-160">배포 하기 전에 응용 프로그램을 테스트 하는 것에 대 한 toocreate 클러스터, 원하는 경우 해당 클러스터의 일부로 동적으로 만드는 것이 좋습니다 프로그램 [연속 통합/연속 배포 파이프라인](service-fabric-set-up-continuous-integration.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-160">If you would like toocreate clusters for testing your application before it is deployed, we recommend that you dynamically create those clusters as part of your [continuous integration/continuous deployment pipeline](service-fabric-set-up-continuous-integration.md).</span></span>


### <a name="how-do-i-upgrade-my-operating-system-for-example-from-windows-server-2012-toowindows-server-2016"></a><span data-ttu-id="90cd0-161">(예: Windows Server 2012 tooWindows 서버 2016)에서 내 운영 체제를 업그레이드 하려면 어떻게 해야 합니까?</span><span class="sxs-lookup"><span data-stu-id="90cd0-161">How do I upgrade my Operating System (for example from Windows Server 2012 tooWindows Server 2016)?</span></span>

<span data-ttu-id="90cd0-162">오늘날, 개선된 된 환경에서 작업 중 동안 사용자는 hello 업그레이드에 대 한입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-162">While we're working on an improved experience, today, you are responsible for hello upgrade.</span></span> <span data-ttu-id="90cd0-163">Hello에 hello OS 이미지를 업그레이드 해야 hello의 가상 컴퓨터를 한 번에 하나의 VM 클러스터입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-163">You must upgrade hello OS image on hello virtual machines of hello cluster one VM at a time.</span></span> 

## <a name="container-support"></a><span data-ttu-id="90cd0-164">컨테이너 지원</span><span class="sxs-lookup"><span data-stu-id="90cd0-164">Container Support</span></span>

### <a name="why-are-my-containers-that-are-deployed-toosf-unable-tooresolve-dns-addresses"></a><span data-ttu-id="90cd0-165">주소 내 된 컨테이너를 배포 된 tooSF 없습니다 tooresolve DNS 이유는 무엇입니까?</span><span class="sxs-lookup"><span data-stu-id="90cd0-165">Why are my containers that are deployed tooSF unable tooresolve DNS addresses?</span></span>

<span data-ttu-id="90cd0-166">이 문제는 5.6.204.9494 버전의 클러스터에 대해 보고되었습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-166">This issue has been reported on clusters that are on 5.6.204.9494 version</span></span> 

<span data-ttu-id="90cd0-167">**완화** : 따라 [이 문서](service-fabric-dnsservice.md) tooenable hello DNS 서비스 패브릭 클러스터에는 서비스입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-167">**Mitigation** :  Follow [this document](service-fabric-dnsservice.md) tooenable hello DNS service fabric service in your cluster.</span></span>

<span data-ttu-id="90cd0-168">**해결** : 지원 되는 업그레이드 tooa 클러스터 버전 사용 가능 해지면 5.6.204.9494, 보다 높은입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-168">**Fix** :  Upgrade tooa supported cluster version that is higher than 5.6.204.9494, when it is available.</span></span> <span data-ttu-id="90cd0-169">클러스터 tooautomatic 업그레이드 설정 되 면 hello 클러스터에는 고정이 문제가 있는 toohello 버전 자동 업그레이드 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-169">If your cluster is set tooautomatic upgrades, then hello cluster will automatically upgrade toohello version that has this issue fixed.</span></span>

  
## <a name="application-design"></a><span data-ttu-id="90cd0-170">응용 프로그램 설계</span><span class="sxs-lookup"><span data-stu-id="90cd0-170">Application Design</span></span>

### <a name="whats-hello-best-way-tooquery-data-across-partitions-of-a-reliable-collection"></a><span data-ttu-id="90cd0-171">신뢰할 수 있는 컬렉션의 파티션에서 hello 가장 좋은 방법은 tooquery 데이터 란?</span><span class="sxs-lookup"><span data-stu-id="90cd0-171">What's hello best way tooquery data across partitions of a Reliable Collection?</span></span>

<span data-ttu-id="90cd0-172">신뢰할 수 있는 컬렉션은 일반적으로 [분할](service-fabric-concepts-partitioning.md) tooenable 큰 성능 및 처리량에 대 한 확장 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-172">Reliable collections are typically [partitioned](service-fabric-concepts-partitioning.md) tooenable scale out for greater performance and throughput.</span></span> <span data-ttu-id="90cd0-173">즉, 지정된 된 서비스에 대 한 hello 상태 또는 수 억 컴퓨터의 단위: 100에서 확산 시킬 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-173">That means that hello state for a given service may be spread across 10s or 100s of machines.</span></span> <span data-ttu-id="90cd0-174">해당 전체 데이터 집합에 대해 tooperform 작업을 몇 가지 옵션은 있습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-174">tooperform operations over that full data set, you have a few options:</span></span>

- <span data-ttu-id="90cd0-175">다른 서비스 toopull hello 필요한 데이터에서의 모든 파티션이 쿼리 하는 서비스를 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-175">Create a service that queries all partitions of another service toopull in hello required data.</span></span>
- <span data-ttu-id="90cd0-176">다른 서비스의 모든 파티션에서 데이터를 수신할 수 있는 서비스를 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-176">Create a service that can receive data from all partitions of another service.</span></span>
- <span data-ttu-id="90cd0-177">각 서비스 tooan 외부 저장소에서 데이터를 주기적으로 푸시하십시오.</span><span class="sxs-lookup"><span data-stu-id="90cd0-177">Periodically push data from each service tooan external store.</span></span> <span data-ttu-id="90cd0-178">이 방법은 hello 쿼리를 수행 하는 핵심 비즈니스 논리의 일부가 아닌 경우 적절 한만 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-178">This approach is only appropriate if hello queries you're performing are not part of your core business logic.</span></span>


### <a name="whats-hello-best-way-tooquery-data-across-my-actors"></a><span data-ttu-id="90cd0-179">내 작업자 간에 hello 가장 좋은 방법은 tooquery 데이터 란?</span><span class="sxs-lookup"><span data-stu-id="90cd0-179">What's hello best way tooquery data across my actors?</span></span>

<span data-ttu-id="90cd0-180">작업자는 상태의 디자인 된 toobe 독립적 단위 및 계산의 경우, 것 이므로 tooperform 런타임에 행위자 상태의 광범위 한 쿼리를 권장 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-180">Actors are designed toobe independent units of state and compute, so it is not recommended tooperform broad queries of actor state at runtime.</span></span> <span data-ttu-id="90cd0-181">행위자 상태의 전체 집합 hello 필요 tooquery를 설정한 경우 중 하나를 고려해 야:</span><span class="sxs-lookup"><span data-stu-id="90cd0-181">If you have a need tooquery across hello full set of actor state, you should consider either:</span></span>

- <span data-ttu-id="90cd0-182">Hello 요청 수입니다. 네트워크 toogather 모든 데이터 서비스에 있는 파티션의 행위자 toohello 수의 hello 수에서 되도록 행위자 서비스 상태 저장 reliable services 대체 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-182">Replacing your actor services with stateful reliable services, such that hello number of network requests toogather all data from hello number of actors toohello number of partitions in your service.</span></span>
- <span data-ttu-id="90cd0-183">행위자 tooperiodically 푸시 쉽게 쿼리를 위한 해당 상태 tooan 외부 저장소를 디자인 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-183">Designing your actors tooperiodically push their state tooan external store for easier querying.</span></span> <span data-ttu-id="90cd0-184">으로 위에이 방법은 hello 쿼리를 수행 하는 런타임 동작에 대 한 필요 하지 않은 경우에 적합 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-184">As above, this approach is only viable if hello queries you're performing are not required for your runtime behavior.</span></span>

### <a name="how-much-data-can-i-store-in-a-reliable-collection"></a><span data-ttu-id="90cd0-185">Reliable Collection에 저장할 수 있는 데이터는 얼마나 되나요?</span><span class="sxs-lookup"><span data-stu-id="90cd0-185">How much data can I store in a Reliable Collection?</span></span>

<span data-ttu-id="90cd0-186">신뢰할 수 있는 서비스는 hello hello 클러스터에 있는 컴퓨터 수와 이러한 컴퓨터에서 사용 가능한 메모리 양에 hello에 의해서만 제한 hello 금액을 저장할 수 있으므로 일반적으로 분할 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-186">Reliable services are typically partitioned, so hello amount you can store is only limited by hello number of machines you have in hello cluster, and hello amount of memory available on those machines.</span></span>

<span data-ttu-id="90cd0-187">예를 들어, 평균 1kb 크기의 개체를 저장하는 100개 파티션과 3개 복제본이 있는 서비스에 Reliable Collection이 있다고 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-187">As an example, suppose that you have a reliable collection in a service with 100 partitions and 3 replicas, storing objects that average 1kb in size.</span></span> <span data-ttu-id="90cd0-188">이제 컴퓨터당 16gb의 메모리가 있는 10개의 컴퓨터 클러스터가 있다고 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-188">Now suppose that you have a 10 machine cluster with 16gb of memory per machine.</span></span> <span data-ttu-id="90cd0-189">간소 성 및 toobe 지점에 대 한 hello 운영 체제 및 시스템 서비스, hello 서비스 패브릭 런타임을 서비스 6gb의는 10gb의 사용 가능한 시스템 당 또는 100gb hello 클러스터에 대 한 종료를 사용을 가정 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-189">For simplicity and toobe very conservative, assume that hello operating system and system services, hello Service Fabric runtime, and your services consume 6gb of that, leaving 10gb available per machine, or 100gb for hello cluster.</span></span>

<span data-ttu-id="90cd0-190">각 개체는 세 번(주에서 1번, 복제본에서 2번) 저장되어야 하며 전체 용량으로 작동할 때 컬렉션에서 약 3천 500만 개체에 대해 충분한 메모리를 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-190">Keeping in mind that each object must be stored three times (one primary and two replicas), you would have sufficient memory for approximately 35 million objects in your collection when operating at full capacity.</span></span> <span data-ttu-id="90cd0-191">그러나 오류 도메인과 업그레이드 도메인 hello 숫자 tooroughly 23 백만 높이면를 용량의 1/3에 대 한 정보를 나타내며의 복원 력이 toohello 동시 손실 되는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-191">However, we recommend being resilient toohello simultaneous loss of a failure domain and an upgrade domain, which represents about 1/3 of capacity, and would reduce hello number tooroughly 23 million.</span></span>

<span data-ttu-id="90cd0-192">이 계산에서는 다음 사항도 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-192">Note that this calculation also assumes:</span></span>

- <span data-ttu-id="90cd0-193">Hello 파티션 간에 데이터의 해당 hello 분포는 균일 한 또는 부하 메트릭을 toohello 클러스터 리소스 관리자를 보고 하는 약입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-193">That hello distribution of data across hello partitions is roughly uniform or that you're reporting load metrics toohello Cluster Resource Manager.</span></span> <span data-ttu-id="90cd0-194">기본적으로 Service Fabric은 복제본 수에 따라 부하 분산을 수행합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-194">By default, Service Fabric will load balance based on replica count.</span></span> <span data-ttu-id="90cd0-195">위의 예제에서는 hello 클러스터의 각 노드에서 10 개의 주 복제본과 보조 복제본을 20를 추가 하는 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-195">In our example above, that would put 10 primary replicas and 20 secondary replicas on each node in hello cluster.</span></span> <span data-ttu-id="90cd0-196">부하가 hello 파티션 간에 고르게 분산 되는 잘 작동 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-196">That works well for load that is evenly distributed across hello partitions.</span></span> <span data-ttu-id="90cd0-197">부하를 없는 경우도 hello 리소스 관리자 및 수 있도록 더 작은 복제본 함께 팩 큰 복제본 tooconsume 더 많은 메모리를 개별 노드에서 부하를 보고 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-197">If load is not even, you must report load so that hello Resource Manager can pack smaller replicas together and allow larger replicas tooconsume more memory on an individual node.</span></span>

- <span data-ttu-id="90cd0-198">에 해당 hello 신뢰할 수 있는 서비스는 hello hello 클러스터에 하나의 저장 상태입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-198">That hello reliable service in question is hello only one storing state in hello cluster.</span></span> <span data-ttu-id="90cd0-199">여러 서비스 tooa 클러스터를 배포할 수 해야 toobe 각각은 toorun 필요 하 고 관리할 수 있는 상태로 hello 리소스에 주의 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-199">Since you can deploy multiple services tooa cluster, you need toobe mindful of hello resources that each will need toorun and manage its state.</span></span>

- <span data-ttu-id="90cd0-200">해당 hello 클러스터 자체 증가 하지 않았거나, 축소.</span><span class="sxs-lookup"><span data-stu-id="90cd0-200">That hello cluster itself is not growing or shrinking.</span></span> <span data-ttu-id="90cd0-201">더 많은 컴퓨터를 추가 하면 서비스 패브릭은 균형을 다시 조정할 복제본 tooleverage hello 추가 용량 개별 복제 컴퓨터에 걸쳐 있을 수 있으므로 컴퓨터 수가 hello 회 hello 서비스에는 파티션 수를 초과 될 때까지 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-201">If you add more machines, Service Fabric will rebalance your replicas tooleverage hello additional capacity until hello number of machines surpasses hello number of partitions in your service, since an individual replica cannot span machines.</span></span> <span data-ttu-id="90cd0-202">이와 반대로 컴퓨터를 제거 하 여 hello hello 클러스터 크기를 줄이고 복제본을 더 밀접 하 게 압축할 수 및 전체 용량이 부족 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-202">By contrast, if you reduce hello size of hello cluster by removing machines, your replicas will be packed more tightly and have less overall capacity.</span></span>

### <a name="how-much-data-can-i-store-in-an-actor"></a><span data-ttu-id="90cd0-203">행위자에 저장할 수 있는 데이터는 얼마나 되나요?</span><span class="sxs-lookup"><span data-stu-id="90cd0-203">How much data can I store in an actor?</span></span>

<span data-ttu-id="90cd0-204">신뢰할 수 있는 서비스와 마찬가지로 hello 행위자 서비스에 저장할 수 있는 데이터 양은 hello 총 디스크 공간을 사용할 수 있는 메모리도 클러스터의 hello 노드에서 제한만 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-204">As with reliable services, hello amount of data that you can store in an actor service is only limited by hello total disk space and memory available across hello nodes in your cluster.</span></span> <span data-ttu-id="90cd0-205">그러나 개별 배우는 사용 되는 상태 및 연결 된 비즈니스 논리의 양이 tooencapsulate 상태일 때 가장 효과적입니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-205">However, individual actors are most effective when they are used tooencapsulate a small amount of state and associated business logic.</span></span> <span data-ttu-id="90cd0-206">일반적으로 개별 행위자는 킬로바이트 단위로 측정되는 상태를 포함하게 됩니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-206">As a general rule, an individual actor should have state that is measured in kilobytes.</span></span>

## <a name="other-questions"></a><span data-ttu-id="90cd0-207">기타 질문</span><span class="sxs-lookup"><span data-stu-id="90cd0-207">Other questions</span></span>

### <a name="how-does-service-fabric-relate-toocontainers"></a><span data-ttu-id="90cd0-208">서비스 패브릭 toocontainers를 연결 하는 방법</span><span class="sxs-lookup"><span data-stu-id="90cd0-208">How does Service Fabric relate toocontainers?</span></span>

<span data-ttu-id="90cd0-209">컨테이너 환경 모두에서 일관성 있게 실행 및 단일 컴퓨터에서 격리 된 방식으로 작동할 수 있도록 하는 간단한 방법 toopackage 서비스와 해당 종속성을 제공 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-209">Containers offer a simple way toopackage services and their dependencies such that they run consistently in all environments and can operate in an isolated fashion on a single machine.</span></span> <span data-ttu-id="90cd0-210">서비스 패브릭 방식으로 toodeploy 제공 및 서비스를 포함 하 여 관리 [서비스 컨테이너에 패키지 된](service-fabric-containers-overview.md)합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-210">Service Fabric offers a way toodeploy and manage services, including [services that have been packaged in a container](service-fabric-containers-overview.md).</span></span>

### <a name="are-you-planning-tooopen-source-service-fabric"></a><span data-ttu-id="90cd0-211">서비스 패브릭 tooopen 소스 계획 입니까?</span><span class="sxs-lookup"><span data-stu-id="90cd0-211">Are you planning tooopen source Service Fabric?</span></span>

<span data-ttu-id="90cd0-212">GitHub에서 tooopen 소스 hello에 대 한 신뢰할 수 있는 서비스 및 신뢰할 수 있는 작업자 프레임 워크를 의도 하 고 커뮤니티 기여 toothose 프로젝트를 수락 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-212">We intend tooopen source hello reliable services and reliable actors frameworks on GitHub and will accept community contributions toothose projects.</span></span> <span data-ttu-id="90cd0-213">Hello 따라 [서비스 패브릭 블로그](https://blogs.msdn.microsoft.com/azureservicefabric/) 발표 하는 대로 자세한 세부 정보에 대 한 합니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-213">Please follow hello [Service Fabric blog](https://blogs.msdn.microsoft.com/azureservicefabric/) for more details as they're announced.</span></span>

<span data-ttu-id="90cd0-214">hello는 현재 계획 tooopen 소스 hello 서비스 패브릭 런타임을 없습니다.</span><span class="sxs-lookup"><span data-stu-id="90cd0-214">hello are currently no plans tooopen source hello Service Fabric runtime.</span></span>

## <a name="next-steps"></a><span data-ttu-id="90cd0-215">다음 단계</span><span class="sxs-lookup"><span data-stu-id="90cd0-215">Next steps</span></span>

- [<span data-ttu-id="90cd0-216">핵심 Service Fabric 개념 및 모범 사례에 대해 알아보기</span><span class="sxs-lookup"><span data-stu-id="90cd0-216">Learn about core Service Fabric concepts and best practices</span></span>](https://mva.microsoft.com/en-us/training-courses/building-microservices-applications-on-azure-service-fabric-16747?l=tbuZM46yC_5206218965)
